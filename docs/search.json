[
  {
    "objectID": "posts/yt4/index.html",
    "href": "posts/yt4/index.html",
    "title": "Generative Adversarial Networks in PyTorch with PyTorch Lightning",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\nimport pytorch_lightning as pl\n\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import FashionMNIST\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os \nfrom utils import *\n\nBATCH_SIZE = 128\nNUM_WORKERS=int(os.cpu_count() / 2)\nversions()\n\n+-------------------+------------+\n|     Component     |  Version   |\n+-------------------+------------+\n|       Python      |   3.12.2   |\n+-------------------+------------+\n|      PyTorch      | 2.2.2+cpu  |\n+-------------------+------------+\n| PyTorch Lightning |   2.2.3    |\n+-------------------+------------+\n|    torchvision    | 0.17.2+cpu |\n+-------------------+------------+\n\n\n\nGenerative Adversarial Networks (GANs)\nGenerative Adversarial Networks (GANs) are a class of artificial intelligence algorithms used in unsupervised machine learning, implemented by a system of two neural networks contesting with each other in a zero-sum game framework. This technique was introduced by Ian Goodfellow and his colleagues in 2014 and has since been an active topic of research with applications including in generative image models, video generation, and voice generation systems.\nGANs consist of two distinct models: a generator and a discriminator:\n\nGenerator: This network learns to generate plausible data. The generated instances become negative training samples for the discriminator.\nDiscriminator: This network learns to distinguish between real and fake data from the generator. The discriminator penalizes the generator for producing implausible results.\n\nWhen training a GAN, the generator and discriminator are trained simultaneously:\n\nGenerator Learning: The generator improves its ability to create fake data by continuously attempting to deceive the discriminator. The optimal generator \\(G\\) minimizes\n\\[\\log(1 - D(G(z)))\\]\nwhere \\(G(z)\\) is the generator’s output when given noise \\(z\\), and \\(D\\) is the discriminator’s estimate of the probability that a sample came from the training data rather than the generator.\nDiscriminator Learning: The discriminator improves its ability to distinguish real data from fake data produced by the generator. The optimal discrimator \\(D\\) maximizes:\n\\[\\log D(x) + \\log(1 - D(G(z)))\\]\nwhere \\(x\\) is data from the true distribution.\n\nThe training involves back-and-forth iterations where the discriminator guides the generator to produce more realistic outputs, and the generator forces the discriminator to become more skilled at distinguishing real data from fakes.\nFor a more comprehensive overview of GANs, refer to the original paper or additional resources on generative models:\n\nOriginal GAN Paper by Ian Goodfellow et al., 2014\nGenerative Models on OpenAI\n\n\n\n\nPyTorch Lightning\n\nPyTorch Lightning is a library built on top of PyTorch that abstracts complexity so that researchers and developers can build models faster and more efficiently. By structuring PyTorch code to be more modular and hardware-agnostic, Lightning enables scalability across different hardware setups without changing the model code.\nPyTorch Lightning is designed for high flexibility and even higher performance, making it a popular choice for both academic researchers and industry practitioners.\nFor more detailed information on PyTorch Lightning, visit the official documentation:\n\nPyTorch Lightning Documentation\n\n\n\nTo install PyTorch Lightning using the pip package manager run\npip install pytorch_lightning\nat the command line.\n\n\nFashion-MNIST Dataset\nFashion-MNIST is a dataset of Zalando’s article images, designed as a more challenging replacement for the traditional MNIST dataset of handwritten digits. Each example in Fashion-MNIST is a 28x28 grayscale image, associated with a label from 10 classes.\n\nNumber of Samples: 70,000 (60,000 training and 10,000 test images)\nImage Size: 28x28 pixels, grayscale\nNumber of Classes: 10\n\nEach class corresponds to a type of clothing:\n\n\nT-shirt/top\nTrouser\nPullover\nDress\nCoat\nSandal\nShirt\nSneaker\nBag\nAnkle boot\n\n\n\n\nCode\nclass FashionMNISTDataModule(pl.LightningDataModule):\n    \"\"\"\n    PyTorch Lightning Data Module for the FashionMNIST dataset.\n    Handles the loading, downloading, and transforming of data into train, validation, and test splits.\n    \"\"\"\n\n    def __init__(self, data_dir=\"./data\", batch_size=32, num_workers=4):\n        \"\"\"\n        Initializes the data module.\n        \n        Args:\n            data_dir (str): The directory to store/download the dataset.\n            batch_size (int): Number of samples in each batch.\n            num_workers (int): Number of subprocesses to use for data loading.\n        \"\"\"\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n        # Define transformations that will be applied to each data sample\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),  # Convert images to PyTorch tensors\n            transforms.Normalize((0.5,), (0.5,)),  # Normalize grayscale images\n        ])\n\n    def prepare_data(self):\n        \"\"\"\n        Download the FashionMNIST dataset if not already available locally.\n        This method is only called from a single GPU.\n        \"\"\"\n        FashionMNIST(self.data_dir, train=True, download=True)\n        FashionMNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        \"\"\"\n        Set up the dataset for the 'fit' and 'test' stages.\n        \n        Args:\n            stage (str, optional): Stage for which the setup is being run. \n                                   If 'fit', set up for training and validation. \n                                   If 'test', set up for testing.\n        \"\"\"\n        if stage == \"fit\" or stage is None:\n            fashion_full = FashionMNIST(self.data_dir, train=True, transform=self.transform)\n            # Randomly split the dataset into training and validation data\n            self.fashion_train, self.fashion_val = random_split(fashion_full, [55000, 5000])\n\n        if stage == \"test\" or stage is None:\n            self.fashion_test = FashionMNIST(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        \"\"\"\n        Returns:\n            DataLoader: DataLoader for the training data.\n        \"\"\"\n        return DataLoader(self.fashion_train, batch_size=self.batch_size, num_workers=self.num_workers, persistent_workers=True)\n\n    def val_dataloader(self):\n        \"\"\"\n        Returns:\n            DataLoader: DataLoader for the validation data.\n        \"\"\"\n        return DataLoader(self.fashion_val, batch_size=self.batch_size, num_workers=self.num_workers, persistent_workers=True)\n\n    def test_dataloader(self):\n        \"\"\"\n        Returns:\n            DataLoader: DataLoader for the test data.\n        \"\"\"\n        return DataLoader(self.fashion_test, batch_size=self.batch_size, num_workers=self.num_workers)\n\n\n\nshow_img()\n\n\n\n\n\n\nGenerator Architecture\n\nThe Generator is designed to map latent space vectors to the data space. It consists of a series of layers that progressively upsample the input vector to a higher resolution, culminating in an image of the desired size.\n\nInput: Receives a latent vector of dimensionality latent_dim.\nLayers:\n\nA fully connected layer expands the latent vector into a 7x7x64 tensor.\nTwo transposed convolutional layers (also known as deconvolutional layers) further upsample the tensor to larger spatial dimensions (14x14 and 28x28).\nTwo batch normalisation layers, one after each convolutional layer.\nThe final convolutional layer reduces the depth to produce a single-channel image, typically representing a grayscale image.\n\n\n\n\n\nCode\nclass Generator(nn.Module):\n    \"\"\"\n    Generator class for a GAN, producing images from a latent space input.\n    \"\"\"\n    def __init__(self, latent_dim):\n        \"\"\"\n        Initializes the Generator model.\n        Args:\n            latent_dim (int): Dimensionality of the latent space vector.\n        \"\"\"\n        super().__init__()\n        self.lin1 = nn.Linear(latent_dim, 7 * 7 * 64)\n        self.ct1 = nn.ConvTranspose2d(64, 32, 4, stride=2)\n        self.ct2 = nn.ConvTranspose2d(32, 16, 4, stride=2)\n        self.conv = nn.Conv2d(16, 1, kernel_size=7)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.bn2 = nn.BatchNorm2d(32)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the Generator.\n        Args:\n            x (Tensor): Latent space input tensor.\n        Returns:\n            Tensor: Generated image tensor of shape [1, 28, 28].\n        \"\"\"\n        x = F.relu(self.lin1(x))\n        x = x.view(-1, 64, 7, 7)\n        x = self.bn1(x)\n        x = F.relu(self.ct1(x))\n        x = self.bn2(x)\n        x = F.relu(self.ct2(x))\n        x = self.conv(x)\n        return torch.tanh(x)\n\n\n\n\nDiscriminator Architecture\n\nThe Discriminator evaluates images, distinguishing between samples drawn from the training data and those generated by the generator. It is structured as a conventional convolutional neural network (CNN), which downsamples the input image to a scalar output that estimates the probability of the input being a real image.\n\nInput: Receives an image (real or generated).\nLayers:\n\nTwo convolutional layers with kernel size 5 for feature extraction, each followed by max pooling for spatial reduction.\nDropout layer after the second convolutional layer to prevent overfitting.\nTwo fully connected layers to output a probability score.\n\n\n\n\n\nCode\nclass Discriminator(nn.Module):\n    \"\"\"\n    Discriminator class for a GAN, distinguishing generated images from real images.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the Discriminator model.\n        \"\"\"\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 1)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the Discriminator.\n        Args:\n            x (Tensor): Input image tensor.\n        Returns:\n            Tensor: Probability tensor indicating the likelihood of the input being real.\n        \"\"\"\n        x = F.leaky_relu(F.max_pool2d(self.conv1(x), 2), negative_slope=0.2)\n        x = F.leaky_relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2), negative_slope=0.2)\n        x = x.view(-1, 320)  # Flatten the output for the dense layer\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return torch.sigmoid(x)\n\n\n\n\nCombining into GAN Class\n\n\nCode\nclass GAN(pl.LightningModule):\n    \"\"\"\n    A GAN class inheriting from PyTorch Lightning Module for generating and discriminating images.\n    \"\"\"\n    def __init__(self, latent_dim=100, lr=0.0002):\n        \"\"\"\n        Initializes the GAN model.\n        Args:\n            latent_dim (int): Dimensionality of the latent space.\n            lr (float): Learning rate for the optimizer.\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n        self.automatic_optimization = False\n\n        self.generator = Generator(latent_dim=self.hparams.latent_dim)\n        self.discriminator = Discriminator()\n        self.validation_z = torch.randn(6, self.hparams.latent_dim)  # Validation noise\n        self.generator_losses = []\n        self.discriminator_losses = []\n\n    def forward(self, z):\n        \"\"\"\n        Forward pass through the generator to create images from noise.\n        Args:\n            z (Tensor): A batch of random noise vectors.\n        Returns:\n            Tensor: Generated images.\n        \"\"\"\n        return self.generator(z)\n\n    def adversarial_loss(self, y_hat, y, is_real=True, is_discriminator=True, label_smoothing=0.1):\n        \"\"\"\n        Computes the binary cross-entropy loss for adversarial training with label smoothing if required. \n        Args:\n            y_hat (Tensor): Predicted probabilities.\n            y (Tensor): True labels.\n        Returns:\n            Tensor: Loss value.\n        \"\"\"\n        if is_discriminator and is_real:\n            smoothed_labels = (1.0 - label_smoothing) * y\n        else:\n            smoothed_labels = y\n        return F.binary_cross_entropy(y_hat, smoothed_labels)\n\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        Training logic for one epoch's step.\n        Args:\n            batch: The output of your DataLoader. A tuple (images, labels) in this case.\n            batch_idx (int): Integer displaying index of this batch.\n        Returns:\n            Dictionary: Training loss and log metrics.\n        \"\"\"\n        real_imgs, _ = batch\n        d_loss = self._train_discriminator(real_imgs)\n        g_loss = self._train_generator(real_imgs.size(0))\n        self.generator_losses.append(g_loss.item())\n        self.discriminator_losses.append(d_loss.item())\n        self.log_dict({'g_loss': g_loss, 'd_loss': d_loss})\n        return {'loss': d_loss, 'progress_bar': {'g_loss': g_loss, 'd_loss': d_loss}, 'log': {'g_loss': g_loss, 'd_loss': d_loss}}\n\n    def plot_learning_curves(self):\n        plt.figure(figsize=(10, 5))\n        plt.title(\"Generator and Discriminator Loss During Training\")\n        plt.plot(self.generator_losses, label=\"Generator Loss\")\n        plt.plot(self.discriminator_losses, label=\"Discriminator Loss\")\n        plt.xlabel(\"Training Steps\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\n    def _train_generator(self, batch_size):\n        \"\"\"\n        Handles the training of the generator.\n        Args:\n            batch_size (int): The size of the batch.\n        Returns:\n            Tensor: Generator loss.\n        \"\"\"\n        z = self._generate_noise(batch_size)\n        fake_imgs = self(z)\n        y_hat = self.discriminator(fake_imgs)\n        y = torch.ones(y_hat.shape, device=self.device)\n        g_loss = self.adversarial_loss(y_hat, y)\n        self.manual_backward(g_loss)\n        self.optimizers()[0].step()\n        self.optimizers()[0].zero_grad()\n        return g_loss\n\n    def _train_discriminator(self, real_imgs):\n        \"\"\"\n        Handles the training of the discriminator.\n        Args:\n            real_imgs (Tensor): Real images from the dataset.\n        Returns:\n            Tensor: Discriminator loss.\n        \"\"\"\n        # Train with real images\n        y_hat_real = self.discriminator(real_imgs)\n        y_real = torch.ones(y_hat_real.shape, device=self.device)\n        real_loss = self.adversarial_loss(y_hat_real, y_real)\n\n        # Train with fake images\n        z = self._generate_noise(real_imgs.size(0))\n        fake_imgs = self(z).detach()\n        y_hat_fake = self.discriminator(fake_imgs)\n        y_fake = torch.zeros(y_hat_fake.shape, device=self.device)\n        fake_loss = self.adversarial_loss(y_hat_fake, y_fake)\n\n        # Average losses for the discriminator\n        d_loss = (real_loss + fake_loss) / 2\n        self.manual_backward(d_loss)\n        self.optimizers()[1].step()\n        self.optimizers()[1].zero_grad()\n        return d_loss\n\n    def validation_step(self, batch, batch_idx):\n        # This is necessary if you have defined a validation dataloader\n        pass\n\n\n    def _generate_noise(self, batch_size):\n        \"\"\"\n        Generates a tensor of random noise.\n        Args:\n            batch_size (int): The size of the batch.\n        Returns:\n            Tensor: A batch of random noise vectors.\n        \"\"\"\n        return torch.randn(batch_size, self.hparams.latent_dim, device=self.device)\n\n    def configure_optimizers(self):\n        \"\"\"\n        Initializes and returns optimizers for generator and discriminator.\n        Returns:\n            List: List containing optimizers for generator and discriminator.\n        \"\"\"\n        lr = self.hparams.lr\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr)\n        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr)\n        return [opt_g, opt_d], []\n\n    def plot_imgs(self):\n        \"\"\"\n        Plots generated images to visualize progress using matplotlib.\n        \"\"\"\n        z = self.validation_z.to(self.generator.lin1.weight.device)\n        with torch.no_grad():  # Ensures that gradients are not calculated in the forward pass\n            sample_imgs = self(z).detach().cpu()  # Detach and move to CPU to avoid RuntimeError\n\n        sample_imgs = (sample_imgs + 1) / 2\n        print(f'Epoch: {self.current_epoch}')\n        fig = plt.figure(figsize=(10, 6))\n        for i in range(sample_imgs.size(0)):\n            ax = fig.add_subplot(2, 3, i+1)\n            ax.imshow(sample_imgs[i, 0, :, :], cmap='Greys_r')\n            ax.axis('off')\n        plt.show()\n\n\n\n\nTraining\n\nDuring training, the generator and discriminator contest with each other:\n\nThe Generator aims to fool the discriminator by generating increasingly convincing images.\nThe Discriminator strives to accurately classify real and generated images.\n\nThe system is trained using a minimax game strategy, optimizing both networks concurrently to improve their accuracy and robustness. This adversarial setup helps improve the generative quality of the images as the training progresses.\n\nThe following code cell creates instances of the FashionMNISTDataModule and GAN classes and then trains the model on the training data, using the jupyter magic command %%time to track the time taken.\n\n%%time\n\ndm = FashionMNISTDataModule()\nmodel = GAN(lr=5e-5)\n\ntrainer = pl.Trainer(\n    max_epochs=50,\n    check_val_every_n_epoch=1,  # Ensures validation happens and might trigger related hooks\n    logger=True,\n    enable_checkpointing=True,\n)\n\ntrainer.fit(model, dm)\n\nprint('\\nTraining Complete.\\n')\n\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name          | Type          | Params\n------------------------------------------------\n0 | generator     | Generator     | 358 K \n1 | discriminator | Discriminator | 21.4 K\n------------------------------------------------\n380 K     Trainable params\n0         Non-trainable params\n380 K     Total params\n1.520     Total estimated model params size (MB)\n`Trainer.fit` stopped: `max_epochs=50` reached.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Complete.\n\nCPU times: total: 5h 35min 24s\nWall time: 57min 18s\n\n\n\nmodel.plot_imgs()\n\nEpoch: 50\n\n\n\n\n\n\nmodel.plot_learning_curves()"
  },
  {
    "objectID": "posts/yt2/index.html",
    "href": "posts/yt2/index.html",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\ncolor_pal = sns.color_palette(\"mako\")\n\nfrom wordcloud import WordCloud\nfrom tqdm.notebook import tqdm  #Progress Bar\n\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom scipy.special import softmax\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch._utils\")\nprint(\"Python version:\")\n!python --version\n\nPython version:\nPython 3.11.4\nSentiment analysis is a standard application of natural language processing (NLP) in which a machine learning algorithm is trained to classify text as having either positive, negative or neutral emotional tone.\nAs a subfield of NLP, sentiment analysis is closely related to computational linguistics. Sentiment analysis has also been found to be useful in the implementation of recommender systems, in which an automated understanding of the emotional content of reviews proves to be crucial for accurate and personalized content recommendation.\nIn this post sentiment analysis is performed on a dataset of Amazon reviews using both NLTK’s VADER sentiment analysis tool and the Hugging Face transformer-based model RoBERTa."
  },
  {
    "objectID": "posts/yt2/index.html#eda",
    "href": "posts/yt2/index.html#eda",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "EDA",
    "text": "EDA\n\nax = df['Score'].value_counts().sort_index() \\\n    .plot(kind='bar',\n          title='Count of Reviews by Stars',\n          figsize=(10,5),\n          color = color_pal[3])\nax.set_xlabel('Review Stars')\nax.set_ylabel('Number of Reviews')\n\nText(0, 0.5, 'Number of Reviews')\n\n\n\n\n\n\n# Generating a worldcloud of all the text in the 'Text' column of the dataframe\n\ntext = ' '.join(df['Text'])\nwordcloud = WordCloud(width=800, height=400, background_color='white',colormap='mako').generate(text)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\n\n(-0.5, 799.5, 399.5, -0.5)\n\n\n\n\n\n\ndf['ReviewLength'] = df['Text'].apply(len)\n\nplt.figure(figsize=(10, 5))\nplt.hist(df['ReviewLength'], bins=50, color=color_pal[2], edgecolor='black')\nplt.title('Distribution of Review Lengths')\nplt.xlabel('Review Length (Number of Characters)')\nplt.ylabel('Number of Reviews')\nplt.show()\n\n\n\n\n\nvectorizer = CountVectorizer(stop_words='english', max_features=20)\nword_matrix = vectorizer.fit_transform(df['Text'])\nword_frequency = word_matrix.sum(axis=0)\n\nwords = vectorizer.get_feature_names_out()\ncounts = word_frequency.A1\n\nsorted_indices = counts.argsort()[::-1]\nwords = [words[i] for i in sorted_indices]\ncounts = counts[sorted_indices]\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=words, y=counts, palette='mako')\nplt.title('Top 20 Most Common Words in Reviews')\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45, ha='right');"
  },
  {
    "objectID": "posts/yt2/index.html#cleaning-text",
    "href": "posts/yt2/index.html#cleaning-text",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "Cleaning Text",
    "text": "Cleaning Text\n“br” is such a common word beacuse it is present in many of the reviews as a html tag: &lt;br&gt;&lt;/br&gt;\nWe can fix this using the BeautifulSoup library.\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\ndef clean_text(text):\n    # Remove HTML tags\n    soup = BeautifulSoup(str(text), 'html.parser')\n    cleaned_text = soup.get_text()\n\n    return cleaned_text\n\n\ndf['Text'] = df['Text'].apply(clean_text)\n\n\n# Generating a worldcloud of all the text in the 'Text' column of the dataframe\n\ntext = ' '.join(df['Text'])\nwordcloud = WordCloud(width=800, height=400, background_color='white',colormap='mako').generate(text)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\n\n(-0.5, 799.5, 399.5, -0.5)\n\n\n\n\n\n\nvectorizer = CountVectorizer(stop_words='english', max_features=20)\nword_matrix = vectorizer.fit_transform(df['Text'])\nword_frequency = word_matrix.sum(axis=0)\n\nwords = vectorizer.get_feature_names_out()\ncounts = word_frequency.A1\n\nsorted_indices = counts.argsort()[::-1]\nwords = [words[i] for i in sorted_indices]\ncounts = counts[sorted_indices]\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=words, y=counts, palette='mako')\nplt.title('Top 20 Most Common Words in Reviews')\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45, ha='right');"
  },
  {
    "objectID": "posts/yt2/index.html#basic-nltk",
    "href": "posts/yt2/index.html#basic-nltk",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "Basic NLTK",
    "text": "Basic NLTK\nNLTK stands for Natural Language Toolkit. It is a comprehensive library in Python that provides tools and resources for working text data. NLTK includes various modules and packages for tasks such as tokenization, stemming, tagging, parsing, and sentiment analysis, making it a valuable resource for NLP tasks.\n\nexample = df['Text'][50]\nprint(example)\n\nThis oatmeal is not good. Its mushy, soft, I don't like it. Quaker Oats is the way to go.\n\n\n\ntokens = nltk.word_tokenize(example)\ntokens[:10]\n\n['This', 'oatmeal', 'is', 'not', 'good', '.', 'Its', 'mushy', ',', 'soft']\n\n\n\ntagged = nltk.pos_tag(tokens)\ntagged[:10]\n\n[('This', 'DT'),\n ('oatmeal', 'NN'),\n ('is', 'VBZ'),\n ('not', 'RB'),\n ('good', 'JJ'),\n ('.', '.'),\n ('Its', 'PRP$'),\n ('mushy', 'NN'),\n (',', ','),\n ('soft', 'JJ')]\n\n\n\nentities = nltk.chunk.ne_chunk(tagged)\nentities.pprint()\n\n(S\n  This/DT\n  oatmeal/NN\n  is/VBZ\n  not/RB\n  good/JJ\n  ./.\n  Its/PRP$\n  mushy/NN\n  ,/,\n  soft/JJ\n  ,/,\n  I/PRP\n  do/VBP\n  n't/RB\n  like/VB\n  it/PRP\n  ./.\n  (ORGANIZATION Quaker/NNP Oats/NNPS)\n  is/VBZ\n  the/DT\n  way/NN\n  to/TO\n  go/VB\n  ./.)"
  },
  {
    "objectID": "posts/yt2/index.html#sentimentintensityanalyzer",
    "href": "posts/yt2/index.html#sentimentintensityanalyzer",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "SentimentIntensityAnalyzer",
    "text": "SentimentIntensityAnalyzer\nSentimentIntensityAnalyzer is a class in the NLTK library’s VADER module. It is designed for sentiment analysis and provides a pre-trained machine learning model to assess the sentiment of a piece of text by analyzing the intensity of positive, negative, and neutral sentiments.\nThis uses a “bag of words” approach in which: 1. Stop words are removed 2. Each word is scored, and combined to give a total score\n\nsia = SentimentIntensityAnalyzer()\n\n\nsia.polarity_scores('This is a positive sentence.')\n\n{'neg': 0.0, 'neu': 0.29, 'pos': 0.71, 'compound': 0.5994}\n\n\n\nsia.polarity_scores('This is a negative sentence.')\n\n{'neg': 0.529, 'neu': 0.286, 'pos': 0.186, 'compound': -0.5267}\n\n\n\nsia.polarity_scores('I am so happy!')\n\n{'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.6468}\n\n\n\nsia.polarity_scores('I hate this product')\n\n{'neg': 0.649, 'neu': 0.351, 'pos': 0.0, 'compound': -0.5719}\n\n\n\nexample\n\n\"This oatmeal is not good. Its mushy, soft, I don't like it. Quaker Oats is the way to go.\"\n\n\n\nsia.polarity_scores(example)\n\n{'neg': 0.22, 'neu': 0.78, 'pos': 0.0, 'compound': -0.5448}\n\n\n\n# Run the polarity score on the entire dataset\nres = {}\nfor i, row in tqdm(df.iterrows(), total=len(df)):\n    text = row['Text']\n    myid = row['Id']\n    res[myid] = sia.polarity_scores(text)\n\n\n\n\n\n# Convert res to a dataframe\nvaders = pd.DataFrame(res).T\nvaders = vaders.reset_index().rename(columns={'index':'Id'})\nvaders = vaders.merge(df,how='left')\n\n\n# Now we have sentiment score and metadata\nvaders\n\n\n\n\n\n\n\n\nId\nneg\nneu\npos\ncompound\nProductId\nUserId\nProfileName\nHelpfulnessNumerator\nHelpfulnessDenominator\nScore\nTime\nSummary\nText\nReviewLength\n\n\n\n\n0\n1\n0.000\n0.695\n0.305\n0.9441\nB001E4KFG0\nA3SGXH7AUHU8GW\ndelmartian\n1\n1\n5\n1303862400\nGood Quality Dog Food\nI have bought several of the Vitality canned d...\n263\n\n\n1\n2\n0.138\n0.862\n0.000\n-0.5664\nB00813GRG4\nA1D87F6ZCVE5NK\ndll pa\n0\n0\n1\n1346976000\nNot as Advertised\nProduct arrived labeled as Jumbo Salted Peanut...\n190\n\n\n2\n3\n0.091\n0.754\n0.155\n0.8265\nB000LQOCH0\nABXLMWJIXXAIN\nNatalia Corres \"Natalia Corres\"\n1\n1\n4\n1219017600\n\"Delight\" says it all\nThis is a confection that has been around a fe...\n509\n\n\n3\n4\n0.000\n1.000\n0.000\n0.0000\nB000UA0QIQ\nA395BORC6FGVXV\nKarl\n3\n3\n2\n1307923200\nCough Medicine\nIf you are looking for the secret ingredient i...\n219\n\n\n4\n5\n0.000\n0.552\n0.448\n0.9468\nB006K2ZZ7K\nA1UQRSCLF8GW1T\nMichael D. Bigham \"M. Wassir\"\n0\n0\n5\n1350777600\nGreat taffy\nGreat taffy at a great price. There was a wid...\n140\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n996\n0.026\n0.716\n0.257\n0.9788\nB006F2NYI2\nA1D3F6UI1RTXO0\nSwopes\n1\n1\n5\n1331856000\nHot & Flavorful\nBLACK MARKET HOT SAUCE IS WONDERFUL.... My hus...\n477\n\n\n996\n997\n0.000\n0.786\n0.214\n0.9309\nB006F2NYI2\nAF50D40Y85TV3\nMike A.\n1\n1\n5\n1328140800\nGreat Hot Sauce and people who run it!\nMan what can i say, this salsa is the bomb!! i...\n305\n\n\n997\n998\n0.000\n0.673\n0.327\n0.9634\nB006F2NYI2\nA3G313KLWDG3PW\nkefka82\n1\n1\n5\n1324252800\nthis sauce is the shiznit\nthis sauce is so good with just about anything...\n265\n\n\n998\n999\n0.063\n0.874\n0.062\n-0.0129\nB006F2NYI2\nA3NIDDT7E7JIFW\nV. B. Brookshaw\n1\n2\n1\n1336089600\nNot Hot\nNot hot at all. Like the other low star review...\n280\n\n\n999\n1000\n0.032\n0.928\n0.041\n-0.1027\nB006F2NYI2\nA132DJVI37RB4X\nScottdrum\n2\n5\n2\n1332374400\nNot hot, not habanero\nI have to admit, I was a sucker for the large ...\n563\n\n\n\n\n1000 rows × 15 columns"
  },
  {
    "objectID": "posts/yt2/index.html#plotting-vader-results",
    "href": "posts/yt2/index.html#plotting-vader-results",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "Plotting VADER Results",
    "text": "Plotting VADER Results\n\nax = sns.barplot(data=vaders,\n                 x = 'Score',\n                 y = 'compound',\n                 palette = color_pal)\nax.set_title('Compound Score by Amazon Star Review')\n\nText(0.5, 1.0, 'Compound Score by Amazon Star Review')\n\n\n\n\n\nUnsuprisingly there is a positive correlation between review score (stars out of five) and the compound score from VADER.\n\nfig, axs = plt.subplots(1, 3, figsize=(15,5))\nsns.barplot(data=vaders, x = 'Score', y = 'pos', ax=axs[0], palette = color_pal)\naxs[0].set_title('Positive Score')\nsns.barplot(data=vaders, x = 'Score', y = 'neu', ax=axs[1], palette = color_pal)\naxs[1].set_title('Neutral Score')\nsns.barplot(data=vaders, x = 'Score', y = 'neg', ax=axs[2], palette = color_pal)\naxs[2].set_title('Negative Score')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/ste1/index.html",
    "href": "posts/ste1/index.html",
    "title": "On projected particles passing through the same point - S1-Q9-03",
    "section": "",
    "text": "Imports\nimport numpy as np\nimport sympy as sp\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport matplotlib.patches as patches\nfrom IPython.display import display, Math, Image, HTML\n\n\n\nStatement of the problem\n\nI set \\(g = 10\\text{ms}^{-2}\\) when a numeric value is required.\nThis is Q9 from STEP1 03.\n\nhttps://stepdatabase.maths.org/database/index.html#\n\n\n\nPython Code Producing GIF\n# Define symbolic variables\nt, g, V, theta = sp.symbols('t g V theta')\n\n# Symbolic equations for x(t) and y(t)\nx_t = V * sp.cos(theta) * t\ny_t = V * sp.sin(theta) * t - 0.5 * g * t**2\n\n# Constants and parameters\ng_val = 10  # Acceleration due to gravity (m/s^2)\nV_val = 30    # Initial velocity in m/s\ntheta_val = np.radians(70)  # Launch angle in radians\n\n# Create lambdified functions for numerical computation\nx_num = sp.lambdify((t, V, theta, g), x_t.subs(g, g_val), 'numpy')\ny_num = sp.lambdify((t, V, theta, g), y_t.subs(g, g_val), 'numpy')\n\n# Calculate time of flight to the point where y(t)=0 again\nt_flight = 2 * V_val * np.sin(theta_val) / g_val\nt_points = np.linspace(0, t_flight, num=300)\nx_points = x_num(t_points, V_val, theta_val, g_val)\ny_points = y_num(t_points, V_val, theta_val, g_val)\n\n# Select a fixed point P on the trajectory at a specific time\nt_P = 0.25 * t_flight\nd = x_num(t_P, V_val, theta_val, g_val)\nh = y_num(t_P, V_val, theta_val, g_val)\n\n# Plotting the trajectory\nfig, ax = plt.subplots()\nax.set_aspect('equal')\nline, = ax.plot([], [], color = '#bd0019', label='Trajectory', zorder=1)\npoint, = ax.plot([], [], color='#4f0000', marker='o', zorder=2)  # Red point for the particle\nfixed_point, = ax.plot([d], [h], color='k', marker='o' , zorder=3)\nax.annotate(r'$P = (d, h)$', (d, h), textcoords=\"offset points\", xytext=(+30,-10), ha='center', color='k')\n\n# Display the projection angle and velocity vector as an arrow\nV_scale = 0.3  # Increased scale factor for better visibility\nvelocity_arrow = ax.arrow(0, 0, V_val * np.cos(theta_val) * V_scale, V_val * np.sin(theta_val) * V_scale,\n                          color='k', head_width=1, head_length=1, length_includes_head=True, alpha=0.6, zorder=4)\n\n# Draw the angle arc\narc = patches.Arc((0, 0), 10, 10, angle=0, theta1=0, theta2=np.degrees(theta_val), color='k', zorder=5)\nax.add_patch(arc)\nax.annotate(r'$\\theta$', (5 * np.cos(theta_val / 2), 5 * np.sin(theta_val / 2)), color='k', fontsize=12)\n\nax.annotate(r'$V$', (V_val * np.cos(theta_val) * V_scale, V_val * np.sin(theta_val) * V_scale),\n            textcoords=\"offset points\", xytext=(6,-10) , color='k', zorder=6)\n\n\nax.set_xlim(0, 65)\nax.set_ylim(0, 45)\nax.set_xlabel('Horizontal distance (m)')\nax.set_ylabel('Vertical distance (m)')\nax.grid(True)\n\n# Function to update the plot\ndef update(frame):\n    line.set_data(x_points[:frame], y_points[:frame])\n    point.set_data([x_points[frame]], [y_points[frame]])\n    return line, point,\n\n# Create and save the animation as a GIF\nani = FuncAnimation(fig, update, frames=len(t_points), interval=30)\nani.save('gif/trajectory.gif', writer='pillow', fps=30)  # Specify writer and fps\n\nplt.close(fig)\n\n# Display the GIF in the notebook\nHTML('&lt;img src=\"gif/trajectory.gif\" /&gt;')\n\n\n\n\n\nThe path of the particle is an inverted parabola. This is a general fact when air resistance is ignored.\n\n\nDeriving the quadratic for \\(T\\)\nClaim:\n\\[ T^2 -2kT + \\frac{2kh}{d} + 1 = 0 \\]\nwhere \\(T = \\tan \\theta\\) and \\(k = \\frac{V^2}{gd}\\).\n\nProof.\nThe idea is to write equations of motion for the \\(x\\) and \\(y\\) components of the particle’s position, and combine to eliminate the time \\(t\\). The claimed quadratic for \\(T = \\tan \\theta\\) then follows after grinding out some algebra.\nWe first use “\\(s = ut + \\frac{1}{2}at^2\\)” in the vertical direction. We have the following variables:\n\\[\n\\begin{cases}\n    &\\text{initial velocity} = V \\sin \\theta\\\\\n    &\\text{displacement} = h\\\\\n    &\\text{acceleration} = -g\\\\\n    &\\text{time} = t\n\\end{cases}\n\\] which result in the equation of motion \\[h = Vt \\sin \\theta - \\frac{1}{2}gt^2\\]\nNow we note that the particle is not accelerating in the \\(x\\)-direction, so all the SUVAT equations are trivial or reduce to “distance = speed \\(\\times\\) time”. Thus the equation of motion in the \\(x\\)-direction is\n\\[ d = Vt\\cos \\theta \\]\nThe particle’s motion is dictated by the pair of equations\n\\[\n\\begin{cases}\n    h = Vt \\sin \\theta - \\frac{1}{2}gt^2\\\\\n    d = Vt\\cos \\theta\n\\end{cases}\n\\]\nWe combine these into a single equation by eliminating the time \\(t\\). Rearranging the second equation gives:\n\\[ t = \\frac{d}{V\\cos\\theta} \\]\nSubstituting this into the first equation and simplifying:\n\\[\n\\begin{aligned}\nh &= V\\sin\\theta \\left(\\frac{d}{V\\cos\\theta}\\right) - \\frac{1}{2}g\\left(\\frac{d}{V\\cos\\theta}\\right)^2\\\\\n\\\\\n&= d \\tan \\theta -\\frac{gd^2}{2V^2}\\,\\sec^2\\theta\\\\\n\\\\\n&= d \\tan \\theta -\\frac{gd^2}{2V^2}\\left(1 + \\tan^2 \\theta \\right)\n\\end{aligned}\n\\]\nwhere we have used \\(\\tan^2 \\theta + 1 \\equiv \\sec^2 \\theta\\).\nTaking all terms to the left hand side and simplifying further:\n\\[\n\\begin{aligned}\n&\\frac{gd^2}{2V^2}\\tan^2\\theta - d\\tan \\theta + \\frac{gd^2}{2V^2} + h = 0\\\\\n\\\\\n\\implies &\\tan^2\\theta - \\frac{2V^2}{gd}\\tan\\theta + \\frac{2hV^2}{gd^2} + 1 = 0\\\\\n\\\\\n\\implies & T^2 -2k T + \\frac{2hk}{d} + 1 = 0\n\\end{aligned}\n\\]\nas required. \\(\\square\\)\n\n\n\nDeriving the inequality\nClaim:\nIf\n\\[kd &gt; h + \\sqrt{h^2 + d^2}\\]\nthen there are two possible angles of projection \\(\\theta\\) whos trajectory passes through the point \\(P = (d,h)\\).\n\nProof.\nThere are two distinct solutions for \\(\\theta\\) if and only if there are two distinct \\(\\tan\\theta\\) (since \\(\\theta\\) is acute).\nTherefore there are two distinct \\(\\theta\\) if and only if the quadratic\n\\[ T^2 -2kT + \\frac{2kh}{d} + 1 = 0 \\]\nhas two distinct real roots. This occurs if and only if the dicriminant “\\(\\Delta = b^2 - 4ac\\)” is positive, \\(\\Delta &gt; 0\\).\nThe quadratic has coefficients\n\\[\n\\begin{cases}\na = 1 \\\\\nb = -2k \\\\\nc = \\frac{2kh}{d} + 1\n\\end{cases}\n\\]\nThus the discriminant \\(\\Delta\\) is\n\\[\n\\begin{aligned}\n\\Delta &= (-2k)^2 - 4\\left(\\frac{2kh}{d} + 1\\right)\\\\\n\\\n&= 4\\left(k^2 - \\frac{2hk}{d} - 1 \\right)\n\\end{aligned}\n\\]\nHence\n\\[\n\\begin{aligned}\nkd &gt; h + \\sqrt{h^2 + d^2} &\\implies kd - h &gt; \\sqrt{h^2 + d^2}\\\\\n                          \\\\\n                          &\\implies (kd-h)^2 &gt; h^2 + d^2 \\\\\n                          \\\\\n                          &\\implies k^2d^2 -2kdh + h^2 &gt; h^2 + d^2\\\\\n                          \\\\\n                          &\\implies k^2d^2 -2kdh - d^2 &gt; 0 \\\\\n                          \\\\\n                          &\\implies k^2 - \\frac{2hk}{d} - 1 &gt; 0 \\\\\n                          \\\\\n                          &\\implies \\Delta &gt; 0  \\\\\n                          \\\\\n                          &\\implies \\text{There are two distinct } \\theta.\n\\end{aligned}\n\\]\nWhich is as required.\nNote that it is fine to square both sides of the inequality in the second line above as both sides are positive, and \\(x \\mapsto x^2\\) is strictly increasing on \\((0, \\infty)\\).\n\\(\\square\\)\n\nI really feel that there should be an geometric or physical meaning behind the requirement \\(kd &gt; h + \\sqrt{h^2 + d^2}\\) but I haven’t thought of one.\nIt seems like a weirdly specific form for an inquality that is not used again in the question.\n\\(\\sqrt{h^2 + d^2}\\) is the length of the straight line connecting the particle to the origin.\n\n\nDeriving the formula relating \\(\\alpha\\) and \\(\\beta\\)\nClaim:\nSuppose we are in the case \\(kd &gt; h + \\sqrt{h^2 + d^2}\\) as above, and denote by \\(\\alpha\\) and \\(\\beta\\) the two (acute) angles of projection at which a projected particle will pass through the point \\(P = (d,h)\\). Then\n\\[\n\\alpha + \\beta = \\pi - \\arctan \\left(\\frac{d}{h}\\right)\n\\]\n\nProof.\n\\(\\tan\\alpha\\) and \\(\\tan\\beta\\) can be found by solving the quadratic\n\\[ T^2 -2kT + \\frac{2kh}{d} + 1 = 0 .\\]\nUse of the quadratic formula gives\n\\[ T = k \\pm \\sqrt{k^2 - \\frac{2kh}{d} - 1} \\]\nTherefore\n\\[\n\\begin{cases}\n\\tan \\alpha = k + \\sqrt{k^2 - \\frac{2kh}{d} - 1}\\\\\n\\\\\n\\tan \\beta = k - \\sqrt{k^2 - \\frac{2kh}{d} - 1}\n\\end{cases}\n\\]\nRecall the compound angle formula for \\(\\tan\\)\n\\[\\tan (A + B) = \\frac{\\tan A + \\tan B}{1 - \\tan A \\tan B} \\]\nIt is immediate that \\(\\tan \\alpha + \\tan \\beta = 2k\\). Meanwhile it is not much harder to compute that\n\\[\n\\begin{aligned}\n\\tan \\alpha \\tan \\beta &= \\left( k + \\sqrt{k^2 - \\frac{2kh}{d} - 1} \\right) \\left( k - \\sqrt{k^2 - \\frac{2kh}{d} - 1} \\right) \\\\\n                       \\\\\n                       &= k^2 - \\left(k^2 - \\frac{2kh}{d} - 1\\right) \\\\\n                       \\\\\n                       &= \\frac{2kh}{d} + 1\n\\end{aligned}\n\\]\nSubsituting these into the compound angle formula gives\n\\[ \\tan (\\alpha + \\beta) = \\frac{2k}{-2kh/d} = -\\frac{d}{h}. \\]\nWe have to be careful when taking the inverse tangent here.\nBoth \\(\\alpha, \\beta \\in \\left(0, \\frac{\\pi}{2}\\right)\\) so \\(\\alpha + \\beta \\in (0, \\pi)\\).\nTaking \\(\\arctan\\) of \\(-d/h\\) will return a negative value for \\(\\alpha + \\beta\\), which is unphysical. Thus we must add \\(\\pi\\) to the inverse tangent in order to return the positive, physical value in the correct range corresponding to acute \\(\\alpha\\) and \\(\\beta\\). (See the below graph for an illustration of this).\nHence\n\\[\n\\begin{aligned}\n\\alpha + \\beta &= \\pi + \\arctan\\left( -\\frac{d}{h} \\right) \\\\\n                \\\\\n               &= \\pi -  \\arctan\\left( \\frac{d}{h} \\right)\n\\end{aligned}\n\\]\nwhere in the final line we have used \\(\\arctan (-x) = - \\arctan x\\).\n\\(\\square\\)\nClaim: \\[ \\arctan (-x) = - \\arctan(x) \\]\n\nProof.\nDefine\n\\[\nf(x) = \\arctan (x) + \\arctan (-x)\n\\]\nIt suffices to show \\(f\\) is equal to zero for all \\(x\\). Using the chain rule\n\\[\n\\begin{aligned}\nf'(x) &= \\frac{1}{1+x^2} + \\frac{1}{1+x^2} \\frac{\\text{d}}{\\text{d}x}(-x)\\\\\n      &=  \\frac{1}{1+x^2} - \\frac{1}{1+x^2} \\\\\n      &= 0\n\\end{aligned}\n\\]\nfor all \\(x\\). So \\(f\\) must be a constant function, as it has \\(f' = 0\\) everywhere.\n\\[\nf(x) = C\n\\] for some \\(C\\in\\mathbb{R}\\). To find \\(C\\) simply evaluate \\(f\\) at zero:\n\\[\nC = \\arctan 0 + \\arctan (-0) = 2 \\arctan 0 = 0\n\\]\nTherefore \\(\\arctan x + \\arctan (-x) = 0\\) for all values of \\(x\\).\n\\(\\square\\)\n\n\n\nPython Code Producing Graph\n# Constants\nc = 6.13\nepsilon = 0.1\nx_interval_1 = np.linspace(-np.pi-1, -np.pi/2 - epsilon, 200)\nx_interval_2 = np.linspace(-np.pi/2 + epsilon, np.pi/2 - epsilon, 200)\nx_interval_3 = np.linspace(np.pi/2 + epsilon, np.pi+1, 200)\n\nplt.figure(figsize=(8, 6))\n\nplt.plot(x_interval_1, np.tan(x_interval_1), 'b')\nplt.plot(x_interval_2, np.tan(x_interval_2), 'b')\nplt.plot(x_interval_3, np.tan(x_interval_3), 'b', label=r'$y = \\tan(x)$')\n\nplt.axhline(-c, color='red', linestyle='--', label=r'$y = -d/h$')\n\nticks = np.arange(-np.pi, np.pi + np.pi/4, step=np.pi/4)\ntick_labels = [rf'{n:.2g}$\\pi$' if n != 0 else '' for n in ticks/np.pi]\nplt.xticks(ticks, tick_labels)\n\nax = plt.gca()\nax.spines['left'].set_position(('data', 0))\nax.spines['bottom'].set_position(('data', 0))\nax.spines['left'].set_linewidth(2)\nax.spines['bottom'].set_linewidth(2)\nax.spines['right'].set_color('none')  # Hide the right spine\nax.spines['top'].set_color('none')  # Hide the top spine\nax.xaxis.set_ticks_position('bottom')\nax.yaxis.set_ticks_position('left')\n\n\n# Eliminate upper and right axes\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\n# Show ticks in the left and lower axes only\nax.xaxis.set_ticks_position('bottom')\nax.yaxis.set_ticks_position('left')\n\n# Remove 0.0 y-tick\nyticks = ax.get_yticks()\nyticks = [ytick for ytick in yticks if ytick != 0.0]\nax.set_yticks(yticks)\n\n# Calculate the x values where y=-c intersects y=tan(x)\nx0 = np.arctan(-c)\nx1 = x0 + np.pi\n\n# Plot these points\nplt.plot(x0, -c, color='#004700', marker='o')\nplt.plot(x1, -c, color='#6c2100', marker='o')\n\n# Annotate these points\nplt.annotate('$x_0$', (x0, -c), textcoords=\"offset points\", xytext=(-10,-10), ha='center', color='#004700')\nplt.annotate('$x_1$', (x1, -c), textcoords=\"offset points\", xytext=(-10,-10), ha='center', color='#6c2100')\n\n# Shade the region 0&lt;=x&lt;=pi with a very low opacity\nplt.fill_betweenx(np.arange(-10, 10, 0.1), 0, np.pi, color='#000f00', alpha=0.1, label = r'Physical Region')\n\nplt.ylim(-10, 10)\nplt.xlim(-np.pi/2 - 0.5, np.pi + 0.5)\n\nplt.legend(loc='upper right')\n\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\nIn the above graph, the point \\(x_0\\) corresponds to taking \\(\\arctan\\) of \\(-d/h\\) without any adjustments. We can see that this would result in \\(\\alpha + \\beta &lt; 0\\) and therefore is a non-physical solution. In coordinates \\[ x_0 = \\left( \\arctan\\left(-\\frac{d}{h}\\right), -\\frac{d}{h} \\right) \\]\nThe point \\(x_1\\) corresponds to adding \\(\\pi\\) onto \\(x_0\\). This point is withing the shaded region \\(0 &lt; x &lt; \\pi\\) in which we require \\(\\alpha + \\beta\\) to be in if both \\(\\alpha\\) and \\(\\beta\\) are acute. In coordinates \\[ x_1 = \\left( \\pi + \\arctan\\left(-\\frac{d}{h}\\right), -\\frac{d}{h} \\right) \\]\n\n\n\nAn explicit example of two trajectories passing through a specific \\(P\\)\nChoose\n\n\\(\\alpha = 30^\\circ = \\frac{\\pi}{6}\\)\n\\(V = 30\\text{ms}^{-1}\\)\n\\(g = 10\\text{ms}^{-2}\\)\n\nThe equations of motion of the particle are:\n\\[\n\\begin{cases}\n    y(t) = 15t - 5t^2\\\\\n    x(t) = 15t\\sqrt{3}\n\\end{cases}\n\\]\nRearranging the second for \\(t\\) and using this to eliminate \\(t\\) in the first equation results at the cartesian equation of the trajectory of the particle\n\\[\ny(x) = \\frac{x}{\\sqrt{3}} - \\frac{x^2}{135}.\n\\]\nIf we then choose \\(d = 6\\sqrt{3}\\) then the corresponding \\(h\\) resulting in a point \\(P\\) lying on this trajectory can be calculated:\n\\[\nh = y(x=6\\sqrt{3}) = 5.2\n\\]\nSo we are looking for another angle of projection \\(\\beta\\) that results in a second trajectory also passing through \\(P = (6\\sqrt{3}, 5.2).\\)\n\n\nPython Code Producing GIF\nV = 30\ng = 10\nalpha_val = np.radians(30)\nd = 12*np.sqrt(3)\nh = 8.8\n\n# Define symbolic variables\nt, g, V, alpha = sp.symbols('t g V theta')\n\n# Symbolic equations for x(t) and y(t)\nx_t = V * sp.cos(alpha) * t\ny_t = V * sp.sin(alpha) * t - 0.5 * g * t**2\n\n\n# Create lambdified functions for numerical computation\nx_num = sp.lambdify((t, V, alpha, g), x_t.subs(g, g_val), 'numpy')\ny_num = sp.lambdify((t, V, alpha, g), y_t.subs(g, g_val), 'numpy')\n\n# Calculate time of flight to the point where y(t)=0 again\nt_flight = 2 * V_val * np.sin(alpha_val) / g_val\nt_points = np.linspace(0, t_flight, num=300)\nx_points = x_num(t_points, V_val, alpha_val, g_val)\ny_points = y_num(t_points, V_val, alpha_val, g_val)\n\n# Plotting the trajectory\nfig, ax = plt.subplots()\nline, = ax.plot([], [], color= '#bd0019', label=r'$\\alpha = 30\\degree$', zorder=1)\npoint, = ax.plot([], [], color='#4f0000', marker='o', zorder=2)  # Red point for the particle\nfixed_point, = ax.plot([d], [h], color='k', marker='o' , zorder=3) \nax.annotate(r'$P = (6\\sqrt{3}, 8.8)$', (d, h), textcoords=\"offset points\", xytext=(+45,-15), ha='center', color='k')\n\nax.set_xlim(0, 80)\nax.set_ylim(0, 50)\nax.set_xlabel('Horizontal distance (m)')\nax.set_ylabel('Vertical distance (m)')\nax.set_title(r'A single trajectory with $V = 30$ms$^{-1}$ and $\\alpha = 30\\degree$')\nax.grid(True)\nax.legend()\n\n# Function to update the plot\ndef update(frame):\n    line.set_data(x_points[:frame], y_points[:frame])\n    point.set_data([x_points[frame]], [y_points[frame]])\n    return line, point,\n\n# Create and save the animation as a GIF\nani = FuncAnimation(fig, update, frames=len(t_points), interval=30)\nani.save('gif/trajectory2.gif', writer='pillow', fps=30)  # Specify writer and fps\n\nplt.close(fig)\n\n# Display the GIF in the notebook\nHTML('&lt;img src=\"gif/trajectory2.gif\" /&gt;')\n\n\n\n\n\nTo determine the existence of \\(\\beta\\) we need to verify the inequality \\(kd &gt; h + \\sqrt{h^2 + d^2}\\).\n\\[\nkd = \\frac{V^2}{g} = \\frac{900}{10} = 90\n\\]\n\\[\nh + \\sqrt{h^2 + d^2} \\approx 16.82\n\\]\nThe inequality \\(kd &gt; h + \\sqrt{h^2 + d^2}\\) is satisfied so there is another angle of projection \\(\\beta\\) resulting in a trajectory passing through \\(P\\). In general \\(\\beta\\) is given by the expression\n\\[\n\\beta = \\pi - \\alpha - \\arctan\\left(\\frac{d}{h}\\right)\n\\]\nIn our case we have \\(\\alpha = \\pi / 3\\) which leads to \\(\\beta\\) exactly given by\n\\[\n\\beta = \\frac{5\\pi}{6} - \\arctan \\left(\\frac{15\\sqrt{3}}{16} \\right)\n\\]\nand \\(\\beta\\) approximately given by\n\\[\n\\begin{aligned}\n\\beta &\\approx 1.511 \\text{ rad}\\\\\n      &\\approx 86.57 ^\\circ\n\\end{aligned}\n\\]\n\n\nPython Code Producing GIF\n# Constants\nV_val = 30\ng_val = 10\nalpha_val = np.radians(30)\nd = 12 * np.sqrt(3)\nh = 8.8\nbeta_val = np.pi - alpha_val - np.arctan(d / h)\n\n# Define symbolic variables\nt, g, V, alpha = sp.symbols('t g V theta')\n\n# Symbolic equations for x(t) and y(t)\nx_t = V * sp.cos(alpha) * t\ny_t = V * sp.sin(alpha) * t - 0.5 * g * t**2\n\n# Create lambdified functions for numerical computation\nx_num = sp.lambdify((t, V, alpha, g), x_t, 'numpy')\ny_num = sp.lambdify((t, V, alpha, g), y_t, 'numpy')\n\n# Calculate time of flight for both trajectories\nt_flight_alpha = 2 * V_val * np.sin(alpha_val) / g_val\nt_flight_beta = 2 * V_val * np.sin(beta_val) / g_val\n\n# Time points for animation\nt_points_alpha = np.linspace(0, t_flight_alpha, num=300)\nt_points_beta = np.linspace(0, t_flight_beta, num=300)\n\nx_points_alpha = x_num(t_points_alpha, V_val, alpha_val, g_val)\ny_points_alpha = y_num(t_points_alpha, V_val, alpha_val, g_val)\n\nx_points_beta = x_num(t_points_beta, V_val, beta_val, g_val)\ny_points_beta = y_num(t_points_beta, V_val, beta_val, g_val)\n\n# Plotting the trajectories\nfig, ax = plt.subplots()\nax.set_aspect('equal')\n\n# Lines for trajectories\nline_alpha, = ax.plot([], [], color = '#bd0019', label=r'$\\alpha = 30\\degree$', zorder=1)\nline_beta, = ax.plot([], [], color='#0380a7', label=r'$\\beta \\approx 86.6\\degree$', zorder=1)\n\n# Points for the particle positions\npoint_alpha, = ax.plot([], [], color='#4f0000', marker='o', zorder=2)  # Red point for the alpha particle\npoint_beta, = ax.plot([], [], color='#0f7418', marker='o', zorder=2)  # Magenta point for the beta particle\n\nfixed_point, = ax.plot([d], [h], color='k', marker='o' , zorder=3) \nax.annotate(r'$P = (6\\sqrt{3}, 8.8)$', (d, h), textcoords=\"offset points\", xytext=(+45,-15), ha='center', color='k')\n\nax.set_xlim(0, 80)\nax.set_ylim(0, 50)\nax.set_xlabel('Horizontal distance (m)')\nax.set_ylabel('Vertical distance (m)')\nax.set_title(r'Two trajectories with $V = 30$ms$^{-1}$ passing through $P$')\nax.grid(True)\nax.legend()\n\n# Function to update the plot\ndef update(frame):\n    frame_alpha = min(frame, len(x_points_alpha)-1)\n    frame_beta = min(frame, len(x_points_beta)-1)\n    \n    line_alpha.set_data(x_points_alpha[:frame], y_points_alpha[:frame])\n    point_alpha.set_data([x_points_alpha[frame_alpha]], [y_points_alpha[frame_alpha]])\n\n    line_beta.set_data(x_points_beta[:frame], y_points_beta[:frame])\n    point_beta.set_data([x_points_beta[frame_beta]], [y_points_beta[frame_beta]])\n\n    return line_alpha, point_alpha, line_beta, point_beta,\n\n# Create and save the animation as a GIF\nani = FuncAnimation(fig, update, frames=max(len(t_points_alpha), len(t_points_beta)), interval=30)\nani.save('gif/trajectory3.gif', writer='pillow', fps=30)  # Specify writer and fps\n\nplt.close(fig)\n\n# Display the GIF in the notebook\nHTML('&lt;img src=\"gif/trajectory3.gif\" /&gt;')"
  },
  {
    "objectID": "posts/ng2/index.html",
    "href": "posts/ng2/index.html",
    "title": "Deep Neural Networks in NumPy with BGD and Adam Optimizers",
    "section": "",
    "text": "import numpy as np\nfrom utils import *\nversions()\n\n+-----------+---------+\n| Component | Version |\n+-----------+---------+\n|   Python  |  3.12.2 |\n+-----------+---------+\n|   NumPy   |  1.26.4 |\n+-----------+---------+"
  },
  {
    "objectID": "posts/ng2/index.html#fitting-bgd-model",
    "href": "posts/ng2/index.html#fitting-bgd-model",
    "title": "Deep Neural Networks in NumPy with BGD and Adam Optimizers",
    "section": "Fitting BGD Model",
    "text": "Fitting BGD Model\nWe use a 4-layer neural network with the architecture \\(12288 \\rightarrow 20 \\rightarrow 7 \\rightarrow 5 \\rightarrow 1\\).\nNote that the input layer needs to have \\(12288\\) neurons as our data is \\(12288\\) dimensional:\n\\[64\\times64\\times3=12288\\]\n\n### Architecture ###\ndims = [12288, 20, 7, 5, 1] #  4-layer model\n\n\nmodel_gd = L_Layer_NN_GradientDescent(layer_dims=dims,\n                                      learning_rate=0.0075)\n\n\nmodel_gd.fit(train_x,\n             train_y, \n             num_iterations=2000,\n             print_cost=True)\n\nCost after iteration 0: 0.7062343014430913\nCost after iteration 100: 0.6404381634500909\nCost after iteration 200: 0.6229251672354292\nCost after iteration 300: 0.5600817276725278\nCost after iteration 400: 0.47732634302936927\nCost after iteration 500: 0.4647746569317722\nCost after iteration 600: 0.4156415884654418\nCost after iteration 700: 0.3626064263804877\nCost after iteration 800: 0.30529033046829496\nCost after iteration 900: 0.25001177974445454\nCost after iteration 1000: 0.24070219047020605\nCost after iteration 1100: 0.08646489850870899\nCost after iteration 1200: 0.060508104342182324\nCost after iteration 1300: 0.04040595887122394\nCost after iteration 1400: 0.02855421388486012\nCost after iteration 1500: 0.021288159964999314\nCost after iteration 1600: 0.016496164189856312\nCost after iteration 1700: 0.013195757354987547\nCost after iteration 1800: 0.010832676095288549\nCost after iteration 1900: 0.009090714076649286\nCost after iteration 1999: 0.007753124670288281\n\n\n\nmodel_gd.learning_curve()\n\n\n\n\n\nprint('Accuracy on training data:\\n')\npredictions_train = model_gd.predict(train_x, train_y)\n\nAccuracy on training data:\n\nAccuracy: 100.00%\n\n\n\nprint('Accuracy on test data:\\n')\npredictions_test = model_gd.predict(test_x, test_y)\n\nAccuracy on test data:\n\nAccuracy: 72.00%\n\n\n\n## helper function from `utils.py` ##\nprint_mislabeled_images(classes, test_x, test_y, predictions_test, number=10)"
  },
  {
    "objectID": "posts/ng2/index.html#the-adam-optimizer",
    "href": "posts/ng2/index.html#the-adam-optimizer",
    "title": "Deep Neural Networks in NumPy with BGD and Adam Optimizers",
    "section": "The Adam Optimizer",
    "text": "The Adam Optimizer\nThe Adam (Adaptive Moment Estimation) Optimizer is a popular optimization algorithm used in deep learning models. It combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\nAdam’s method is computationally efficient and has little memory requirement. It is invariant to diagonal rescale of the gradients, and it is well suited for problems that are large in terms of data and/or parameters. The method is straightforward to implement and has been proven to work well in practice.\nThe implementation of the Adam optimizer is as follows:\n\nCompute the gradient: The first step in the process is to compute the gradient of the loss function with respect to each weight in the network. This gradient, \\(g_t\\), is computed using backpropagation.\n\\[g_t = \\nabla_\\theta f_t(\\theta)\\]\nwhere \\(\\theta\\) denotes the parameters of the model (i.e. the weights and biases of the network).\nUpdate biased first moment estimate: The first moment estimate, \\(m_t\\), is an exponentially decaying average of past gradients. The decay rate is controlled by the hyperparameter \\(\\beta_1\\in [0,1)\\).\n\\[m_t = \\beta_1 * m_{t-1} + (1 - \\beta_1) * g_t\\]\nUpdate biased second raw moment estimate: The second moment estimate, \\(v_t\\), is an exponentially decaying average of past squared gradients. The decay rate is controlled by the hyperparameter \\(\\beta_2\\in [0,1)\\).\n\\[v_t = \\beta_2 * v_{t-1} + (1 - \\beta_2) * g_t^2\\]\nCompute bias-corrected estimates: The first and second moment estimates are biased towards zero in the initial time steps. To correct this bias, we compute the bias-corrected first moment estimate, \\(\\hat{m}_t\\), and the bias-corrected second raw moment estimate, \\(\\hat{v}_t\\).\n\\[\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\\] \\[\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\\]\nUpdate weights: Finally, we update the weights of our neural network. Each weight, \\(\\theta\\), is updated by subtracting a fraction of the bias-corrected first moment estimate. This fraction is determined by the learning rate, \\(\\alpha &gt; 0\\). The denominator is the square root of the bias-corrected second raw moment estimate plus a small constant, \\(\\varepsilon &lt;&lt; 1\\), to prevent division by zero.\n\\[\\theta \\leftarrow \\theta - \\alpha * \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}\\]\n\nThe default values of the hyperparameters \\(\\beta_1\\), \\(\\beta_2\\) and \\(\\varepsilon\\) are\n\n\\(\\beta_1 = 0.9\\)\n\\(\\beta_2 = 0.999\\)\n\\(\\varepsilon = 10^{-8}\\)\n\nwhich work well for most problems.\nFor more details on the theory and implementation of the Adam optimzer see Wikipedia:\n\nhttps://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam\n\n\n\nCode\nclass L_Layer_NN_AdamOptimizer(BaseModel):\n    \"\"\"\n    A neural network model with L layers trained using Adam optimization.\n\n    Inherits from BaseModel.\n\n    Attributes:\n    layer_dims (list): List containing the dimensions of each layer in the network.\n    learning_rate (float): The learning rate of the Adam optimization.\n    beta1 (float): The exponential decay rate for the first moment estimates.\n    beta2 (float): The exponential decay rate for the second moment estimates.\n    epsilon (float): A small constant for numerical stability.\n    \"\"\"\n\n    def __init__(self, layer_dims, learning_rate=0.0075, beta1=0.9, beta2=0.999, epsilon=1e-8):\n        \"\"\"\n        Initializes the L_Layer_NN_Adam model with given layer dimensions and learning rate.\n\n        Args:\n        layer_dims (list): List containing the dimensions of each layer in the network.\n        learning_rate (float, optional): The learning rate of the Adam optimization. Default is 0.0075.\n        beta1 (float, optional): The exponential decay rate for the first moment estimates. Default is 0.9.\n        beta2 (float, optional): The exponential decay rate for the second moment estimates. Default is 0.999.\n        epsilon (float, optional): A small constant for numerical stability. Default is 1e-8.\n        \"\"\"\n        super().__init__(layer_dims)\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n\n    def _initialize_adam(self, parameters):\n        \"\"\"\n        Initializes v and s as two python dictionaries with:\n                    - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                    - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n        \n        Arguments:\n        parameters -- python dictionary containing your parameters.\n                        parameters[\"W\" + str(l)] = Wl\n                        parameters[\"b\" + str(l)] = bl\n        \n        Returns: \n        v -- python dictionary that will contain the exponentially weighted average of the gradient. Initialized with zeros.\n                        v[\"dW\" + str(l)] = ...\n                        v[\"db\" + str(l)] = ...\n        s -- python dictionary that will contain the exponentially weighted average of the squared gradient. Initialized with zeros.\n                        s[\"dW\" + str(l)] = ...\n                        s[\"db\" + str(l)] = ...\n\n        \"\"\"\n        L = len(self.layer_dims)\n        v = {}\n        s = {}\n        \n        for l in range(1, L):\n            v[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n            v[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)\n            s[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n            s[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)\n            \n        return v, s\n\n    def _update_parameters_with_adam(self, parameters, grads, v, s, t):\n        \"\"\"\n        Update parameters using Adam optimization.\n\n        Args:\n        parameters (dict): Python dictionary containing the parameters.\n        grads (dict): Python dictionary containing the gradients, output of L_model_backward.\n        v (dict): Adam variable, moving average of the first gradient, python dictionary.\n        s (dict): Adam variable, moving average of the squared gradient, python dictionary.\n        t (int): Adam variable, counts the number of taken steps.\n\n        Returns:\n        parameters (dict): Python dictionary containing the updated parameters.\n                           parameters[\"W\" + str(l)] = ...\n                           parameters[\"b\" + str(l)] = ...\n        v (dict): Adam variable, moving average of the first gradient, python dictionary.\n        s (dict): Adam variable, moving average of the squared gradient, python dictionary.\n        \"\"\"\n        L = len(parameters) // 2                 # number of layers in the neural networks\n        v_corrected = {}                         # Initializing first moment estimate, python dictionary\n        s_corrected = {}                         # Initializing second moment estimate, python dictionary\n        \n        # Perform Adam update on all parameters\n        for l in range(1, L):\n            v[\"dW\" + str(l)] = self.beta1 * v[\"dW\" + str(l)] + (1-self.beta1) * grads[\"dW\" + str(l)] \n            v[\"db\" + str(l)] = self.beta1 * v[\"db\" + str(l)] + (1-self.beta1) * grads[\"db\" + str(l)] \n\n            v_corrected[\"dW\" + str(l)] = v[\"dW\" + str(l)]/(1-self.beta1**t)\n            v_corrected[\"db\" + str(l)] = v[\"db\" + str(l)]/(1-self.beta1**t)\n\n            s[\"dW\" + str(l)] = self.beta2*s[\"dW\" + str(l)] + (1-self.beta2)*(grads['dW' + str(l)]*grads['dW' + str(l)])\n            s[\"db\" + str(l)] = self.beta2*s[\"db\" + str(l)] + (1-self.beta2)*(grads['db' + str(l)]*grads['db' + str(l)])\n\n            s_corrected[\"dW\" + str(l)] = s[\"dW\" + str(l)]/(1-self.beta2**t)\n            s_corrected[\"db\" + str(l)] = s[\"db\" + str(l)]/(1-self.beta2**t)\n\n            parameters[\"W\" + str(l)] -= self.learning_rate * np.divide(v_corrected[\"dW\" + str(l)], np.sqrt(s_corrected[\"dW\" + str(l)]) + self.epsilon)\n            parameters[\"b\" + str(l)] -= self.learning_rate * np.divide(v_corrected[\"db\" + str(l)], np.sqrt(s_corrected[\"db\" + str(l)]) + self.epsilon)\n\n        return parameters, v, s\n\n    def fit(self, X, Y, num_iterations=2000, print_cost=True):\n        \"\"\"\n        Implements an L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.\n\n        Args:\n        X (numpy.ndarray): Input data, of shape (n_x, number of examples).\n        Y (numpy.ndarray): True \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples).\n        num_iterations (int, optional): Number of iterations of the optimization loop. Default is 1000.\n        print_cost (bool, optional): If True, it prints the cost every 100 steps. Default is True.\n\n        Returns:\n        parameters (dict): Parameters learnt by the model. They can then be used to predict.\n        \"\"\"\n        costs = []\n        t = 0\n        parameters = self._initialise_parameters(self.layer_dims)\n        v, s = self._initialize_adam(parameters)\n    \n        for i in range(0, num_iterations):\n            t += 1\n            AL, caches = self._L_model_forward(X, parameters)\n            cost = self._compute_cost(AL, Y)\n            grads = self._L_model_backward(AL, Y, caches)\n            parameters, v, s = self._update_parameters_with_adam(parameters, grads, v, s, t)\n\n            if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n                print(f\"Cost after iteration {i}: {np.squeeze(cost)}\")\n            if i % 100 == 0:\n                costs.append(cost)\n\n        self.best_parameters = parameters\n        self.costs = costs"
  },
  {
    "objectID": "posts/ng2/index.html#fitting-adam-model-with-default-parameters",
    "href": "posts/ng2/index.html#fitting-adam-model-with-default-parameters",
    "title": "Deep Neural Networks in NumPy with BGD and Adam Optimizers",
    "section": "Fitting Adam Model with default parameters",
    "text": "Fitting Adam Model with default parameters\nAgain, we use a 4-layer neural network with the architecture \\(12288 \\rightarrow 20 \\rightarrow 7 \\rightarrow 5 \\rightarrow 1\\).\n\n### ARCHITECTURE ###\ndims = [12288, 20, 7, 5, 1] #  4-layer model\n\n\nmodel_adam = L_Layer_NN_AdamOptimizer(dims,\n                                      learning_rate=0.0075,\n                                      beta1=0.9,\n                                      beta2=0.999,\n                                      epsilon=1e-08,\n)\n\n\nmodel_adam.fit(train_x,\n               train_y,\n               num_iterations=2000,\n               print_cost=True)\n\nCost after iteration 0: 0.677432506089035\nCost after iteration 100: 0.6931451805619453\nCost after iteration 200: 0.6931451805619453\nCost after iteration 300: 0.6931451805619453\nCost after iteration 400: 0.6931451805619453\nCost after iteration 500: 0.6931451805619453\nCost after iteration 600: 0.6931451805619453\nCost after iteration 700: 0.6931451805619453\nCost after iteration 800: 0.6931451805619453\nCost after iteration 900: 0.6931451805619453\nCost after iteration 1000: 0.6931451805619453\nCost after iteration 1100: 0.6931451805619453\nCost after iteration 1200: 0.6931451805619453\nCost after iteration 1300: 0.6931451805619453\nCost after iteration 1400: 0.6931451805619453\nCost after iteration 1500: 0.6931451805619453\nCost after iteration 1600: 0.6931451805619453\nCost after iteration 1700: 0.6931451805619453\nCost after iteration 1800: 0.6931451805619453\nCost after iteration 1900: 0.6931451805619453\nCost after iteration 1999: 0.6931451805619453\n\n\n\nmodel_adam.learning_curve()\n\n\n\n\nThe optimizer seems to be making no progress at all. We certainly do not see convergence in the figure above.\nAdam might be getting trapped in a local minimum of the cost function. We can investigate further by reducing the learning rate \\(\\alpha\\)."
  },
  {
    "objectID": "posts/ng2/index.html#reducing-learning_rate",
    "href": "posts/ng2/index.html#reducing-learning_rate",
    "title": "Deep Neural Networks in NumPy with BGD and Adam Optimizers",
    "section": "Reducing learning_rate",
    "text": "Reducing learning_rate\nRepeating with learning_rate=0.0075 replaced with learning_rate=0.00075:\n\nmodel_adam = L_Layer_NN_AdamOptimizer(dims,\n                                      learning_rate=0.00075,\n                                      beta1=0.9,\n                                      beta2=0.999,\n                                      epsilon=1e-08,\n)\n\n\nmodel_adam.fit(train_x,\n               train_y,\n               num_iterations=2000,\n               print_cost=True)\n\nCost after iteration 0: 0.7338611614590664\nCost after iteration 100: 0.6193308404218576\nCost after iteration 200: 0.54815296945738\nCost after iteration 300: 0.5215827481065662\nCost after iteration 400: 0.501795625864784\nCost after iteration 500: 0.48482784356842423\nCost after iteration 600: 0.5124480583158955\nCost after iteration 700: 0.4523106134778213\nCost after iteration 800: 0.43338227634083676\nCost after iteration 900: 0.4185369325974699\nCost after iteration 1000: 0.4070917394995416\nCost after iteration 1100: 0.393033861083731\nCost after iteration 1200: 0.38478273430086796\nCost after iteration 1300: 0.3784207769398178\nCost after iteration 1400: 0.3723088150348833\nCost after iteration 1500: 0.36674669812415955\nCost after iteration 1600: 0.36113997520254826\nCost after iteration 1700: 0.35527077809021834\nCost after iteration 1800: 0.34978673486308376\nCost after iteration 1900: 0.3444941711355614\nCost after iteration 1999: 0.33937269527813974\n\n\n\nmodel_adam.learning_curve()\n\n\n\n\nThis is definitely an improvement over the first learning curve but still not as good as the results achieved by batch gradient descent."
  },
  {
    "objectID": "posts/ng2/index.html#reducing-beta1-and-increasing-epsilon",
    "href": "posts/ng2/index.html#reducing-beta1-and-increasing-epsilon",
    "title": "Deep Neural Networks in NumPy with BGD and Adam Optimizers",
    "section": "Reducing beta1 and increasing epsilon",
    "text": "Reducing beta1 and increasing epsilon\nReplacing beta1=0.9 with beta1=0.8.\nReplacing epsilon=1e-08 with epsilon=1e-04:\n\nmodel_adam = L_Layer_NN_AdamOptimizer(dims,\n                                      learning_rate=0.00075,\n                                      beta1=0.8,\n                                      beta2=0.999,\n                                      epsilon=1e-04,\n)\n\n\nmodel_adam.fit(train_x,\n               train_y,\n               num_iterations=2000,\n               print_cost=True)\n\nCost after iteration 0: 0.7084378508584687\nCost after iteration 100: 0.292130246866318\nCost after iteration 200: 0.13664169388368827\nCost after iteration 300: 0.07645948653035053\nCost after iteration 400: 0.04742691205157283\nCost after iteration 500: 0.031115162350943176\nCost after iteration 600: 0.02209212192755511\nCost after iteration 700: 0.01641684834702707\nCost after iteration 800: 0.010959383189569771\nCost after iteration 900: 0.007495424042212977\nCost after iteration 1000: 0.006330333559985924\nCost after iteration 1100: 0.005618136476453307\nCost after iteration 1200: 0.005143165972074695\nCost after iteration 1300: 0.004787370371286934\nCost after iteration 1400: 0.004535495041426705\nCost after iteration 1500: 0.0013435492833420732\nCost after iteration 1600: 0.0008856651799834186\nCost after iteration 1700: 0.0007068606565058238\nCost after iteration 1800: 0.0005917661104938714\nCost after iteration 1900: 0.0005080772404285796\nCost after iteration 1999: 0.0004430968047141927\n\n\n\nmodel_adam.learning_curve()\n\n\n\n\nThis looks much better. Not only is the Adam optimizer smoothly convering with these choices of hyperparameters but it is doing so much faster than batch gradient descent.\n\nprint('Accuracy on training data:\\n')\npredictions_train = model_adam.predict(train_x, train_y)\n\nAccuracy on training data:\n\nAccuracy: 100.00%\n\n\n\nprint('Accuracy on test data:\\n')\npredictions_test = model_adam.predict(test_x, test_y)\n\nAccuracy on test data:\n\nAccuracy: 72.00%"
  },
  {
    "objectID": "posts/ng2/index.html#reducing-num_iterations-to-counteract-overfitting",
    "href": "posts/ng2/index.html#reducing-num_iterations-to-counteract-overfitting",
    "title": "Deep Neural Networks in NumPy with BGD and Adam Optimizers",
    "section": "Reducing num_iterations to counteract overfitting",
    "text": "Reducing num_iterations to counteract overfitting\nNow that we have Adam smoothy and rapidly converging we can reduce num_iterations. The learning curve immediately above suggests that num_iterations=2000 is unnecessarily large and will surely result in overfitting from training for too long.\nReplacing num_iterations=2000 with num_iterations=500:\n\nmodel_adam = L_Layer_NN_AdamOptimizer(dims,\n                                      learning_rate=0.00075,\n                                      beta1=0.8,\n                                      beta2=0.999,\n                                      epsilon=1e-04,\n)\n\n\nmodel_adam.fit(train_x,\n               train_y,\n               num_iterations=500,\n               print_cost=True)\n\nCost after iteration 0: 0.6994950488552396\nCost after iteration 100: 0.26011356134545854\nCost after iteration 200: 0.06927856142422689\nCost after iteration 300: 0.02430204158752645\nCost after iteration 400: 0.011715179992492213\nCost after iteration 499: 0.006809646501698239\n\n\n\nmodel_adam.learning_curve()\n\n\n\n\n\nprint('Accuracy on training data:\\n')\npredictions_train = model_adam.predict(train_x, train_y)\n\nAccuracy on training data:\n\nAccuracy: 100.00%\n\n\n\nprint('Accuracy on test data:\\n')\npredictions_test = model_adam.predict(test_x, test_y)\n\nAccuracy on test data:\n\nAccuracy: 74.00%\n\n\n\nprint_mislabeled_images(classes, test_x, test_y, predictions_test, number=10)\n\n\n\n\n\n\nThe models seem to struggle with cats in unusual poses and at unusual scales (i.e. close to the camera).\nThey also seem to struggle when cats are a similar colour to their backgrounds.\nWe could improve the performance of the model by including more of such images in the training set, using data augmentation or otherwise.\nA convincing theory as for why the model seems to consistently misclassify moths and butterflies as cats is that they have a very similar shape to cat ears. (Credit to Eleanor)"
  },
  {
    "objectID": "posts/k2/index.html",
    "href": "posts/k2/index.html",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nimport xgboost as xgb\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\nimport optuna\nprint(\"Python version:\")\n!python --version\n\nPython version:\nPython 3.11.4\nprint(\"Optuna version:\")\nprint(optuna.__version__)\n\nOptuna version:\n3.5.0\nprint(\"TensorFlow version:\")\nprint(tf.__version__)\n\nTensorFlow version:\n2.14.0"
  },
  {
    "objectID": "posts/k2/index.html#section",
    "href": "posts/k2/index.html#section",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "—",
    "text": "—\n\ndf = pd.read_csv('lending_club_loan.csv')\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 396030 entries, 0 to 396029\nData columns (total 27 columns):\n #   Column                Non-Null Count   Dtype  \n---  ------                --------------   -----  \n 0   loan_amnt             396030 non-null  float64\n 1   term                  396030 non-null  object \n 2   int_rate              396030 non-null  float64\n 3   installment           396030 non-null  float64\n 4   grade                 396030 non-null  object \n 5   sub_grade             396030 non-null  object \n 6   emp_title             373103 non-null  object \n 7   emp_length            377729 non-null  object \n 8   home_ownership        396030 non-null  object \n 9   annual_inc            396030 non-null  float64\n 10  verification_status   396030 non-null  object \n 11  issue_d               396030 non-null  object \n 12  loan_status           396030 non-null  object \n 13  purpose               396030 non-null  object \n 14  title                 394274 non-null  object \n 15  dti                   396030 non-null  float64\n 16  earliest_cr_line      396030 non-null  object \n 17  open_acc              396030 non-null  float64\n 18  pub_rec               396030 non-null  float64\n 19  revol_bal             396030 non-null  float64\n 20  revol_util            395754 non-null  float64\n 21  total_acc             396030 non-null  float64\n 22  initial_list_status   396030 non-null  object \n 23  application_type      396030 non-null  object \n 24  mort_acc              358235 non-null  float64\n 25  pub_rec_bankruptcies  395495 non-null  float64\n 26  address               396030 non-null  object \ndtypes: float64(12), object(15)\nmemory usage: 81.6+ MB\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nloan_amnt\nterm\nint_rate\ninstallment\ngrade\nsub_grade\nemp_title\nemp_length\nhome_ownership\nannual_inc\n...\nopen_acc\npub_rec\nrevol_bal\nrevol_util\ntotal_acc\ninitial_list_status\napplication_type\nmort_acc\npub_rec_bankruptcies\naddress\n\n\n\n\n0\n10000.0\n36 months\n11.44\n329.48\nB\nB4\nMarketing\n10+ years\nRENT\n117000.0\n...\n16.0\n0.0\n36369.0\n41.8\n25.0\nw\nINDIVIDUAL\n0.0\n0.0\n0174 Michelle Gateway\\r\\nMendozaberg, OK 22690\n\n\n1\n8000.0\n36 months\n11.99\n265.68\nB\nB5\nCredit analyst\n4 years\nMORTGAGE\n65000.0\n...\n17.0\n0.0\n20131.0\n53.3\n27.0\nf\nINDIVIDUAL\n3.0\n0.0\n1076 Carney Fort Apt. 347\\r\\nLoganmouth, SD 05113\n\n\n2\n15600.0\n36 months\n10.49\n506.97\nB\nB3\nStatistician\n&lt; 1 year\nRENT\n43057.0\n...\n13.0\n0.0\n11987.0\n92.2\n26.0\nf\nINDIVIDUAL\n0.0\n0.0\n87025 Mark Dale Apt. 269\\r\\nNew Sabrina, WV 05113\n\n\n3\n7200.0\n36 months\n6.49\n220.65\nA\nA2\nClient Advocate\n6 years\nRENT\n54000.0\n...\n6.0\n0.0\n5472.0\n21.5\n13.0\nf\nINDIVIDUAL\n0.0\n0.0\n823 Reid Ford\\r\\nDelacruzside, MA 00813\n\n\n4\n24375.0\n60 months\n17.27\n609.33\nC\nC5\nDestiny Management Inc.\n9 years\nMORTGAGE\n55000.0\n...\n13.0\n0.0\n24584.0\n69.8\n43.0\nf\nINDIVIDUAL\n1.0\n0.0\n679 Luna Roads\\r\\nGreggshire, VA 11650\n\n\n\n\n5 rows × 27 columns"
  },
  {
    "objectID": "posts/k2/index.html#emp_title-and-emp_length",
    "href": "posts/k2/index.html#emp_title-and-emp_length",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "emp_title and emp_length",
    "text": "emp_title and emp_length\n\ndf['emp_title']\n\n0                        Marketing\n1                  Credit analyst \n2                     Statistician\n3                  Client Advocate\n4          Destiny Management Inc.\n                    ...           \n396025            licensed bankere\n396026                       Agent\n396027                City Carrier\n396028        Gracon Services, Inc\n396029    Internal Revenue Service\nName: emp_title, Length: 396030, dtype: object\n\n\n\ndf['emp_title'].nunique()\n\n173105\n\n\nemp_title has too many unique values to encode\n\ndf = df.drop('emp_title', axis=1)\n\n\ndf['emp_length']\n\n0         10+ years\n1           4 years\n2          &lt; 1 year\n3           6 years\n4           9 years\n            ...    \n396025      2 years\n396026      5 years\n396027    10+ years\n396028    10+ years\n396029    10+ years\nName: emp_length, Length: 396030, dtype: object\n\n\n\nsorted(df['emp_length'].dropna().unique())\n\n['1 year',\n '10+ years',\n '2 years',\n '3 years',\n '4 years',\n '5 years',\n '6 years',\n '7 years',\n '8 years',\n '9 years',\n '&lt; 1 year']\n\n\n\nordered_emp_lengths = ['&lt; 1 year',\n '1 year',\n '2 years',\n '3 years',\n '4 years',\n '5 years',\n '6 years',\n '7 years',\n '8 years',\n '9 years',\n '10+ years']\n\n\nplt.figure(figsize=(12,5))\nsns.countplot(data=df,\n              x='emp_length',\n              order=ordered_emp_lengths,\n              palette='viridis')\n\n&lt;Axes: xlabel='emp_length', ylabel='count'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(12,5))\nsns.countplot(data=df,\n              x='emp_length',\n              order=ordered_emp_lengths,\n              hue='loan_repaid')\n\n&lt;Axes: xlabel='emp_length', ylabel='count'&gt;\n\n\n\n\n\n\nemp_co = df[df['loan_repaid'] == 0].groupby('emp_length').count()['loan_repaid']\nemp_fp = df[df['loan_repaid'] == 1].groupby('emp_length').count()['loan_repaid']\n\nplt.figure(figsize=(6,2))\n((emp_co/(emp_fp+emp_co))*100).plot(kind='bar')\n\n&lt;Axes: xlabel='emp_length'&gt;\n\n\n\n\n\nemp_length doesn’t look to tell us anything about loan_repaid. Very similar fractions of all employment lengths did not manage to repay their loans, so we drop this column too.\n\ndf = df.drop('emp_length',axis=1)"
  },
  {
    "objectID": "posts/k2/index.html#filling-mort_acc-using-total_acc",
    "href": "posts/k2/index.html#filling-mort_acc-using-total_acc",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "Filling mort_acc using total_acc",
    "text": "Filling mort_acc using total_acc\n\npercentage_null(df)\n\n\nPercentages of values missing:\n\nloan_amnt               0.00\nterm                    0.00\nint_rate                0.00\ninstallment             0.00\ngrade                   0.00\nsub_grade               0.00\nhome_ownership          0.00\nannual_inc              0.00\nverification_status     0.00\nissue_d                 0.00\nloan_status             0.00\npurpose                 0.00\ndti                     0.00\nearliest_cr_line        0.00\nopen_acc                0.00\npub_rec                 0.00\nrevol_bal               0.00\nrevol_util              0.07\ntotal_acc               0.00\ninitial_list_status     0.00\napplication_type        0.00\nmort_acc                9.54\npub_rec_bankruptcies    0.14\naddress                 0.00\nloan_repaid             0.00\ndtype: float64\n\n\n\ndf['mort_acc'].value_counts()\n\nmort_acc\n0.0     139777\n1.0      60416\n2.0      49948\n3.0      38049\n4.0      27887\n5.0      18194\n6.0      11069\n7.0       6052\n8.0       3121\n9.0       1656\n10.0       865\n11.0       479\n12.0       264\n13.0       146\n14.0       107\n15.0        61\n16.0        37\n17.0        22\n18.0        18\n19.0        15\n20.0        13\n24.0        10\n22.0         7\n21.0         4\n25.0         4\n27.0         3\n32.0         2\n31.0         2\n23.0         2\n26.0         2\n28.0         1\n30.0         1\n34.0         1\nName: count, dtype: int64\n\n\n\ndf.corr(numeric_only=True)['mort_acc'].sort_values()\n\nint_rate               -0.082583\ndti                    -0.025439\nrevol_util              0.007514\npub_rec                 0.011552\npub_rec_bankruptcies    0.027239\nloan_repaid             0.073111\nopen_acc                0.109205\ninstallment             0.193694\nrevol_bal               0.194925\nloan_amnt               0.222315\nannual_inc              0.236320\ntotal_acc               0.381072\nmort_acc                1.000000\nName: mort_acc, dtype: float64\n\n\nmort_acc correlates most strongly with total_acc, so we can leverage the fact that total_acc has no missing values to fill the missing values in mort_acc\n\ntotal_acc_avgs = df.groupby('total_acc').mean(numeric_only=True)['mort_acc']\n\n\ndef fill_mort_acc(total_acc,mort_acc):\n    \n    if np.isnan(mort_acc):\n        return total_acc_avgs[total_acc]\n    else:\n        return mort_acc\n\n\ndf['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'],x['mort_acc']),axis=1)\n\n\ndf.isnull().sum()\n\nloan_amnt                 0\nterm                      0\nint_rate                  0\ninstallment               0\ngrade                     0\nsub_grade                 0\nhome_ownership            0\nannual_inc                0\nverification_status       0\nissue_d                   0\nloan_status               0\npurpose                   0\ndti                       0\nearliest_cr_line          0\nopen_acc                  0\npub_rec                   0\nrevol_bal                 0\nrevol_util              276\ntotal_acc                 0\ninitial_list_status       0\napplication_type          0\nmort_acc                  0\npub_rec_bankruptcies    535\naddress                   0\nloan_repaid               0\ndtype: int64\n\n\n\npercentage_null(df)\n\n\nPercentages of values missing:\n\nloan_amnt               0.00\nterm                    0.00\nint_rate                0.00\ninstallment             0.00\ngrade                   0.00\nsub_grade               0.00\nhome_ownership          0.00\nannual_inc              0.00\nverification_status     0.00\nissue_d                 0.00\nloan_status             0.00\npurpose                 0.00\ndti                     0.00\nearliest_cr_line        0.00\nopen_acc                0.00\npub_rec                 0.00\nrevol_bal               0.00\nrevol_util              0.07\ntotal_acc               0.00\ninitial_list_status     0.00\napplication_type        0.00\nmort_acc                0.00\npub_rec_bankruptcies    0.14\naddress                 0.00\nloan_repaid             0.00\ndtype: float64\n\n\nOnly a neglible amount of rows still have missing data points, so we now feel comfortable dropping these rows\n\ndf = df.dropna()\n\n\npercentage_null(df)\n\n\nPercentages of values missing:\n\nloan_amnt               0.0\nterm                    0.0\nint_rate                0.0\ninstallment             0.0\ngrade                   0.0\nsub_grade               0.0\nhome_ownership          0.0\nannual_inc              0.0\nverification_status     0.0\nissue_d                 0.0\nloan_status             0.0\npurpose                 0.0\ndti                     0.0\nearliest_cr_line        0.0\nopen_acc                0.0\npub_rec                 0.0\nrevol_bal               0.0\nrevol_util              0.0\ntotal_acc               0.0\ninitial_list_status     0.0\napplication_type        0.0\nmort_acc                0.0\npub_rec_bankruptcies    0.0\naddress                 0.0\nloan_repaid             0.0\ndtype: float64"
  },
  {
    "objectID": "posts/k2/index.html#categorical-features",
    "href": "posts/k2/index.html#categorical-features",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "Categorical Features",
    "text": "Categorical Features\nHere we treat the categorical features such as home_ownership and purpose by either dropping them or one-hot encoding them as to end with a dataframe consisting only of numeric features.\n\ndf.select_dtypes(['object']).columns\n\nIndex(['term', 'grade', 'sub_grade', 'home_ownership', 'verification_status',\n       'issue_d', 'loan_status', 'purpose', 'earliest_cr_line',\n       'initial_list_status', 'application_type', 'address'],\n      dtype='object')\n\n\n\ndf['term'].value_counts()\n\nterm\n 36 months    301247\n 60 months     93972\nName: count, dtype: int64\n\n\n\ndf['term'] = df['term'].apply(lambda x: int(x.split()[0]))\n\n\ndf['term']\n\n0         36\n1         36\n2         36\n3         36\n4         60\n          ..\n396025    60\n396026    36\n396027    36\n396028    60\n396029    36\nName: term, Length: 395219, dtype: int64\n\n\n\ndf['zip_code'] = df['address'].apply(lambda address: address[-5:])\n\n\ndf['zip_code'].value_counts()\n\nzip_code\n70466    56880\n22690    56413\n30723    56402\n48052    55811\n00813    45725\n29597    45393\n05113    45300\n11650    11210\n93700    11126\n86630    10959\nName: count, dtype: int64\n\n\n\ndf['home_ownership'].value_counts()\n\nhome_ownership\nMORTGAGE    198022\nRENT        159395\nOWN          37660\nOTHER          110\nNONE            29\nANY              3\nName: count, dtype: int64\n\n\n\ndf['home_ownership'] = df['home_ownership'].replace(['NONE','ANY'],'OTHER')\n\n\ndf['home_ownership'].value_counts()\n\nhome_ownership\nMORTGAGE    198022\nRENT        159395\nOWN          37660\nOTHER          142\nName: count, dtype: int64\n\n\none-hot encoding with pandas.get_dummies:\n\ncolumns_to_1hot = ['home_ownership',\n                   'verification_status',\n                   'application_type',\n                   'initial_list_status',\n                   'purpose',\n                   'zip_code',\n                   'sub_grade']\n\n\ndf = pd.get_dummies(data=df,\n                    columns=columns_to_1hot,\n                    drop_first=True,\n                    dtype=int)\n\n\ndf.columns\n\nIndex(['loan_amnt', 'term', 'int_rate', 'installment', 'grade', 'annual_inc',\n       'issue_d', 'loan_status', 'dti', 'earliest_cr_line', 'open_acc',\n       'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'mort_acc',\n       'pub_rec_bankruptcies', 'address', 'loan_repaid',\n       'home_ownership_OTHER', 'home_ownership_OWN', 'home_ownership_RENT',\n       'verification_status_Source Verified', 'verification_status_Verified',\n       'application_type_INDIVIDUAL', 'application_type_JOINT',\n       'initial_list_status_w', 'purpose_credit_card',\n       'purpose_debt_consolidation', 'purpose_educational',\n       'purpose_home_improvement', 'purpose_house', 'purpose_major_purchase',\n       'purpose_medical', 'purpose_moving', 'purpose_other',\n       'purpose_renewable_energy', 'purpose_small_business',\n       'purpose_vacation', 'purpose_wedding', 'zip_code_05113',\n       'zip_code_11650', 'zip_code_22690', 'zip_code_29597', 'zip_code_30723',\n       'zip_code_48052', 'zip_code_70466', 'zip_code_86630', 'zip_code_93700',\n       'sub_grade_A2', 'sub_grade_A3', 'sub_grade_A4', 'sub_grade_A5',\n       'sub_grade_B1', 'sub_grade_B2', 'sub_grade_B3', 'sub_grade_B4',\n       'sub_grade_B5', 'sub_grade_C1', 'sub_grade_C2', 'sub_grade_C3',\n       'sub_grade_C4', 'sub_grade_C5', 'sub_grade_D1', 'sub_grade_D2',\n       'sub_grade_D3', 'sub_grade_D4', 'sub_grade_D5', 'sub_grade_E1',\n       'sub_grade_E2', 'sub_grade_E3', 'sub_grade_E4', 'sub_grade_E5',\n       'sub_grade_F1', 'sub_grade_F2', 'sub_grade_F3', 'sub_grade_F4',\n       'sub_grade_F5', 'sub_grade_G1', 'sub_grade_G2', 'sub_grade_G3',\n       'sub_grade_G4', 'sub_grade_G5'],\n      dtype='object')\n\n\n\ndf['earliest_cr_line']\n\n0         Jun-1990\n1         Jul-2004\n2         Aug-2007\n3         Sep-2006\n4         Mar-1999\n            ...   \n396025    Nov-2004\n396026    Feb-2006\n396027    Mar-1997\n396028    Nov-1990\n396029    Sep-1998\nName: earliest_cr_line, Length: 395219, dtype: object\n\n\n\ndf['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda x: int(x.split('-')[-1]))\n\n\ndf['earliest_cr_year']\n\n0         1990\n1         2004\n2         2007\n3         2006\n4         1999\n          ... \n396025    2004\n396026    2006\n396027    1997\n396028    1990\n396029    1998\nName: earliest_cr_year, Length: 395219, dtype: int64\n\n\nDropping categorical features not worth encoding:\n\ncolumns_to_drop = ['earliest_cr_line',\n                   'grade',\n                   'issue_d',\n                   'loan_status',\n                   'address']\n\n\ndf = df.drop(columns_to_drop,axis=1)"
  },
  {
    "objectID": "posts/k2/index.html#traintest-split",
    "href": "posts/k2/index.html#traintest-split",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "TRAIN/TEST SPLIT",
    "text": "TRAIN/TEST SPLIT\nSplitting the dataset into a training set and a test set:\n(Note that cross-validation is performed inside the Optuna study, so we don’t need to separate out a validation set here)\n\nX = df.drop('loan_repaid', axis=1).values\ny = df['loan_repaid'].values\n\nfeatures = df.columns.drop('loan_repaid')\ntarget = 'loan_repaid'\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=594)\n\n# Convert to DataFrame and add column names\nX_test_df = pd.DataFrame(X_test, columns=features)\ny_test_df = pd.DataFrame(y_test, columns=[target])\n\ndf_test = pd.concat([X_test_df, y_test_df], axis=1)\n\n\nprint(\"Training set shape: \", X_train.shape, y_train.shape)\nprint(\"Test set shape: \", X_test.shape, y_test.shape)\n\nTraining set shape:  (375458, 78) (375458,)\nTest set shape:  (19761, 78) (19761,)"
  },
  {
    "objectID": "posts/k2/index.html#normalising-the-data",
    "href": "posts/k2/index.html#normalising-the-data",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "Normalising the Data",
    "text": "Normalising the Data\nSome machine learning algorithms require all columns of the dataframe to consist of data on comparable scales. We use StandardScalar from scikit-learn to force this to be true.\nThere is rarely a drawback to normalising data like this. If in doubt, normalise.\n\nscaler = StandardScaler()\n\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nX_train\n\narray([[ 0.82301704, -0.55823002, -1.28362299, ..., -0.03076372,\n        -0.02818382, -2.75955691],\n       [-1.45078579, -0.55823002, -0.46740923, ..., -0.03076372,\n        -0.02818382,  0.01934793],\n       [-1.25930765, -0.55823002,  1.14265627, ..., -0.03076372,\n        -0.02818382,  0.5751289 ],\n       ...,\n       [ 0.70334321, -0.55823002, -0.36901634, ..., -0.03076372,\n        -0.02818382,  0.29723841],\n       [ 1.90008154, -0.55823002, -0.33994571, ..., -0.03076372,\n        -0.02818382, -1.78694022],\n       [-0.52929727, -0.55823002, -1.17404909, ..., -0.03076372,\n        -0.02818382, -0.25854255]])"
  },
  {
    "objectID": "posts/k2/index.html#producing-some-visualisations-investigating-the-distribution-of-misclassified-points",
    "href": "posts/k2/index.html#producing-some-visualisations-investigating-the-distribution-of-misclassified-points",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "Producing some visualisations investigating the distribution of misclassified points",
    "text": "Producing some visualisations investigating the distribution of misclassified points\n\nmisclassified = df.iloc[np.concatenate((fp_indices[0], fn_indices[0]))].copy()\nmisclassified.loc[:, 'type'] = ['FP' if i in fp_indices[0] else 'FN' for i in misclassified.index]\n\n\n# Boxplot of loan_amnt\nplt.figure(figsize=(10,6))\nsns.boxplot(x='type', y='loan_amnt', data=misclassified)\nplt.title('Loan Amount Spread for Misclassified Points')\nplt.show()\n\n\n\n\n\n# Boxplot of int_rate\nplt.figure(figsize=(10,6))\nsns.boxplot(x='type', y='int_rate', data=misclassified)\nplt.title('Interest Rate Spread for Misclassified Points')\nplt.show()\n\n\n\n\n\n# Boxplot of annual_inc\nplt.figure(figsize=(10,6))\nsns.boxplot(x='type', y='annual_inc', data=misclassified)\nplt.title('Annual Income Spread for Misclassified Points')\nplt.ylim(0,200000)\nplt.show()\n\n\n\n\n\n# Boxplot of revol_bal\nplt.figure(figsize=(10,6))\nsns.boxplot(x='type', y='revol_bal', data=misclassified)\nplt.title('Revolving Balance Spread for Misclassified Points')\nplt.ylim(0,60000)\nplt.show()"
  },
  {
    "objectID": "posts/eh5/index.html",
    "href": "posts/eh5/index.html",
    "title": "Joining Demographic Statistics onto a Dataset of Charity Donors with Python",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\nfrom branca.colormap import LinearColormap\nfrom sklearn.impute import KNNImputer\nDownload FakeIndividualConstituents.csv\ndf = pd.read_csv('files/FakeIndividualConstituents.csv')\ndf.head()\n\n\n\n\n\n\n\n\nPostcode\nNumberDonations\nTotalDonated\nAverageDonated\nNewsletter\n\n\n\n\n0\nNG9 3WF\n4\n61\n15.25\n1\n\n\n1\nNG9 4WP\n1\n23\n23.00\n0\n\n\n2\nNG9 3EL\n1\n30\n30.00\n0\n\n\n3\nNG1 9FH\n5\n75\n15.00\n1\n\n\n4\nNG5 6QZ\n1\n15\n15.00\n0\ndf.describe()\n\n\n\n\n\n\n\n\nNumberDonations\nTotalDonated\nAverageDonated\nNewsletter\n\n\n\n\ncount\n100.000000\n100.000000\n100.00000\n100.000000\n\n\nmean\n4.320000\n250.150000\n46.51950\n0.500000\n\n\nstd\n5.454828\n1657.135245\n212.23961\n0.502519\n\n\nmin\n1.000000\n15.000000\n15.00000\n0.000000\n\n\n25%\n1.000000\n30.000000\n15.00000\n0.000000\n\n\n50%\n2.000000\n45.000000\n15.19000\n0.500000\n\n\n75%\n5.000000\n92.000000\n16.92500\n1.000000\n\n\nmax\n37.000000\n16618.000000\n2077.25000\n1.000000"
  },
  {
    "objectID": "posts/eh5/index.html#what-are-output-areas-oas",
    "href": "posts/eh5/index.html#what-are-output-areas-oas",
    "title": "Joining Demographic Statistics onto a Dataset of Charity Donors with Python",
    "section": "What are Output Areas (OAs)?",
    "text": "What are Output Areas (OAs)?\n\n\nOutput Areas (OAs) are the smallest geographical unit for which census data is reported. In the UK, they were introduced for the 2001 Census and have been used since. They are designed to be of a similar population size, with each one containing at least 40 households and 100 individuals where possible. The aim is to provide a consistent base for reporting statistics over time.\nPostcodes are a common way to identify locations in the UK, but they aren’t designed for statistical analysis. They can vary in size and can change over time. On the other hand, Output Areas are specifically designed for statistical analysis and are more stable over time.\nBy translating postcodes into Output Areas, we can join our postcode-level data with other datasets that use Output Areas."
  },
  {
    "objectID": "posts/eh5/index.html#joining-lsoa2021-and-oa2021-onto-postcode",
    "href": "posts/eh5/index.html#joining-lsoa2021-and-oa2021-onto-postcode",
    "title": "Joining Demographic Statistics onto a Dataset of Charity Donors with Python",
    "section": "JOINING LSOA2021 and OA2021 ONTO POSTCODE",
    "text": "JOINING LSOA2021 and OA2021 ONTO POSTCODE\nLookup table for converting Postcode to LSOA2021 and OA2021:\n\nhttps://geoportal.statistics.gov.uk/…\n\n\n# Read the lookup table and display 5 random rows\ndata = pd.read_csv('LOOKUP_GEO/PCD_OA21_LSOA21_MSOA21_LTLA22_UTLA22_CAUTH22_NOV23_UK_LU_v2.csv', low_memory=False)\ndata.sample(5)\n\n\n\n\n\n\n\n\npcd7\npcd8\npcds\ndointr\ndoterm\nusertype\noa21cd\nlsoa21cd\nlsoa21nm\nmsoa21cd\nmsoa21nm\nltla22cd\nltla22nm\nltla22nmw\nutla22cd\nutla22nm\nutla22nmw\ncauth22cd\ncauth22nm\n\n\n\n\n2411384\nTN4 0JY\nTN4 0JY\nTN4 0JY\n198001\nNaN\n0\nE00126111\nE01024846\nTunbridge Wells 002C\nE02005163\nTunbridge Wells 002\nE07000116\nTunbridge Wells\nNaN\nE10000016\nKent\nNaN\nNaN\nNaN\n\n\n238683\nBN3 7WA\nBN3 7WA\nBN3 7WA\n200012\n200212.0\n1\nE00085160\nE01016873\nBrighton and Hove 019D\nE02003509\nBrighton and Hove 019\nE06000043\nBrighton and Hove\nNaN\nE06000043\nBrighton and Hove\nNaN\nNaN\nNaN\n\n\n1484466\nML3 6PJ\nML3 6PJ\nML3 6PJ\n198001\nNaN\n0\nS00131878\nS01012725\nNaN\nS02002385\nNaN\nS12000029\nSouth Lanarkshire\nNaN\nS12000029\nSouth Lanarkshire\nNaN\nNaN\nNaN\n\n\n2320876\nSW1X8DL\nSW1X 8DL\nSW1X 8DL\n198001\nNaN\n0\nE00190381\nE01004691\nWestminster 019E\nE02000978\nWestminster 019\nE09000033\nWestminster\nNaN\nE09000033\nWestminster\nNaN\nNaN\nNaN\n\n\n2516422\nW1J 5LF\nW1J 5LF\nW1J 5LF\n200012\nNaN\n0\nE00024109\nE01004761\nWestminster 018D\nE02000977\nWestminster 018\nE09000033\nWestminster\nNaN\nE09000033\nWestminster\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n# Take and rename only the relevant columns ('pcds', 'oa21cd', 'lsoa21cd') of lookup table `data`\ndata = data[['pcds','oa21cd','lsoa21cd']].rename({'pcds':'Postcode', \n                                                  'oa21cd':'OA21', \n                                                  'lsoa21cd':'LSOA21'}, axis=1)\n\n# Display 5 randomly chosen rows of lookup table\ndata.sample(5)\n\n\n\n\n\n\n\n\nPostcode\nOA21\nLSOA21\n\n\n\n\n1217999\nL37 3JL\nE00035430\nE01006990\n\n\n2198583\nSO1 3GD\nE00086950\nE01017216\n\n\n123853\nBA2 5JZ\nE00072680\nE01014389\n\n\n206780\nBL4 9RE\nE00024606\nE01004861\n\n\n149180\nBB7 9YF\nE00128667\nE01025351\n\n\n\n\n\n\n\n\n# Joining 'OA21' and 'LSOA21' from the lookup table `data` onto donors `df` using the 'Postcode' column\ndf = df.merge(data, on='Postcode', how='left')\n\n# Display number of null values in each column to see if successful\ndf.isna().sum()\n\nPostcode           0\nLatitude           0\nLongitude          0\nNumberDonations    0\nTotalDonated       0\nAverageDonated     0\nNewsletter         0\nOA21               0\nLSOA21             0\ndtype: int64\n\n\n\ndf.sample(5)\n\n\n\n\n\n\n\n\nPostcode\nLatitude\nLongitude\nNumberDonations\nTotalDonated\nAverageDonated\nNewsletter\nOA21\nLSOA21\n\n\n\n\n72\nNG2 7DL\n52.945107\n-1.135586\n1\n16\n16.00\n0\nE00173532\nE01033402\n\n\n39\nNG8 9QN\n52.959883\n-1.222125\n9\n138\n15.33\n1\nE00069921\nE01013860\n\n\n37\nNG8 3DJ\n52.970564\n-1.219671\n4\n60\n15.00\n1\nE00069940\nE01013867\n\n\n97\nNG9 2QA\n52.930121\n-1.198353\n1\n15\n15.00\n0\nE00187586\nE01028074\n\n\n59\nNG11 7AX\n52.925360\n-1.157251\n1\n539\n539.00\n1\nE00070145\nE01013901"
  },
  {
    "objectID": "posts/eh5/index.html#joining-msoa2011-onto-postcode",
    "href": "posts/eh5/index.html#joining-msoa2011-onto-postcode",
    "title": "Joining Demographic Statistics onto a Dataset of Charity Donors with Python",
    "section": "JOINING MSOA2011 ONTO POSTCODE",
    "text": "JOINING MSOA2011 ONTO POSTCODE\nLookup table for converting Postcode to MSOA2011:\n\nhttps://geoportal.statistics.gov.uk/…\n\nPlease note that I have renamed my file to pcd_to_oa11_msoa11_lsoa11.csv from PCD_OA_LSOA_MSOA_LAD_MAY22_UK_LU.csv which was the name it had at download time.\n\n# Read the lookup table and display 5 randomly chosen rows\ndata = pd.read_csv('LOOKUP_GEO/pcd_to_oa11_msoa11_lsoa11.csv', encoding='ISO-8859-1', low_memory=False)\ndata.sample(5)\n\n\n\n\n\n\n\n\npcd7\npcd8\npcds\ndointr\ndoterm\nusertype\noa11cd\nlsoa11cd\nmsoa11cd\nladcd\nlsoa11nm\nmsoa11nm\nladnm\nladnmw\n\n\n\n\n359020\nBT8 6ZS\nBT8 6ZS\nBT8 6ZS\n200912\nNaN\n0\nN00001771\n95II03S1\nN99999999\nN09000007\nCairnshill 1\n(pseudo) Northern Ireland\nLisburn and Castlereagh\nNaN\n\n\n1700528\nNW8 9QR\nNW8 9QR\nNW8 9QR\n198001\nNaN\n0\nE00023413\nE01004648\nE02000961\nE09000033\nWestminster 002B\nWestminster 002\nWestminster\nNaN\n\n\n945738\nGL6 8WR\nGL6 8WR\nGL6 8WR\n200212\n200904.0\n1\nE00113923\nE01022411\nE02004656\nE07000082\nStroud 006D\nStroud 006\nStroud\nNaN\n\n\n1056104\nHU120NG\nHU12 0NG\nHU12 0NG\n198001\nNaN\n0\nE00066007\nE01013079\nE02002722\nE06000011\nEast Riding of Yorkshire 039A\nEast Riding of Yorkshire 039\nEast Riding of Yorkshire\nNaN\n\n\n267517\nBS161UY\nBS16 1UY\nBS16 1UY\n201808\nNaN\n0\nE00173035\nE01033333\nE02003106\nE06000025\nSouth Gloucestershire 017F\nSouth Gloucestershire 017\nSouth Gloucestershire\nNaN\n\n\n\n\n\n\n\n\n# Take and rename only the relevant columns ('pcds', 'msoa11cd') of lookup table `data`\ndata = data[['pcds', 'msoa11cd']].rename({'pcds':'Postcode',\n                                          'msoa11cd':'MSOA11'}, axis=1)\n\n# Display 5 randomly chosen rows\ndata.sample(5)\n\n\n\n\n\n\n\n\nPostcode\nMSOA11\n\n\n\n\n2292291\nSW18 1BW\nE02000932\n\n\n939836\nGL52 4JT\nE02004670\n\n\n2630775\nWV12 5TE\nE02002123\n\n\n2617558\nWS13 6TL\nE02006150\n\n\n663239\nDH7 9RF\nE02004307\n\n\n\n\n\n\n\n\n# Joining 'MSOA11' from the lookup table `data` onto donors `df` using the 'Postcode' column:\ndf = df.merge(data, on='Postcode', how='left')\n \n# Display number of null values in each column to see if successful:\ndf.isna().sum()\n\nPostcode           0\nLatitude           0\nLongitude          0\nNumberDonations    0\nTotalDonated       0\nAverageDonated     0\nNewsletter         0\nOA21               0\nLSOA21             0\nMSOA11             1\ndtype: int64"
  },
  {
    "objectID": "posts/eh5/index.html#understanding-the-index-of-multiple-deprivation-imd-score",
    "href": "posts/eh5/index.html#understanding-the-index-of-multiple-deprivation-imd-score",
    "title": "Joining Demographic Statistics onto a Dataset of Charity Donors with Python",
    "section": "Understanding the Index of Multiple Deprivation (IMD) Score",
    "text": "Understanding the Index of Multiple Deprivation (IMD) Score\nThe IMD Score is a measure of relative deprivation for small areas in the UK. It combines information from seven domains to produce an overall relative measure of deprivation. The higher the IMD Score, the more deprived an area is considered to be. Unlike the IMD Rank, which ranks areas in order, the IMD Score provides a measure of the absolute level of deprivation."
  },
  {
    "objectID": "posts/eh5/index.html#joining-imd2019-onto-df-by-first-coverting-lsoa2021-to-lsoa2011",
    "href": "posts/eh5/index.html#joining-imd2019-onto-df-by-first-coverting-lsoa2021-to-lsoa2011",
    "title": "Joining Demographic Statistics onto a Dataset of Charity Donors with Python",
    "section": "Joining IMD2019 onto df by first coverting LSOA2021 to LSOA2011:",
    "text": "Joining IMD2019 onto df by first coverting LSOA2021 to LSOA2011:\nLookup table coverting LSOA2021 to LSOA2011:\n\nhttps://geoportal.statistics.gov.uk/…\n\nLookup table converting LSOA2011 to Index of Multiple Deprivation (IMD) 2019:\n\nhttps://www.gov.uk/government/statistics/…\n\n\n# Read the LSOA lookup data\nlookup_data = pd.read_csv('LOOKUP_GEO/LSOA_(2011)_to_LSOA_(2021)_to_Local_Authority_District_(2022)_Lookup_for_England_and_Wales_(Version_2).csv')\n\n# Take only the relevant columns of `lookup_data` and rename\nlookup_data = lookup_data[['LSOA21CD','LSOA11CD']].rename({'LSOA21CD':'LSOA21', \n                                                           'LSOA11CD':'LSOA11'}, axis=1)\n\n\n# Read the IMD data\nimd_data = pd.read_csv('LOOKUP_STATS/File_7_-_All_IoD2019_Scores__Ranks__Deciles_and_Population_Denominators_3.csv')\n\n# Take only the relevant columns of `imd_data` and rename\nimd_data = imd_data[['LSOA code (2011)', 'Index of Multiple Deprivation (IMD) Score']].rename({'LSOA code (2011)':'LSOA11', \n                                                                                               'Index of Multiple Deprivation (IMD) Score':'IMD_Score'}, axis=1)\n\nThere is not a 1-to-1 correspondence between LSOA2011 and LSOA2021. We work around this by aggregating over the different LSOA2011 corresponding to each LSOA2021 and returning the mean of the IMD scores of each.\n\n# Merge the IMD data with the lookup data\nimd_data = imd_data.merge(lookup_data, on='LSOA11', how='left')\n\n# Aggregate the IMD scores for each LSOA21 by taking the mean\nimd_data = imd_data.groupby('LSOA21')['IMD_Score'].mean().reset_index()\n\n\n# Joining 'IMD_Score' from the aggregated lookup table `imd_data` onto donors `df` using the 'LSOA21' column\ndf = df.merge(imd_data, on='LSOA21', how='left')\n\n# Display number of null values in 'IMD_Score' column of `df` after the join\ndf['IMD_Score'].isna().sum()\n\n2\n\n\n\nUsing KNNImputer to fill missing IMD scores:\n\n# Create a new dataframe with only the columns of interest\ngeo_data = df[['Latitude', 'Longitude', 'IMD_Score']]\n\n# Create the imputer\nimputer = KNNImputer(n_neighbors=5, weights='distance')\n\n# Apply the imputer to the geo_data\ngeo_data_filled = imputer.fit_transform(geo_data)\n\n# Convert the result back to a dataframe and preserve the original index\ngeo_data_filled = pd.DataFrame(geo_data_filled, columns=geo_data.columns, index=geo_data.index)\n\n# Replace the original 'IMD_Score' column with the 'IMD_Score' with missing values filled\ndf['IMD_Score'] = geo_data_filled['IMD_Score']\n\n\ndf['IMD_Score'].isna().sum()\n\n0"
  },
  {
    "objectID": "posts/eh5/index.html#joining-population-density-onto-lsoa2021",
    "href": "posts/eh5/index.html#joining-population-density-onto-lsoa2021",
    "title": "Joining Demographic Statistics onto a Dataset of Charity Donors with Python",
    "section": "Joining population density onto LSOA2021:",
    "text": "Joining population density onto LSOA2021:\nLookup table connecting LSOA2021 to population density:\n\nhttps://www.ons.gov.uk/peoplepopulationandcommunity/…\n\nPlease note I used the ‘Mid-2020: SAPE23DT11’ edition of this dataset.\n\n# Read the lookup table and display 5 randomly chosen rows\nPop_density_lookup = pd.read_csv('LOOKUP_STATS/sape23dt11mid2020lsoapopulationdensity 4(Sheet1).csv')\nPop_density_lookup.sample(5)\n\n\n\n\n\n\n\n\nLSOA Code\nLSOA Name\nMid-2020 population\nArea Sq Km\nPeople per Sq Km\n\n\n\n\n17470\nE01029204\nSouth Somerset 009C\n1,205\n3.99\n302\n\n\n28309\nE01000300\nBarnet 019B\n1,763\n0.17\n10,161\n\n\n30690\nE01002662\nHounslow 017C\n2,658\n0.24\n10,862\n\n\n23151\nE01007102\nSefton 030D\n1,516\n0.20\n7,535\n\n\n29991\nE01032790\nHammersmith and Fulham 023F\n1,440\n0.08\n18,228\n\n\n\n\n\n\n\n\n# Take and rename only the relevant columns ('LSOA Code', 'People per Sq Km') of lookup table `Pop_density_lookup`\nPop_density_lookup = Pop_density_lookup[['LSOA Code','People per Sq Km']].rename({'LSOA Code':'LSOA21', \n                                                                                  'People per Sq Km':'PopDensity'},axis=1)\n\n# Display 5 randomly chosen rows\nPop_density_lookup.sample(5)\n\n\n\n\n\n\n\n\nLSOA21\nPopDensity\n\n\n\n\n18268\nE01030319\n1,476\n\n\n10776\nE01021912\n194\n\n\n1907\nE01013840\n6,024\n\n\n15518\nE01026885\n55\n\n\n7355\nE01015276\n9,177\n\n\n\n\n\n\n\n\n# Joining 'PopDensity' from the lookup table `Pop_density_lookup` onto donors `df` using the 'LSOA21' column:\ndf = df.merge(Pop_density_lookup, on='LSOA21', how='left')\n\n# Remove commas from joined population densities and convert to float type:\ndf['PopDensity'] = df['PopDensity'].str.replace(',', '').astype(float)\n\n# Display number of null values in 'PopDensity' column:\ndf['PopDensity'].isna().sum()\n\n19\n\n\n\nUsing KNNImputer to fill missing population densities\n\n# Create a new dataframe with only the columns of interest\npop_data = df[['Latitude', 'Longitude', 'PopDensity']]\n\n# Create the imputer\nimputer = KNNImputer(n_neighbors=5, weights='distance')\n\n# Apply the imputer to the `pop_data`\npop_data_filled = imputer.fit_transform(pop_data)\n\n# Convert the result back to a dataframe and preserve the original index\npop_data_filled = pd.DataFrame(pop_data_filled, columns=pop_data.columns, index=pop_data.index)\n\n# Replace original 'PopDensity' column with the 'PopDensity' column with missing values filled\ndf['PopDensity'] = pop_data_filled['PopDensity']\n\n\ndf['PopDensity'].isna().sum()\n\n0"
  },
  {
    "objectID": "posts/eh5/index.html#joining-average-annual-income-onto-msoa2011",
    "href": "posts/eh5/index.html#joining-average-annual-income-onto-msoa2011",
    "title": "Joining Demographic Statistics onto a Dataset of Charity Donors with Python",
    "section": "Joining Average Annual Income onto MSOA2011",
    "text": "Joining Average Annual Income onto MSOA2011\nLookup table joining MSOA2011 to Average Annual Income:\n\nhttps://www.ons.gov.uk/employmentandlabourmarket/…\n\nPlease note that my file MSOA2011_to_Income.csv is the ‘Total Annual Income’ sheet of the Financial year ending 2020 edition of this dataset.\n\n# Read the lookup table and display 5 randomly chosen rows\nincome_lookup = pd.read_csv('LOOKUP_STATS/MSOA2011_to_Income.csv', encoding='ISO-8859-1', low_memory=False)\nincome_lookup.sample(5)\n\n\n\n\n\n\n\n\nMSOA code\nMSOA name\nLocal authority code\nLocal authority name\nRegion code\nRegion name\nTotal annual income (£)\nUpper confidence limit (£)\nLower confidence limit (£)\nConfidence interval (£)\n\n\n\n\n6820\nW02000030\nConwy 004\nW06000003\nConwy\nW92000004\nWales\n34,400\n40,900\n28,900\n12,000\n\n\n2082\nE02006827\nAmber Valley 017\nE07000032\nAmber Valley\nE12000004\nEast Midla\n35,500\n42,100\n29,800\n12,300\n\n\n6309\nE02003135\nPlymouth 014\nE06000026\nPlymouth\nE12000009\nSouth West\n40,900\n49,000\n34,200\n14,800\n\n\n1074\nE02005327\nWyre 009\nE07000128\nWyre\nE12000002\nNorth West\n42,300\n50,000\n35,700\n14,300\n\n\n4319\nE02000880\nTower Hamlets 017\nE09000030\nTower Hamlets\nE12000007\nLondon\n50,500\n59,800\n42,600\n17,200\n\n\n\n\n\n\n\n\n# Take and rename only the relevant columns ('MSOA code', 'Total annual income (£)') of lookup table `income_lookup`\nincome_lookup = income_lookup[['MSOA code','Total annual income (£)']].rename({'MSOA code':'MSOA11', \n                                                                               'Total annual income (£)':'AvgAreaIncome'}, axis=1)\n\n# Display 5 randomly chosen rows\nincome_lookup.sample(5)\n\n\n\n\n\n\n\n\nMSOA11\nAvgAreaIncome\n\n\n\n\n717\nE02001117\n46,900\n\n\n2198\nE02005364\n42,500\n\n\n4561\nE02000195\n59,500\n\n\n6000\nE02006543\n45,500\n\n\n3514\nE02004466\n51,700\n\n\n\n\n\n\n\n\n# Joining 'AvgAreaIncome' from the lookup table `income_lookup` onto donors `df` using the 'MSOA11' column:\ndf = df.merge(income_lookup, on='MSOA11', how='left')\n\n# Removing commas from incomes and converting to float type:\ndf['AvgAreaIncome'] = df['AvgAreaIncome'].str.replace(',', '').astype(float)\n\n# Display number of null values in 'AvgAreaIncome' column:\ndf['AvgAreaIncome'].isna().sum()\n\n1\n\n\n\nUsing KNNImputer to fill missing ‘AvgAreaIncome’ values\n\n# Create a new dataframe `inc_data` with only the columns of interest\ninc_data = df[['Latitude', 'Longitude', 'AvgAreaIncome']]\n\n# Create the imputer\nimputer = KNNImputer(n_neighbors=5, weights='distance')\n\n# Apply the imputer to the `inc_data` dataframe\ninc_data_filled = imputer.fit_transform(inc_data)\n\n# Convert the result back to a dataframe and preserve the original index\ninc_data_filled = pd.DataFrame(inc_data_filled, columns=inc_data.columns, index=inc_data.index)\n\n# Replace original 'AvgAreaIncome' column with the corresponding column with missing values filled\ndf['AvgAreaIncome'] = inc_data_filled['AvgAreaIncome']\n\n\ndf['AvgAreaIncome'].isna().sum()\n\n0"
  },
  {
    "objectID": "posts/eh5/index.html#imd-score",
    "href": "posts/eh5/index.html#imd-score",
    "title": "Joining Demographic Statistics onto a Dataset of Charity Donors with Python",
    "section": "IMD Score",
    "text": "IMD Score\n\nfig, axs = plt.subplots(1, 2, figsize=(24, 8))\n\n# Total Donated vs IMD Score scatterplot\nsns.scatterplot(x=df['IMD_Score'], y=df['TotalDonated'], ax=axs[0])\nsns.regplot(x=df['IMD_Score'], y=df['TotalDonated'], scatter=False, color='red', ax=axs[0], ci=None)\ncorr = np.corrcoef(df['IMD_Score'], df['TotalDonated'])[0, 1]\naxs[0].annotate(f'Correlation: {corr:.2f}', xy=(0.95, 0.95), xycoords='axes fraction',\n                fontsize=12, ha='right', va='top', bbox=dict(boxstyle='round', fc='firebrick', alpha=0.5))\naxs[0].set_ylim(0,1500)\naxs[0].set_ylabel('Total Donated (£)')\naxs[0].set_title('Lifetime Donation Total vs IMD Score')\n\n# Number Donations vs IMD Score scatterplot\nsns.scatterplot(x=df['IMD_Score'], y=df['NumberDonations'], ax=axs[1])\nsns.regplot(x=df['IMD_Score'], y=df['NumberDonations'], scatter=False, color='red', ax=axs[1], ci=None)\ncorr = np.corrcoef(df['IMD_Score'], df['NumberDonations'])[0, 1]\naxs[1].annotate(f'Correlation: {corr:.2f}', xy=(0.95, 0.95), xycoords='axes fraction',\n                fontsize=12, ha='right', va='top', bbox=dict(boxstyle='round', fc='firebrick', alpha=0.5))\naxs[1].set_ylabel('Number of Donations')\naxs[1].set_title('Number of Donations vs IMD Score')\n\nplt.show()"
  },
  {
    "objectID": "posts/eh5/index.html#population-density",
    "href": "posts/eh5/index.html#population-density",
    "title": "Joining Demographic Statistics onto a Dataset of Charity Donors with Python",
    "section": "Population Density",
    "text": "Population Density\n\nfig, axs = plt.subplots(1, 2, figsize=(24, 8))\n\n# Total Donated vs Population Density scatterplot\nsns.scatterplot(x=df['PopDensity'], y=df['TotalDonated'], ax=axs[0])\nsns.regplot(x=df['PopDensity'], y=df['TotalDonated'], scatter=False, color='red', ax=axs[0], ci=None)\ncorr = np.corrcoef(df['PopDensity'], df['TotalDonated'])[0, 1]\naxs[0].annotate(f'Correlation: {corr:.2f}', xy=(0.95, 0.95), xycoords='axes fraction',\n                fontsize=12, ha='right', va='top', bbox=dict(boxstyle='round', fc='firebrick', alpha=0.5))\naxs[0].set_ylim(0,800)\naxs[0].set_ylabel('Total Donated (£)')\naxs[0].set_xlabel('Local Population Density (People per Sq Km)')\naxs[0].set_title('Lifetime Donation Total vs Population Density')\n\n# Number Donations vs Population Density scatterplot\nsns.scatterplot(x=df['PopDensity'], y=df['NumberDonations'], ax=axs[1])\nsns.regplot(x=df['PopDensity'], y=df['NumberDonations'], scatter=False, color='red', ax=axs[1], ci=None)\ncorr = np.corrcoef(df['PopDensity'], df['NumberDonations'])[0, 1]\naxs[1].annotate(f'Correlation: {corr:.2f}', xy=(0.95, 0.95), xycoords='axes fraction',\n                fontsize=12, ha='right', va='top', bbox=dict(boxstyle='round', fc='firebrick', alpha=0.5))\naxs[1].set_ylim(0,30)\naxs[1].set_ylabel('Number of Donations')\naxs[1].set_xlabel('Local Population Density (People per Sq Km)')\naxs[1].set_title('Number of Donations vs Population Density')\n\nplt.show()"
  },
  {
    "objectID": "posts/eh5/index.html#average-income",
    "href": "posts/eh5/index.html#average-income",
    "title": "Joining Demographic Statistics onto a Dataset of Charity Donors with Python",
    "section": "Average Income",
    "text": "Average Income\n\nfig, axs = plt.subplots(1, 2, figsize=(24, 8))\n\n# Total Donated vs Average Income scatterplot\nsns.scatterplot(x=df['AvgAreaIncome'], y=df['TotalDonated'], ax=axs[0])\nsns.regplot(x=df['AvgAreaIncome'], y=df['TotalDonated'], scatter=False, color='red', ax=axs[0], ci=None)\ncorr = np.corrcoef(df['AvgAreaIncome'], df['TotalDonated'])[0, 1]\naxs[0].annotate(f'Correlation: {corr:.2f}', xy=(0.95, 0.95), xycoords='axes fraction',\n                fontsize=12, ha='right', va='top', bbox=dict(boxstyle='round', fc='firebrick', alpha=0.5))\naxs[0].set_ylim(-200,2000)\naxs[0].set_ylabel('Total Donated (£)')\naxs[0].set_xlabel('Average Income (£)')\naxs[0].set_title('Lifetime Donation Total vs Average Income')\n\n# Number Donations vs Average Income scatterplot\nsns.scatterplot(x=df['AvgAreaIncome'], y=df['NumberDonations'], ax=axs[1])\nsns.regplot(x=df['AvgAreaIncome'], y=df['NumberDonations'], scatter=False, color='red', ax=axs[1], ci=None)\ncorr = np.corrcoef(df['AvgAreaIncome'], df['NumberDonations'])[0, 1]\naxs[1].annotate(f'Correlation: {corr:.2f}', xy=(0.95, 0.95), xycoords='axes fraction',\n                fontsize=12, ha='right', va='top', bbox=dict(boxstyle='round', fc='firebrick', alpha=0.5))\n#axs[1].set_ylim(0,30)\naxs[1].set_ylabel('Number of Donations')\naxs[1].set_xlabel('Average Income (£)')\naxs[1].set_title('Number of Donations vs Average Income')\n\nplt.show()"
  },
  {
    "objectID": "posts/eh3/index.html",
    "href": "posts/eh3/index.html",
    "title": "Geographic Analysis of Charity Donors - Latest Leaflet Maps",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport folium\nfrom folium.plugins import MarkerCluster\nfrom branca.colormap import LinearColormap\n\nfrom geopy.distance import geodesic\n\npd.set_option('display.max_columns', None)\n\n\ndf = pd.read_csv('data.csv').drop('DistanceFromEHSC', axis=1)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nLatitude\nLongitude\nNewsletter\nTransactions_LifetimeGiftsAmount\nTransactions_LifetimeGiftsNumber\nTransactions_AverageGiftAmount\nDonationFrequency\nDonationFrequencyActive\nTransactions_Months1To12GiftsAmount\nTransactions_Months1To12GiftsNumber\nmonthlyDonorMonths1to12\nTransactions_Months13To24GiftsAmount\nTransactions_Months13To24GiftsNumber\nmonthlyDonorMonths13to24\nTransactions_Months25To36GiftsAmount\nTransactions_Months25To36GiftsNumber\nmonthlyDonorMonths25to36\nTransactions_DateOfFirstGift\nTransactions_FirstGiftAmount\nTransactions_DateOfLastGift\nTransactions_LastGiftAmount\nmonthsSinceFirstDonation\nmonthsSinceLastDonation\nactiveMonths\nTransactions_DateOfHighestGift\nTransactions_HighestGiftAmount\nTransactions_DateOfLowestGift\nTransactions_LowestGiftAmount\n\n\n\n\n0\n52.961066\n-1.205200\n1\n419.66\n4\n104.915000\n0.133333\n0.148148\n152.89\n1\n0\n154.77\n1\n0\n97.19\n1\n0\n2021-10-23\n96.09\n2023-12-31\n148.05\n29\n3\n27\n2024-01-01\n148.44\n2021-10-20\n99.05\n\n\n1\n52.924105\n-1.216433\n1\n111.05\n3\n37.016667\n0.103448\n0.230769\n0.00\n0\n0\n50.43\n1\n0\n52.93\n1\n0\n2021-11-26\n48.27\n2022-11-17\n50.97\n28\n16\n13\n2022-11-21\n48.67\n2022-11-19\n49.69\n\n\n2\n52.936510\n-1.127547\n1\n982.82\n11\n89.347273\n0.297297\n1.000000\n0.00\n0\n0\n0.00\n0\n0\n901.00\n10\n0\n2021-03-30\n97.38\n2022-01-13\n103.32\n36\n26\n11\n2022-01-12\n97.97\n2022-01-13\n102.72\n\n\n3\n52.997952\n-1.189854\n1\n20.45\n2\n10.225000\n0.117647\n2.000000\n0.00\n0\n0\n10.45\n1\n0\n0.00\n0\n0\n2022-12-06\n10.03\n2022-12-06\n9.92\n16\n16\n1\n2022-12-06\n9.79\n2022-12-03\n9.94\n\n\n4\n52.971756\n-1.203969\n1\n20.75\n2\n10.375000\n0.074074\n2.000000\n0.00\n0\n0\n0.00\n0\n0\n10.75\n1\n0\n2022-01-20\n10.52\n2022-01-20\n9.68\n26\n26\n1\n2022-01-23\n9.64\n2022-01-20\n10.02\n\n\n\n\n\n\n\n\nThe Data\nThe fictional donors in data.csv have the following features:\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nLatitude\nThe approximate latitude of the donor.\n\n\nLongitude\nThe approximate longitude of the donor.\n\n\nNewsletter\nA binary indicator (1 or 0) representing whether the donor is subscribed to the Charity’s email newsletter.\n\n\n\n\n\n\nTransactions_LifetimeGiftsAmount\nThe total amount of donations made by the donor over their lifetime.\n\n\nTransactions_LifetimeGiftsNumber\nThe total number of donations made by the donor over their lifetime.\n\n\nTransactions_AverageGiftAmount\nThe average donation made by the donor.\n\n\nDonationFrequency\nThe frequency of donations made by the donor (donations/month).\n\n\nDonationFrequencyActive\nThe frequency of donations made by the donor while they were active (donations/month).\n\n\n\n\n\n\nTransactions_Months1To12GiftsAmount\nThe total amount of donations made by the donor in the latest 12 months.\n\n\nmonthlyDonorMonths1to12\nA binary indicator showing if the donor made monthly donations in the latest 12 months.\n\n\nTransactions_Months13To24GiftsAmount\nThe total amount of donations made by the donor in the latest 13-24 months.\n\n\nTransactions_Months13To24GiftsNumber\nThe total number of donations made by the donor in the latest 13-24 months.\n\n\nmonthlyDonorMonths13to24\nA binary indicator showing if the donor made monthly donations in the latest 13-24 months.\n\n\nTransactions_Months25To36GiftsAmount\nThe total amount of donations made by the donor in the latest 25-36 months.\n\n\nTransactions_Months25To36GiftsNumber\nThe total number of donations made by the donor in the latest 25-36 months.\n\n\nmonthlyDonorMonths25to36\nA binary indicator showing if the donor made monthly donations in the latest 25-36 months.\n\n\n\n\n\n\nTransactions_DateOfFirstGift\nThe date of the first donation made by the donor.\n\n\nTransactions_FirstGiftAmount\nThe amount of the first donation made by the donor.\n\n\nTransactions_DateOfLastGift\nThe date of the last donation made by the donor.\n\n\nTransactions_LastGiftAmount\nThe amount of the last donation made by the donor.\n\n\n\n\n\n\nmonthsSinceFirstDonation\nThe number of months since the donor’s first donation.\n\n\nmonthsSinceLastDonation\nThe number of months since the donor’s last donation.\n\n\nactiveMonths\nThe number of months the donor has been an active supported of the Charity.\n\n\n\n\n\n\nTransactions_DateOfHighestGift\nThe date of the highest donation made by the donor.\n\n\nTransactions_HighestGiftAmount\nThe amount of the highest donation made by the donor.\n\n\nTransactions_DateOfLowestGift\nThe date of the lowest donation made by the donor.\n\n\nTransactions_LowestGiftAmount\nThe amount of the lowest donation made by the donor.\n\n\n\nNote that these data points have been randomized, permuted and anonymized. This is not the true data of real donors to the charity.\n\ndf.sample(10)\n\n\n\n\n\n\n\n\nLatitude\nLongitude\nNewsletter\nTransactions_LifetimeGiftsAmount\nTransactions_LifetimeGiftsNumber\nTransactions_AverageGiftAmount\nDonationFrequency\nDonationFrequencyActive\nTransactions_Months1To12GiftsAmount\nTransactions_Months1To12GiftsNumber\nmonthlyDonorMonths1to12\nTransactions_Months13To24GiftsAmount\nTransactions_Months13To24GiftsNumber\nmonthlyDonorMonths13to24\nTransactions_Months25To36GiftsAmount\nTransactions_Months25To36GiftsNumber\nmonthlyDonorMonths25to36\nTransactions_DateOfFirstGift\nTransactions_FirstGiftAmount\nTransactions_DateOfLastGift\nTransactions_LastGiftAmount\nmonthsSinceFirstDonation\nmonthsSinceLastDonation\nactiveMonths\nTransactions_DateOfHighestGift\nTransactions_HighestGiftAmount\nTransactions_DateOfLowestGift\nTransactions_LowestGiftAmount\n\n\n\n\n425\n52.976130\n-1.142525\n0\n148.69\n2\n74.345000\n0.054054\n2.000000\n0.00\n0\n0\n0.00\n0\n0\n73.69\n1\n0\n2021-03-30\n76.11\n2021-04-01\n74.40\n36\n36\n1\n2021-04-03\n75.72\n2021-04-03\n75.02\n\n\n39\n52.974401\n-1.105213\n1\n19.90\n2\n9.950000\n0.071429\n2.000000\n0.00\n0\n0\n0.00\n0\n0\n9.90\n1\n0\n2021-12-15\n9.73\n2021-12-15\n10.00\n27\n27\n1\n2021-12-16\n10.15\n2021-12-19\n9.91\n\n\n13\n52.911530\n-1.107064\n0\n198.84\n3\n66.280000\n0.250000\n3.000000\n98.84\n2\n0\n0.00\n0\n0\n0.00\n0\n0\n2023-04-23\n49.53\n2023-04-23\n50.55\n11\n11\n1\n2023-04-27\n48.94\n2023-04-23\n50.41\n\n\n227\n52.925344\n-1.259633\n1\n50.17\n2\n25.085000\n0.153846\n2.000000\n25.17\n1\n0\n0.00\n0\n0\n0.00\n0\n0\n2023-03-29\n24.72\n2023-03-30\n24.49\n12\n12\n1\n2023-03-26\n24.75\n2023-03-28\n24.65\n\n\n310\n52.952328\n-1.159730\n1\n337.34\n35\n9.638286\n0.972222\n1.000000\n105.45\n11\n0\n125.94\n13\n1\n96.48\n10\n0\n2021-05-01\n9.94\n2024-02-28\n9.87\n35\n1\n35\n2024-02-29\n9.62\n2024-02-27\n9.32\n\n\n563\n52.970545\n-1.136419\n1\n323.56\n16\n20.222500\n0.444444\n1.066667\n0.00\n0\n0\n83.93\n4\n0\n219.63\n11\n0\n2021-04-30\n20.04\n2022-06-30\n19.66\n35\n21\n15\n2022-06-26\n20.73\n2022-06-29\n21.06\n\n\n302\n52.986120\n-1.147062\n0\n1794.31\n24\n74.762917\n0.648649\n1.043478\n0.00\n0\n0\n912.18\n12\n1\n807.13\n11\n0\n2021-04-02\n71.64\n2023-01-30\n77.85\n36\n14\n23\n2023-01-30\n73.85\n2023-02-01\n74.46\n\n\n37\n52.990988\n-1.137386\n1\n231.41\n4\n57.852500\n1.333333\n2.000000\n156.41\n3\n0\n0.00\n0\n0\n0.00\n0\n0\n2024-01-03\n51.10\n2024-02-27\n48.58\n2\n1\n2\n2024-03-02\n50.08\n2024-02-28\n50.89\n\n\n398\n52.940981\n-1.214600\n1\n93.46\n18\n5.192222\n0.900000\n1.000000\n52.85\n10\n0\n35.89\n7\n0\n0.00\n0\n0\n2022-08-27\n5.05\n2024-01-02\n5.05\n19\n2\n18\n2024-01-04\n5.17\n2023-12-31\n4.99\n\n\n95\n52.964628\n-1.149133\n1\n86.08\n16\n5.380000\n0.444444\n0.444444\n63.50\n12\n1\n10.05\n2\n0\n10.17\n1\n0\n2021-04-30\n10.22\n2024-03-08\n4.84\n35\n0\n36\n2021-05-01\n10.12\n2024-03-05\n4.95\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 750 entries, 0 to 749\nData columns (total 28 columns):\n #   Column                                Non-Null Count  Dtype  \n---  ------                                --------------  -----  \n 0   Latitude                              750 non-null    float64\n 1   Longitude                             750 non-null    float64\n 2   Newsletter                            750 non-null    int64  \n 3   Transactions_LifetimeGiftsAmount      750 non-null    float64\n 4   Transactions_LifetimeGiftsNumber      750 non-null    int64  \n 5   Transactions_AverageGiftAmount        750 non-null    float64\n 6   DonationFrequency                     750 non-null    float64\n 7   DonationFrequencyActive               750 non-null    float64\n 8   Transactions_Months1To12GiftsAmount   750 non-null    float64\n 9   Transactions_Months1To12GiftsNumber   750 non-null    int64  \n 10  monthlyDonorMonths1to12               750 non-null    int64  \n 11  Transactions_Months13To24GiftsAmount  750 non-null    float64\n 12  Transactions_Months13To24GiftsNumber  750 non-null    int64  \n 13  monthlyDonorMonths13to24              750 non-null    int64  \n 14  Transactions_Months25To36GiftsAmount  750 non-null    float64\n 15  Transactions_Months25To36GiftsNumber  750 non-null    int64  \n 16  monthlyDonorMonths25to36              750 non-null    int64  \n 17  Transactions_DateOfFirstGift          750 non-null    object \n 18  Transactions_FirstGiftAmount          750 non-null    float64\n 19  Transactions_DateOfLastGift           750 non-null    object \n 20  Transactions_LastGiftAmount           750 non-null    float64\n 21  monthsSinceFirstDonation              750 non-null    int64  \n 22  monthsSinceLastDonation               750 non-null    int64  \n 23  activeMonths                          750 non-null    int64  \n 24  Transactions_DateOfHighestGift        750 non-null    object \n 25  Transactions_HighestGiftAmount        750 non-null    float64\n 26  Transactions_DateOfLowestGift         750 non-null    object \n 27  Transactions_LowestGiftAmount         750 non-null    float64\ndtypes: float64(13), int64(11), object(4)\nmemory usage: 164.2+ KB\n\n\nConverting the columns containing dates to datetime type\n\ndate_cols = ['Transactions_DateOfFirstGift', 'Transactions_DateOfLowestGift', 'Transactions_DateOfHighestGift', 'Transactions_DateOfLastGift']\n\nfor col in date_cols:\n    df[col] = pd.to_datetime(df[col])\n\n\n\nENGINEERING DISTANCE FROM EHSC\n\ndef calculate_distance(row, base_coords):\n    return geodesic((row['Latitude'], row['Longitude']), base_coords).km\n\n\nehsc_coords = (52.95383, -1.14168)\n\ndf['DistanceFromEHSC'] = df.apply(calculate_distance, axis=1, base_coords=ehsc_coords)\n\n\ndf['DistanceFromEHSC']\n\n0      4.344046\n1      6.016685\n2      2.148884\n3      5.880628\n4      4.636596\n         ...   \n745    2.801735\n746    3.900813\n747    6.047917\n748    3.728712\n749    3.338494\nName: DistanceFromEHSC, Length: 750, dtype: float64\n\n\n\ndf['DistanceFromEHSC'].plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf['DistanceFromEHSC'].describe()\n\ncount    750.000000\nmean       4.603878\nstd        2.307654\nmin        0.096575\n25%        2.821600\n50%        4.273026\n75%        6.219849\nmax        9.978092\nName: DistanceFromEHSC, dtype: float64\n\n\n\ndf.to_csv('data.csv', index=False)\n\n\n\nVISUALISING DISTRIBUTION OF DONORS WITH FOLIUM\n\nm = folium.Map(location=[52.9548, -1.1581], zoom_start=12)\n\ncolors = ['green', 'yellow', 'orange', 'red', 'purple']\nlinear_colormap = LinearColormap(colors=colors,\n                                 index=[0, 100, 250, 500, 1000],\n                                 vmin=df['Transactions_LifetimeGiftsAmount'].min(),\n                                 vmax=df['Transactions_LifetimeGiftsAmount'].quantile(0.94))\n\n# Create FeatureGroups\nfgroups = [folium.map.FeatureGroup(name=f\"Total Donated:  £{lower}{('-£' + str(upper)) if upper != float('inf') else '+'}\") for lower, upper in zip([0, 100, 250, 500, 750, 1000], [100, 250, 500, 750, 1000, float('inf')])]\n\n\nfor index, row in df.iterrows():    \n    fname = 'Example'\n    lname = 'Donor'\n    email = 'exampledonor@email.com'\n    \n    total_don = row['Transactions_LifetimeGiftsAmount']\n    num_don = row['Transactions_LifetimeGiftsNumber']\n    avg_don = row['Transactions_AverageGiftAmount']\n    \n    news = bool(row['Newsletter'])\n    monthly = bool(row['monthlyDonorMonths1to12'])\n    \n    lat = row['Latitude']\n    long = row['Longitude']\n    \n    dateoffirst = row['Transactions_DateOfFirstGift'].strftime('%d/%m/%Y')\n    dateoflast = row['Transactions_DateOfLastGift'].strftime('%d/%m/%Y')\n\n    active = row['activeMonths']\n    freq = row['DonationFrequency']\n    freq_active = row['DonationFrequencyActive']\n\n    dist = row['DistanceFromEHSC']\n    \n    popup_text = f'''\n                    &lt;div style=\"width: 200px; font-family: Arial; line-height: 1.2;\"&gt;\n                        &lt;h4 style=\"margin-bottom: 5px;\"&gt;{fname} {lname}&lt;/h4&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Total Donated:&lt;/b&gt; £{total_don:.2f}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Number of Donations:&lt;/b&gt; {num_don}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Average Donation:&lt;/b&gt; £{avg_don:.2f}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;First Recorded Donation:&lt;/b&gt; {dateoffirst}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Last Recorded Donation:&lt;/b&gt; {dateoflast}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;ActiveMonths:&lt;/b&gt; {active}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;DonationFrequency&lt;/b&gt; {freq:.2f}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;DonationFrequencyActive&lt;/b&gt; {freq_active:.2f}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Subscribed to Newsletter:&lt;/b&gt; {\"Yes\" if news else \"No\"}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Current Monthly Donor:&lt;/b&gt; {\"Yes\" if monthly else \"No\"}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Distance from EHSC:&lt;/b&gt; {dist:.2f}km&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Email:&lt;/b&gt;&lt;br&gt; {email}&lt;/p&gt;\n                    &lt;/div&gt;\n                    '''\n\n    \n    color = linear_colormap(total_don)\n    \n    marker = folium.CircleMarker(\n        location=[lat, long],\n        radius=5,\n        color=color, \n        fill=True,\n        fill_color=color,\n        fill_opacity=0.7,\n        popup=popup_text\n    )\n    \n    # Add the marker to the appropriate FeatureGroup\n    for fgroup, (lower, upper) in zip(fgroups, zip([0, 100, 250, 500, 750, 1000], [100, 250, 500, 750, 1000, float('inf')])):\n        if lower &lt;= total_don &lt; upper:\n            fgroup.add_child(marker)\n            break\n\n# Add the FeatureGroups to the map\nfor fgroup in fgroups:\n    m.add_child(fgroup)\n\nlinear_colormap.add_to(m)\nlinear_colormap.caption = 'Total Donated (£)'\nm.add_child(folium.LayerControl())\n\n# Create a marker at EHSC\npopup_html = '''&lt;h4 style=\"margin-bottom: 5px;\"&gt;Emmanuel House Support Centre&lt;/h4&gt;\n&lt;a href=\"https://www.emmanuelhouse.org.uk/\" target=\"_blank\"&gt;https://www.emmanuelhouse.org.uk/&lt;/a&gt;\n&lt;p&gt;Emmanuel House is an independent charity that supports people who are homeless, rough sleeping, in crisis, or at risk of homelessness in Nottingham.&lt;/p&gt;\n'''\nmarker = folium.Marker(location=ehsc_coords, popup=folium.Popup(popup_html))\nm.add_child(marker)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nNote the popups that appear when clicking on each datapoint in the above map!\nThere is a layer control menu hidden in the top right corner until mouseover. It lets you show and hide the points with donation totals in specific ranges.\n\n\n\nREINTERPRETING AS A CLUSTER MAP\n\nm = folium.Map(location=[52.9548, -1.1581], zoom_start=12)\n\ncolors = ['green', 'yellow', 'orange', 'red', 'purple']\nlinear_colormap = LinearColormap(colors=colors,\n                                 index=[0, 100, 250, 500, 1000],\n                                 vmin=df['Transactions_LifetimeGiftsAmount'].min(),\n                                 vmax=df['Transactions_LifetimeGiftsAmount'].quantile(0.94))\n\n# Create a MarkerCluster\nmarker_cluster = MarkerCluster().add_to(m)\n\nfor index, row in df.iterrows():    \n    fname = 'Example'\n    lname = 'Donor'\n    email = 'exampledonor@email.com'\n    \n    total_don = row['Transactions_LifetimeGiftsAmount']\n    num_don = row['Transactions_LifetimeGiftsNumber']\n    avg_don = row['Transactions_AverageGiftAmount']\n    \n    news = bool(row['Newsletter'])\n    monthly = bool(row['monthlyDonorMonths1to12'])\n    \n    lat = row['Latitude']\n    long = row['Longitude']\n    \n    dateoffirst = row['Transactions_DateOfFirstGift'].strftime('%d/%m/%Y')\n    dateoflast = row['Transactions_DateOfLastGift'].strftime('%d/%m/%Y')\n\n    active = row['activeMonths']\n    freq = row['DonationFrequency']\n    freq_active = row['DonationFrequencyActive']\n\n    dist = row['DistanceFromEHSC']\n    \n    popup_text = f'''\n                    &lt;div style=\"width: 200px; font-family: Arial; line-height: 1.2;\"&gt;\n                        &lt;h4 style=\"margin-bottom: 5px;\"&gt;{fname} {lname}&lt;/h4&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Total Donated:&lt;/b&gt; £{total_don:.2f}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Number of Donations:&lt;/b&gt; {num_don}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Average Donation:&lt;/b&gt; £{avg_don:.2f}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;First Recorded Donation:&lt;/b&gt; {dateoffirst}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Last Recorded Donation:&lt;/b&gt; {dateoflast}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;ActiveMonths:&lt;/b&gt; {active}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;DonationFrequency&lt;/b&gt; {freq:.2f}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;DonationFrequencyActive&lt;/b&gt; {freq_active:.2f}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Subscribed to Newsletter:&lt;/b&gt; {\"Yes\" if news else \"No\"}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Current Monthly Donor:&lt;/b&gt; {\"Yes\" if monthly else \"No\"}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Distance from EHSC:&lt;/b&gt; {dist:.2f}km&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Email:&lt;/b&gt;&lt;br&gt; {email}&lt;/p&gt;\n                    &lt;/div&gt;\n                    '''\n\n    \n    color = linear_colormap(total_don)\n    \n    marker = folium.CircleMarker(\n        location=[lat, long],\n        radius=5,\n        color=color, \n        fill=True,\n        fill_color=color,\n        fill_opacity=0.7,\n        popup=popup_text\n    )\n    \n    # Add the marker to the MarkerCluster\n    marker.add_to(marker_cluster)\n\nlinear_colormap.add_to(m)\nlinear_colormap.caption = 'Total Donated (£)'\nm.add_child(folium.LayerControl())\n\n# Create a marker at EHSC\npopup_html = '''&lt;h4 style=\"margin-bottom: 5px;\"&gt;Emmanuel House Support Centre&lt;/h4&gt;\n&lt;a href=\"https://www.emmanuelhouse.org.uk/\" target=\"_blank\"&gt;https://www.emmanuelhouse.org.uk/&lt;/a&gt;\n&lt;p&gt;Emmanuel House is an independent charity that supports people who are homeless, rough sleeping, in crisis, or at risk of homelessness in Nottingham.&lt;/p&gt;\n'''\nmarker = folium.Marker(location=ehsc_coords, popup=folium.Popup(popup_html))\nm.add_child(marker)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nREMARKS\n\nThe distribution of the fictional donors contained in data.csv more closely resembles the distribution of the real donors than the synthetic data constructed in my previous blog post:\nInvestigating The Geographic Distribution Of Charity Donors With Interactive Maps Made Using Folium\nThe Latitudes and Longitudes have been constructed to be the features following a distribution that is the least representative of the real data, for obvious privacy concerns. This is the reason for the donors on main roads, in Wollaton park etc."
  },
  {
    "objectID": "posts/eh1/index.html",
    "href": "posts/eh1/index.html",
    "title": "Geocoding Postcodes in Python: pgeocode v ONS",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport folium\nimport pgeocode\n\nAs part of my voluntary work for Emmanuel House, a Nottingham charity providing support and shelter for the homeless and vulnerably housed, I undertook a project studying the geographic distribution of individual donors to the charity across the UK.\nIn this post I illustrate an issue I encountered while attempting to encode the postcodes of individual donors as latitude and longitude pairs, and the solution.\nThe goal of encoding the postcodes was to produce visualisations showing how donors are distributed across Nottingham. This allowed us to gain insight into which areas/demographics should be targeted by marketing campaigns, and which social groups are underrepresented in Emmanuel House’s donor database.\n\ndf = pd.read_csv('files/nottm_postcodes.csv')\n\n Download nottm_postcodes.csv \nnottm_postcodes.csv is a file I constructed consisting of 100 random postcodes within Nottingham city.\nImportantly, these are not postcodes of real donors to the charity. Sharing such information online would violate GDPR. These postcodes are randomly selected from publicly available datasets\n\ndf.head(10)\n\n\n\n\n\n\n\n\nPostcode\n\n\n\n\n0\nNG9 3WF\n\n\n1\nNG9 4WP\n\n\n2\nNG9 3EL\n\n\n3\nNG1 9FH\n\n\n4\nNG5 6QZ\n\n\n5\nNG7 5QL\n\n\n6\nNG7 2FT\n\n\n7\nNG4 1PY\n\n\n8\nNG8 3SL\n\n\n9\nNG9 4AX\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   Postcode  100 non-null    object\ndtypes: object(1)\nmemory usage: 928.0+ bytes\n\n\n\ndf.nunique()\n\nPostcode    100\ndtype: int64\n\n\n\npostcodes = df['Postcode'].to_list()\n\n\nGeocoding with pgeocode\nI first attempted to geocode the postcodes using pgeocode, a lightweight Python library for geocoding postcodes and distance calculations.\n\nnomi = pgeocode.Nominatim('gb')\n\n\nprint('Querying postcode:', df.iloc[0]['Postcode'])\nnomi.query_postal_code(df.iloc[0]['Postcode'])\n\nQuerying postcode: NG9 3WF\n\n\npostal_code                                                     NG9\ncountry_code                                                     GB\nplace_name        Bramcote, Beeston, Toton, Stapleford, Attenbor...\nstate_name                                                  England\nstate_code                                                      ENG\ncounty_name                                         Nottinghamshire\ncounty_code                                                11609044\ncommunity_name                                                  NaN\ncommunity_code                                                  NaN\nlatitude                                                   52.91914\nlongitude                                                  -1.24548\naccuracy                                                        4.0\nName: 0, dtype: object\n\n\n\nnomi.query_postal_code(postcodes)[['latitude','longitude']].head()\n\n\n\n\n\n\n\n\nlatitude\nlongitude\n\n\n\n\n0\n52.91914\n-1.24548\n\n\n1\n52.91914\n-1.24548\n\n\n2\n52.91914\n-1.24548\n\n\n3\n52.95360\n-1.15050\n\n\n4\n53.00060\n-1.13150\n\n\n\n\n\n\n\n\ndf[['Latitude','Longitude']] = nomi.query_postal_code(postcodes)[['latitude','longitude']]\n\n\ndf.head()\n\n\n\n\n\n\n\n\nPostcode\nLatitude\nLongitude\n\n\n\n\n0\nNG9 3WF\n52.91914\n-1.24548\n\n\n1\nNG9 4WP\n52.91914\n-1.24548\n\n\n2\nNG9 3EL\n52.91914\n-1.24548\n\n\n3\nNG1 9FH\n52.95360\n-1.15050\n\n\n4\nNG5 6QZ\n53.00060\n-1.13150\n\n\n\n\n\n\n\nAt a first glance it seems that pgeocode has provided an elegent solution to the problem in only a few lines of fast-running code. However upon closer inspection:\n\ndf[['Latitude','Longitude']].value_counts()\n\nLatitude   Longitude\n52.953600  -1.150500    42\n52.929150  -1.114850    13\n52.964800  -1.213200    12\n52.919140  -1.245480    11\n53.000600  -1.131500    10\n52.998900  -1.197100     5\n52.970960  -1.081000     3\n52.878620  -1.195240     2\n52.903579  -1.044274     1\n52.952566  -0.894169     1\nName: count, dtype: int64\n\n\n\ndf[['Latitude','Longitude']].nunique()\n\nLatitude     10\nLongitude    10\ndtype: int64\n\n\nDespite df containing 100 unique postcodes, pgeocode has only encoded these into 10 different latitude, longitude pairs.\nWhats worse is that 42(!!) distinct postcodes were each encoded into (52.953600, -1.150500).\nWe can judge this approach by contructing a visualisation in the form as an interactive .html map using Folium:\n\npgeocode_map = folium.Map(location=[52.9548, -1.1581], zoom_start=13)\n\nfor index, row in df.iterrows():\n    folium.Marker(\n        location=[row['Latitude'], row['Longitude']],\n    ).add_to(pgeocode_map)\n\npgeocode_map.save('pgeocode_map.html')\npgeocode_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nTaking a screenshot of this map and displaying using matplotlib:\n\nimg = mpimg.imread('pgeocode_map.png')\nimgplot = plt.imshow(img)\nplt.show()\n\n\n\n\nIt indeed seems that pgeocode has encoded the postcodes with a less than desirable accuracy, resulting in large numbers of distinct postcodes being sent to the same point on the map.\n\ndf = df.drop(['Latitude', 'Longitude'], axis=1)\n\n\n\nGeocoding with ONS data\nUpon suggestion from a friend I proceeded to directly encode the postcodes by downloading data from the Office for National Statistics (ONS) instead of relying on a Python library.\nThis had the advantage of resulting in a far more accurate geocoding, but had the disadvantage of being more computationally expensive.\n\npost = pd.read_csv(r'C:\\Users\\Daniel\\Downloads\\open_postcode_geo.csv\\open_postcode_geo.csv', header=None)\npost = post[[0, 7, 8]]\npost = post.rename({0: 'Postcode', 7: 'Latitude', 8: 'Longitude'}, axis=1)\npost.sample(5)\n\n\n\n\n\n\n\n\nPostcode\nLatitude\nLongitude\n\n\n\n\n717388\nEC4A 2LE\n51.513622\n-0.112106\n\n\n1402379\nMK40 2LE\n52.144992\n-0.462139\n\n\n1094844\nKT1 3QG\n51.407402\n-0.280400\n\n\n1612623\nNR29 4RQ\n52.702233\n1.623660\n\n\n2294608\nTA20 4DN\n50.868756\n-2.933350\n\n\n\n\n\n\n\nopen_postcode_geo.csv can be downloaded from https://www.data.gov.uk/dataset/091feb1c-aea6-45c9-82bf-768a15c65307/open-postcode-geo and is contructed from data made publicly available by the ONS.\nopen_postcode_geo.csv consists of over 2.6 million UK postcodes, along with their latitudes and longitudes. Other information about the postcodes is also included which is irrelevant for the purposes of this post.\nBecause of the length of the dataset the code cells in this section take some time to run.\n\nlen(post)\n\n2631536\n\n\nWe can now geocode the postcodes in df by joining the corresponding Latitude and Longitude from post:\n\ndf = df.merge(post, on='Postcode', how='left')\ndf.head()\n\n\n\n\n\n\n\n\nPostcode\nLatitude\nLongitude\n\n\n\n\n0\nNG9 3WF\n52.930121\n-1.198353\n\n\n1\nNG9 4WP\n52.921587\n-1.247504\n\n\n2\nNG9 3EL\n52.938985\n-1.239510\n\n\n3\nNG1 9FH\n52.955008\n-1.141045\n\n\n4\nNG5 6QZ\n52.996670\n-1.106307\n\n\n\n\n\n\n\n\ndf[['Latitude', 'Longitude']].value_counts()\n\nLatitude   Longitude\n52.945107  -1.135586    8\n52.955008  -1.141045    4\n52.930121  -1.198353    3\n52.955053  -1.141030    2\n52.959917  -1.222127    2\n                       ..\n52.953058  -1.144924    1\n52.952972  -1.228161    1\n52.952828  -1.145360    1\n52.952059  -1.170039    1\n52.998288  -1.135252    1\nName: count, Length: 85, dtype: int64\n\n\n\ndf[['Latitude','Longitude']].nunique()\n\nLatitude     85\nLongitude    85\ndtype: int64\n\n\nThis solution has resulted in 85 unique latitude, longitude pairs. A significant improvement over the results from pgeocode.\nProducing another visualisation using Folium:\n\nons_map = folium.Map(location=[52.9548, -1.1581], zoom_start=13)\n\nfor index, row in df.iterrows():\n    folium.Marker(\n        location=[row['Latitude'], row['Longitude']],\n    ).add_to(ons_map)\n\nons_map.save('ons_map.html')\nons_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nTaking a screenshot of the map using ShareX and displaying using matplotlib:\n\nimg = mpimg.imread('ons_map.png')\nimgplot = plt.imshow(img)\nplt.show()\n\n\n\n\nThis looks significantly more realistic than the map produced using pgeocode to geocode."
  },
  {
    "objectID": "posts/bte3/index.html#relative-entropy-mutual-information",
    "href": "posts/bte3/index.html#relative-entropy-mutual-information",
    "title": "The Boltzmann Equation - 3. Information Theory",
    "section": "- Relative Entropy & Mutual Information",
    "text": "- Relative Entropy & Mutual Information\n\nDefinition 2.2.1 - KL Divergence\nThe relative entropy (also known as the Kullback-Liebler divergence) \\(D(f\\,||\\,g)\\) between two densities \\(f\\) and \\(g\\) is defined as\n\\[D(f\\,||\\,g) = \\int f(x)\\log\\frac{f(x)}{g(x)}\\,\\text{d}x.\\]\n\nNote: \\[D(f\\,||\\,g) &lt; \\infty \\Longleftrightarrow \\text{supp}\\,f\\subseteq\\text{supp}\\,g.\\]\n\n\nDefinition 2.2.2 - Mutual Information\nGiven two random variables \\(X\\) and \\(Y\\) with joint density \\(f_{X,Y}\\) define the mutual information \\(I(X;Y)\\) between \\(X\\) and \\(Y\\) by\n\\[I(X;Y) = \\int f_{X,Y}(x,y)\\log\\left[\\frac{f_{X,Y}(x,y)}{f_X(x)f_Y(y)}\\right]\\,\\text{d}x\\,\\text{d}y.\\]\n\nFrom the definition it is clear that we have the formulas\n\\[\\begin{aligned}\n    I(X;Y) &= h(X) - h(X|Y)\\\\\n    &= h(Y) - h(Y|X)\\\\\n    &= h(X) + h(Y) - h(X,Y).\n\\end{aligned}\\]\nAlong with\n\\[I(X;Y) = D(f_{X,Y}\\,||\\,f_X\\otimes f_Y).\\]\nNote the special cases\n\\[\\begin{aligned}\nI(X;Y) &= I(Y;X),\\\\\nI(X;X) &= h(X).\n\\end{aligned}\\]\n\n\nTheorem 2.2.3 - Information inequality\nFor any pair of densities \\(f,\\,g\\):\n\\[D(f\\,||\\,g) \\geq 0,\\]\nwith equality if and only if \\(f = g\\) a.e.\nProof.\nWithout loss of generality assume \\(f/g\\geq1\\). (??)\nUse the fact that \\(\\int f = \\int g = 1\\) to rewrite\n\\[\\begin{aligned}\n    D(f\\,||\\,g) &= \\int f\\log\\frac{f}{g}\\\\\n    &= \\int f\\left(\\frac{g}{f}-1-\\log\\frac{g}{f}\\right).\n\\end{aligned}\\]\nNow note that for \\(t\\geq1\\) we have the inequality:\n\\[t - 1 -\\log t \\geq 0,\\]\nin which equality holds iff \\(t=1.\\) This can be easily established graphically or by means of elementary calculus. Applying this inequality to \\(f/g\\) and integrating yields\n\\[\\int \\frac{g}{f}-1-\\log\\frac{g}{f}\\;\\geq\\; 0\\]\nwith equality iff \\(f = g\\) a.e. ◻\n\n\n\nCorollary 2.2.4\nFor any pair of random variables \\(X,\\,Y\\): \\[I(X;Y)\\geq 0,\\] with equality if and only if \\(X\\) and \\(Y\\) are independent.\nProof.\n\\[I(X;Y) = D(f_{X,Y}\\,||\\,f_X\\otimes f_Y) \\geq 0\\]\nwith equality iff \\(f_{X,Y} = f_X\\otimes f_Y\\) a.e. i.e. iff \\(X\\) and \\(Y\\) are independent. ◻\n\n\n\nCorollary 2.2.5\nFor any pair of random variables \\(X,\\,Y\\):\n\\[h(X|Y) \\leq h(X),\\]\nwith equality if and only if \\(X\\) and \\(Y\\) are independent.\nProof.\n\\[h(X) - h(X|Y) = I(X;Y)\\,\\geq\\,0,\\]\nwith equality iff \\(X\\) and \\(Y\\) are independent by Corollary 3.2.4. ◻"
  },
  {
    "objectID": "posts/bte1/index.html",
    "href": "posts/bte1/index.html",
    "title": "The Boltzmann Equation - 1. Introduction",
    "section": "",
    "text": "The Boltzmann equation (also known as the Boltzmann transport equation) models the behaviour of a gas comprised of a single particle species, sufficiently dilute so that quantum effects are negligible and when all inter-particle interactions are assumed to be elastic binary collisions:\n\\[\\begin{aligned}\n\\frac{\\partial f}{\\partial t} + v\\cdot \\nabla_{x} f = Q(f,f),\\quad x\\in X,\\: v\\in \\mathbb{R}^d,\\: t\\geq0.\n\\end{aligned}\\]\nWhere \\(f=f(t,x,v)=f_t(x,v)\\) is the particle distribution function in phase space and \\(X \\subset \\mathbb{R}^d\\) \\((d\\geq2)\\) is the spatial domain of the gas. If we instead adopt index notation the Boltzmann equation reads\n\\[\\begin{aligned}\n    \\frac{\\partial f}{\\partial t} + v_j\\frac{\\partial f}{\\partial x_j} = Q(f,f).\n\\end{aligned}\\]\nwhere summation over the term \\(v_j\\frac{\\partial f}{\\partial x_j}\\) is left implicit following Einstein’s summation convention. The operator \\(Q\\) appearing in the above is the Boltzmann collision operator, defined by the integral\n\\[\\begin{aligned}\n    Q(f,f) = \\int_{\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(f'f'_*-ff_*)\\,\\mathrm{d}\\sigma\\,\\mathrm{d}v_*,\n\\end{aligned}\\]\nwhere the function \\(B\\) is the so-called collision kernel, whose precise form depends on the nature of the inter-particle interactions being considered, and we have used the standard abbreviations \\(f'=f(t,x,v')\\), \\(f_*=f(t,x,v_*)\\) and \\(f'_*=f(t,x,v'_*)\\).\nHere we denote by \\((v',v'_*)\\) the precollisional velocities of a pair of particles who have velocities \\((v,v_*)\\) post-collision. By applying the conservation of momentum and energy we can relate the pairs \\((v',v'_*)\\) and \\((v,v_*)\\) via\n\\[\\begin{aligned}\n    \\begin{cases}\n    v' +v'_* = v + v_*\\\\\n    |v'|^2+|v'_*|^2 = |v|^2 +|v_*|^2\n    \\end{cases}\n\\end{aligned}\\]\nWe parameterise the solution space of this system via the so-called \\(\\mathbf{\\sigma}\\)-representation:\n\\[\\begin{aligned}\n\\begin{cases}\nv' = \\frac{v+v_*}{2} + \\frac{|v-v_*|}{2}\\sigma\\\\\n  v_*' = \\frac{v+v_*}{2} - \\frac{|v-v_*|}{2}\\sigma\n\\end{cases}   \n\\end{aligned}\\]\nwhere \\(\\sigma \\in \\mathbb{S}^{d-1}\\) is precisely the variable \\(\\sigma\\) integrated over in the above integral expression for \\(Q(f,f)\\). The Boltzmann operator \\(Q\\) admits a clear decomposition into a gain term \\(Q^+\\) and a loss term \\(Q^-\\):\n\\[\\begin{aligned}\nQ(f,f) = Q^+(f,f)-Q^-(f,f),\n\\end{aligned}\\]\nwhere \\(Q^+(f,f)\\) ‘counts’ all the collisions which result in a new particle moving with velocity \\(v\\) and \\(Q^-(f,f)\\) ‘counts’ all the collisions in which a particle of velocity \\(v\\) collides with another particle resulting in less particles at velocity \\(v\\). It is frequently convenient to view the Boltzmann operator as the quadratic form induced by the (non-symmetric) bilinear form\n\\[\\begin{aligned}\n    Q(g,f) = \\int_{\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(g'f'_*-gf_*)\\,\\mathrm{d}\\sigma\\,\\mathrm{d}v_*.\n\\end{aligned}\\]\nWhen the distribution function \\(f\\) is independent of the spatial variable \\(x\\) the full Boltzmann equation reduces to the spatially homogeneous Boltzmann equation\n\\[\\begin{aligned}\n    \\frac{\\partial f}{\\partial t} = Q(f,f).\n\\end{aligned}\\]\nIf an external force \\(F = F(x)\\) is applied to the system, we instead consider the more general Boltzmann equation with force term\n\\[\\begin{aligned}\n    \\frac{\\partial f}{\\partial t} + v\\cdot \\nabla_{x} f + F\\cdot\\nabla_{v} f = Q(f,f),\n\\end{aligned}\\]\nwhich in index notation reads as\n\\[\\begin{aligned}\n    \\frac{\\partial f}{\\partial t} + v_j\\frac{\\partial f}{\\partial x_j} + F_j\\frac{\\partial f}{\\partial v_j} =  Q(f,f).\n\\end{aligned}\\]\nGiven a solution \\(f\\) of the Boltzmann equation (without external force) we can define (in adimensional form) the local density \\(\\rho\\), the local macroscopic velocity \\(u\\) and the local temperature \\(T\\) by\n\\[\\begin{aligned}\n\\begin{split}\n    \\rho &= \\int_{\\mathbb{R}^d}f(t,x,v)\\,\\text{d}v,\\\\\n    u &= \\frac{1}{\\rho}\\int_{\\mathbb{R}^d}f(t,x,v)v\\,\\text{d}v,\\\\\n    T &= \\frac{1}{\\rho d}\\int_{\\mathbb{R}^d}f(t,x,v)|v-u|^2\\,\\text{d}v.\n\\end{split}\n\\end{aligned}\\]\nFor a spatially inhomogeneous, equilibrium solution of the Boltzmann equation (i.e. a density \\(f = f(x,v)\\) solving \\(v\\cdot \\nabla_{x} f = Q(f,f)\\)) we use these quantities to define the local Maxwellian distribution \\(M_{\\text{loc}}^f\\) associated to \\(f\\) by\n\\[\\begin{aligned}\n    M_{\\mathrm{loc}}^f(x,v) = \\frac{\\rho(x)}{(2\\pi T(x))^{d/2}}\\,\\mathrm{exp}\\left[-\\frac{1}{2T(x)}|v-u(x)|^2\\right].\n\\end{aligned}\\]\nFor a spatially homogeneous, equilibrium solution of the Boltzmann equation (i.e. a density \\(f = f(v)\\) solving \\(Q(f,f)=0\\)) \\(\\,\\rho,\\,u\\) and \\(T\\) are constant and correspond to the macroscopic density, velocity and temperature respectively.\nUsing these quantities we define the (global) Maxwellian distribution \\(M^f\\) associated to \\(f\\), which physically describes the state of thermodynamic equilibrium in which the gas is maximally diffused, by\n\\[\\begin{aligned}\n    M^f(v) = \\frac{\\rho}{(2\\pi T)^{d/2}}\\,\\mathrm{exp}\\left[-\\frac{1}{2T}|v-u|^2\\right].\n\\end{aligned}\\]\nIn the theory of the Boltzmann equation the following question is of central importance:\nUnder what conditions, and in what sense, do we have \\(f\\longrightarrow M^f\\) as \\(t \\longrightarrow \\infty\\)?\n\n\n\n\nFor there to be any hope of our problem being well-posed we need to supplement the Boltzmann equation with boundary conditions, modelling the interactions between the particles and the boundary \\(\\partial X\\) of the spatial domain \\(X\\).\nOf course, the only thing restricting the choice of such boundary conditions is our imagination. We briefly discuss some of the most popular and physically relevant choices.\n\nSpecular reflection: \\[\\begin{aligned}\n        f(x,R_xv)& = f(x,v),\\quad x\\in\\partial X,\\\\\n    \\end{aligned}\\] where\n\\[R_xv = v - 2(v\\cdot n(x))n(x)\\]\nand \\(n(x)\\) denotes the unit normal at \\(x\\in\\partial X\\). Physically, specular reflection corresponds to the particles elastically colliding with a static, hard wall. Although not particularly accurate, specular reflection is a natural first guess at a boundary condition as it avoids the complex question of modelling the interactions of the particles with the fine microscopic structure of the wall.\nBounce-back: \\[\\begin{aligned}\n        f(x,v) = f(x,-v),\\quad x\\in\\partial X.\n        \\end{aligned}\\] Physically, this condition simply states that particles reflect from the boundary with a velocity opposite to their incident velocity. Although clearly not particularly realistic, it occasionally leads to more physical results than specular reflection as it allows for the transfer of some tangential momentum to the wall, which is not allowed by specular reflection.\nMaxwellian diffusion: \\[\\begin{aligned}\n    f(x,v) &= \\rho_-(x)M_w(v), \\quad v\\cdot n(x)&gt;0,\\\\\n     \\rho_-(x) &= \\int_{v\\cdot n\\lt0}f(x,v)|v\\cdot n| \\,\\mathrm{d}v,\\\\\n    M_w(v) &= \\frac{1}{(2\\pi)^{\\frac{d-1}{2}}T_w^{\\frac{d+1}{2}}}\\,\\mathrm{exp}\\left(-\\frac{|v|^2}{2T_w}\\right).\n    \\end{aligned}\\]\n\n\n\n\n\nThe collision kernel \\(B\\) is related to the physical cross section \\(\\Sigma(v-v_*,\\sigma)\\) by the identity\n\\[\\begin{aligned}\n    B = |v-v_*|\\Sigma\n\\end{aligned}\\]\nOn physical grounds (Galilean invariance) it is assumed that the collision kernel \\(B\\) depends only on the magnitude of the relative velocity \\(|v-v_*|\\) and the cosine of the deviation angle \\(\\mathrm{cos}\\:\\theta = \\left\\langle \\frac{v-v_*}{|v-v_*|},\\sigma\\right\\rangle\\). For this reason it is common to abuse notation by writing\n\\[\\begin{aligned}\n    B(v-v_*,\\sigma) = B(|v-v_*|,\\mathrm{cos}\\:\\theta)\n\\end{aligned}\\]\nto emphasise the specific forms that collision kernels can take. Maxwell showed that for a given impact parameter \\(p\\geq0\\) and relative velocity \\(z\\in\\mathbb{R}^3\\) the collision kernel is implicitly given by\n\\[\\begin{aligned}\n    B(|z|,\\mathrm{cos}\\:\\theta) = \\frac{p}{\\mathrm{sin}\\theta}\\frac{\\text{d}p}{\\text{d}\\theta}|z|.\n\\end{aligned}\\]\nThis can be made explicit in the crucially important example of a gas of hard spheres, where the gas particles are treated as spheres of fixed radius that interact via elastic collisions (‘billiard balls’). For hard spheres in \\(d = 3\\) the cross section is constant, and thus the collision kernel is given by\n\\[\\begin{aligned}\n    B = |v-v_*|.\n\\end{aligned}\\]\nIn the case of particles interacting by an inverse \\(s\\)-power law force the collision kernel factorises as\n\\[\\begin{aligned}\n\\begin{split}\n    B(v-v_*,\\cos\\theta) &= \\Psi(|v-v_*|)\\,b(\\cos\\theta)\\\\\n    &= |v-v_*|^{\\gamma}\\,b(\\cos\\theta),\n\\end{split}\n\\end{aligned}\\]\nwhere \\(\\Psi(|z|) = |z|^\\gamma\\) is conventionally called the kinetic collision kernel,\n\\(\\gamma = (s-5)/(s-1)\\) in dimension \\(d = 3\\) and \\(b(\\cos\\theta)\\) is conventionally called the angular collision kernel. The the function \\(b\\) is typically complicated, smooth away from \\(0\\) and is only known implicitly.\nSuch collision kernels are often further classified by the values of \\(\\gamma\\) as follows:\n\n\\(\\gamma &gt; 0\\) : hard potentials\n\\(\\gamma &lt; 0\\) : soft potentials\n\\(\\gamma = 0\\) : Maxwellian potentials\n\nThe edge case of Maxwellian potentials is interesting as in such cases the collision kernel has no kinetic part and only depends on the cosine of the deviation angle. This occurs for particles interacting via an inverse \\((2d-1)\\)-power law force in \\(\\mathbb{R}^d\\) (e.g. a force like \\(r^{-5}\\) in \\(\\mathbb{R}^3\\)). Such particles are called Maxwellian molecules and should only be considered a theoretical construction.\nA crucial assumption frequently made in analytical treatments of the Boltzmann equation is Grad’s angular cut-off, which simply assumes that the angular collision kernel \\(b\\) is integrable:\n\\[\\begin{aligned}\n\\int_{\\mathbb{S}^{d-1}}b(k\\cdot\\sigma)\\,\\text{d}\\sigma = \\left|\\mathbb{S}^{d-2}\\right|\\int_0^{\\pi}b(\\cos\\theta)\\sin^{d-2}\\theta\\,\\text{d}\\theta&lt;\\infty.\n\\end{aligned}\\]\n\n\n\nFor a probability density \\(f\\) on \\(X \\times \\mathbb{R}^d\\) define Boltzmann’s H functional by \\[\\begin{aligned}\n    H(f)\n= \\int_{X\\times\\mathbb{R}^d}f\\log f\\,\\text{d} x\\,\\text{d} v.\n\\end{aligned}\\]\n\\(H(f)\\) is well-defined in \\(\\mathbb{R}\\cup\\{\\infty\\}\\) provided that \\[\\int f(x,v)|v|^2\\,\\text{d} x\\,\\text{d} v &lt; \\infty,\\] i.e. if \\(f\\) has finite energy. We note that up to a change of sign the \\(H\\) functional is just the differential entropy of \\(f\\), to be defined in a future post.\nBoltzmann’s H theorem loosely states that \\(H(f)\\) is a quantity monotonically decreasing (non-increasing) in time, and is stationary if and only if \\(f\\) is a Maxwellian. Making this precise is a task more technical that it might first appear.\n\n\n\nLet \\((f_t)_{t\\geq0}\\) be a well-behaved (smooth) solution of the Boltzmann equation (in particular with finite entropy), with either periodic, bounce-back or specular boundary conditions. In the latter case assume further that \\(d = 2 \\text{ or } 3\\) and the spatial domain \\(X\\) has no axis of symmetry. Then:\n\n\\[\\frac{\\text{d}}{\\text{d} t} H(f_t) \\leq 0.\\] Moreover, one can define another functional \\(D\\) on \\(L^1\\left(\\mathbb{R}^d\\right)\\) called the such that \\[\\frac{\\text{d}}{\\text{d} t} H(f_t) = -\\int_X D(f_t(x,\\cdot\\,)\\,\\text{d} x.\\]\nAssume that \\(B(v-v_*,\\sigma)&gt;0\\) for a.e. \\((v,v_*,\\sigma)\\in\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}\\) (this is always true in cases of physical interest).\nLet \\(f(x,v)\\) be a probability density on \\(X\\times\\mathbb{R}^d\\) with finite energy, \\(\\int f(x,v)|v|^2\\,\\text{d} x\\,\\text{d} v &lt;\\infty.\\) Then:\n\\[\\int_X D(f(x,\\cdot\\,)\\text{d} x = 0\\,\\, \\Longleftrightarrow\\,\\, f\\; \\text{is a local Maxwellian}, \\text{ i.e. } f = M_{\\mathrm{loc}}^f.\\]\n\\[\\begin{aligned}\n        (f_t)_{t\\geq0} \\text{ is stationary }\\,\\,&\\Longleftrightarrow\\,\\, D(f_t(x,\\cdot\\,) = 0\\quad\\forall\\, t\\geq 0\\\\\n        &\\Longleftrightarrow\\,\\, f_t \\text{ is a global Maxwellian, i.e. } f_t(x,v) = M^f(v) \\quad\\forall\\, t\\geq 0.\n\\end{aligned}\\]\n\nProof.\nSee Theorem 1, section 1.1.2 in Rezakhanlou, Villani & Golse [3.].\n\n\n\n\n\nThe entropy dissipation functional \\(D\\) introduced in Theorem 1 can be expressed as\n\\[\\begin{aligned}\n   D(f) = \\frac{1}{4}\\int_{\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(f'f'_*-ff_*)\\mathrm{log}\\,\\frac{f'f'_*}{ff_*}\\,\\text{d}\\sigma\\,\\text{d} v_*\\,\\text{d} v.\n\\end{aligned}\\]\nAnd formally satisfies\n\\[\\frac{\\text{d}}{\\text{d} t} H(f_t) = - \\int D(f_t(x,\\cdot\\,)\\,\\text{d} x\\]\nin the spatially inhomogeneous case and\n\\[\\frac{\\text{d}}{\\text{d} t} H(f_t) = D(f_t),\\]\nin the spatially homogeneous case.\nThe derivation of the above formula for \\(D\\) relies on the following lemma used to symmetrize the Boltzmann operator \\(Q\\):\n\n\n\nThe change of variables interchanging the primed and unprimed velocities\n\\[\\begin{aligned}\n    (v,v_*,\\sigma) \\longmapsto (v',v'_*,k)\\\\\n\\end{aligned}\\]\nwhere\n\\[\\begin{aligned}\nk &= \\frac{v-v_*}{|v-v_*|}\\\\\n\\\\\n\\sigma &= \\frac{v'-v'_*}{|v'-v'_*|}\n\\end{aligned}\\]\nis involutive (self-inverse) and has unit Jacobian determinant.\nSimilarly, the change of variables interchanging the starred and unstarred velocities \\((v,v_*,v',v'_*)\\longmapsto(v_*,v,v'_*,v')\\) is also involutive with unit Jacobian.\n\nMorally, this lemma simply states than under an integral sign one can interchange primed and unprimed velocities \\((v,v_*) \\longmapsto (v',v'_*)\\) and starred and unstarred velocities \\((v,v_*) \\longmapsto (v_*,v)\\) at will. Physically, this property relies on the time- and space-reversal symmetry of the microscopic dynamics.\nRepeated application of Lemma 1 to an integral of the form \\(\\int Q(f,f)\\phi\\) for an arbitrary continuous function \\(\\phi\\) of velocity gives\n\\[\\begin{aligned}\n\\int_{\\mathbb{R}^d} Q(f,f)\\phi\\,dv &= \\int_{\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(f'f'_*-ff_*)\\phi\\;\\text{d}\\sigma\\,\\text{d}v_*\\text{d}v\\\\\n&=\\int_{\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(ff_*)(\\phi'-\\phi)\\;\\text{d}\\sigma\\,\\text{d}v_*\\text{d}v\\\\\n&=\\frac{1}{2}\\int_{\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(ff_*)(\\phi'+\\phi'_*-\\phi-\\phi_*)\\;\\text{d}\\sigma\\,\\text{d}v_*\\text{d}v\\\\\n&=-\\frac{1}{4}\\int_{\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(f'f'_*-ff_*)(\\phi'+\\phi'_*-\\phi-\\phi_*)\\;\\text{d}\\sigma\\,\\text{d}v_*\\text{d}v\\\\\n\\end{aligned}\\]\nTaking \\(\\phi = \\mathrm{log}\\,f\\) yields the above integral formula for \\(D\\). Moreover, we see from the last line of the above that for \\(\\phi\\) satisfying the functional equation\n\\[\\begin{aligned}\n    \\phi + \\phi_* = \\phi' + \\phi'_*\n\\end{aligned}\\]\nwe have, at least formally, the conservation law:\n\\[\\begin{aligned}\n        \\frac{\\text{d}}{\\text{d} t}\\int f(t,x,v)\\phi(v)\\,\\text{d} x\\,\\text{d} v = 0.\n\\end{aligned}\\]\nSuch functions \\(\\phi\\) are called collision invariants as they correspond to quantities conserved by the microscopic dynamics.\nThe functional equation \\(\\phi + \\phi_* = \\phi' + \\phi'_*\\) has been solved under progressively weaker and weaker assumptions on \\(\\phi\\), each time concluding that the most general solution is\n\\[\\phi(v) = A + B\\cdot v + C|v|^2\\]\nfor some \\(A,\\,C\\in\\mathbb{R}\\), \\(B\\in\\mathbb{R}^d.\\)\nThus, all collision invariants can be written as a linear combination of the so-called elementary collision invariants\n\\[\\begin{aligned}\n    \\phi(v) = 1, v_1,\\dots,v_d,\\frac{v^2}{2},\n\\end{aligned}\\]\nwhich correspond to the conservation of mass, the conservation of each of the \\(d\\) components of momentum and the conservation of energy respectively."
  },
  {
    "objectID": "posts/bte1/index.html#the-evolution-equations",
    "href": "posts/bte1/index.html#the-evolution-equations",
    "title": "The Boltzmann Equation - 1. Introduction",
    "section": "",
    "text": "The Boltzmann equation (also known as the Boltzmann transport equation) models the behaviour of a gas comprised of a single particle species, sufficiently dilute so that quantum effects are negligible and when all inter-particle interactions are assumed to be elastic binary collisions:\n\\[\\begin{aligned}\n\\frac{\\partial f}{\\partial t} + v\\cdot \\nabla_{x} f = Q(f,f),\\quad x\\in X,\\: v\\in \\mathbb{R}^d,\\: t\\geq0.\n\\end{aligned}\\]\nWhere \\(f=f(t,x,v)=f_t(x,v)\\) is the particle distribution function in phase space and \\(X \\subset \\mathbb{R}^d\\) \\((d\\geq2)\\) is the spatial domain of the gas. If we instead adopt index notation the Boltzmann equation reads\n\\[\\begin{aligned}\n    \\frac{\\partial f}{\\partial t} + v_j\\frac{\\partial f}{\\partial x_j} = Q(f,f).\n\\end{aligned}\\]\nwhere summation over the term \\(v_j\\frac{\\partial f}{\\partial x_j}\\) is left implicit following Einstein’s summation convention. The operator \\(Q\\) appearing in the above is the Boltzmann collision operator, defined by the integral\n\\[\\begin{aligned}\n    Q(f,f) = \\int_{\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(f'f'_*-ff_*)\\,\\mathrm{d}\\sigma\\,\\mathrm{d}v_*,\n\\end{aligned}\\]\nwhere the function \\(B\\) is the so-called collision kernel, whose precise form depends on the nature of the inter-particle interactions being considered, and we have used the standard abbreviations \\(f'=f(t,x,v')\\), \\(f_*=f(t,x,v_*)\\) and \\(f'_*=f(t,x,v'_*)\\).\nHere we denote by \\((v',v'_*)\\) the precollisional velocities of a pair of particles who have velocities \\((v,v_*)\\) post-collision. By applying the conservation of momentum and energy we can relate the pairs \\((v',v'_*)\\) and \\((v,v_*)\\) via\n\\[\\begin{aligned}\n    \\begin{cases}\n    v' +v'_* = v + v_*\\\\\n    |v'|^2+|v'_*|^2 = |v|^2 +|v_*|^2\n    \\end{cases}\n\\end{aligned}\\]\nWe parameterise the solution space of this system via the so-called \\(\\mathbf{\\sigma}\\)-representation:\n\\[\\begin{aligned}\n\\begin{cases}\nv' = \\frac{v+v_*}{2} + \\frac{|v-v_*|}{2}\\sigma\\\\\n  v_*' = \\frac{v+v_*}{2} - \\frac{|v-v_*|}{2}\\sigma\n\\end{cases}   \n\\end{aligned}\\]\nwhere \\(\\sigma \\in \\mathbb{S}^{d-1}\\) is precisely the variable \\(\\sigma\\) integrated over in the above integral expression for \\(Q(f,f)\\). The Boltzmann operator \\(Q\\) admits a clear decomposition into a gain term \\(Q^+\\) and a loss term \\(Q^-\\):\n\\[\\begin{aligned}\nQ(f,f) = Q^+(f,f)-Q^-(f,f),\n\\end{aligned}\\]\nwhere \\(Q^+(f,f)\\) ‘counts’ all the collisions which result in a new particle moving with velocity \\(v\\) and \\(Q^-(f,f)\\) ‘counts’ all the collisions in which a particle of velocity \\(v\\) collides with another particle resulting in less particles at velocity \\(v\\). It is frequently convenient to view the Boltzmann operator as the quadratic form induced by the (non-symmetric) bilinear form\n\\[\\begin{aligned}\n    Q(g,f) = \\int_{\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(g'f'_*-gf_*)\\,\\mathrm{d}\\sigma\\,\\mathrm{d}v_*.\n\\end{aligned}\\]\nWhen the distribution function \\(f\\) is independent of the spatial variable \\(x\\) the full Boltzmann equation reduces to the spatially homogeneous Boltzmann equation\n\\[\\begin{aligned}\n    \\frac{\\partial f}{\\partial t} = Q(f,f).\n\\end{aligned}\\]\nIf an external force \\(F = F(x)\\) is applied to the system, we instead consider the more general Boltzmann equation with force term\n\\[\\begin{aligned}\n    \\frac{\\partial f}{\\partial t} + v\\cdot \\nabla_{x} f + F\\cdot\\nabla_{v} f = Q(f,f),\n\\end{aligned}\\]\nwhich in index notation reads as\n\\[\\begin{aligned}\n    \\frac{\\partial f}{\\partial t} + v_j\\frac{\\partial f}{\\partial x_j} + F_j\\frac{\\partial f}{\\partial v_j} =  Q(f,f).\n\\end{aligned}\\]\nGiven a solution \\(f\\) of the Boltzmann equation (without external force) we can define (in adimensional form) the local density \\(\\rho\\), the local macroscopic velocity \\(u\\) and the local temperature \\(T\\) by\n\\[\\begin{aligned}\n\\begin{split}\n    \\rho &= \\int_{\\mathbb{R}^d}f(t,x,v)\\,\\text{d}v,\\\\\n    u &= \\frac{1}{\\rho}\\int_{\\mathbb{R}^d}f(t,x,v)v\\,\\text{d}v,\\\\\n    T &= \\frac{1}{\\rho d}\\int_{\\mathbb{R}^d}f(t,x,v)|v-u|^2\\,\\text{d}v.\n\\end{split}\n\\end{aligned}\\]\nFor a spatially inhomogeneous, equilibrium solution of the Boltzmann equation (i.e. a density \\(f = f(x,v)\\) solving \\(v\\cdot \\nabla_{x} f = Q(f,f)\\)) we use these quantities to define the local Maxwellian distribution \\(M_{\\text{loc}}^f\\) associated to \\(f\\) by\n\\[\\begin{aligned}\n    M_{\\mathrm{loc}}^f(x,v) = \\frac{\\rho(x)}{(2\\pi T(x))^{d/2}}\\,\\mathrm{exp}\\left[-\\frac{1}{2T(x)}|v-u(x)|^2\\right].\n\\end{aligned}\\]\nFor a spatially homogeneous, equilibrium solution of the Boltzmann equation (i.e. a density \\(f = f(v)\\) solving \\(Q(f,f)=0\\)) \\(\\,\\rho,\\,u\\) and \\(T\\) are constant and correspond to the macroscopic density, velocity and temperature respectively.\nUsing these quantities we define the (global) Maxwellian distribution \\(M^f\\) associated to \\(f\\), which physically describes the state of thermodynamic equilibrium in which the gas is maximally diffused, by\n\\[\\begin{aligned}\n    M^f(v) = \\frac{\\rho}{(2\\pi T)^{d/2}}\\,\\mathrm{exp}\\left[-\\frac{1}{2T}|v-u|^2\\right].\n\\end{aligned}\\]\nIn the theory of the Boltzmann equation the following question is of central importance:\nUnder what conditions, and in what sense, do we have \\(f\\longrightarrow M^f\\) as \\(t \\longrightarrow \\infty\\)?"
  },
  {
    "objectID": "posts/bte1/index.html#boundary-conditions",
    "href": "posts/bte1/index.html#boundary-conditions",
    "title": "The Boltzmann Equation - 1. Introduction",
    "section": "",
    "text": "For there to be any hope of our problem being well-posed we need to supplement the Boltzmann equation with boundary conditions, modelling the interactions between the particles and the boundary \\(\\partial X\\) of the spatial domain \\(X\\).\nOf course, the only thing restricting the choice of such boundary conditions is our imagination. We briefly discuss some of the most popular and physically relevant choices.\n\nSpecular reflection: \\[\\begin{aligned}\n        f(x,R_xv)& = f(x,v),\\quad x\\in\\partial X,\\\\\n    \\end{aligned}\\] where\n\\[R_xv = v - 2(v\\cdot n(x))n(x)\\]\nand \\(n(x)\\) denotes the unit normal at \\(x\\in\\partial X\\). Physically, specular reflection corresponds to the particles elastically colliding with a static, hard wall. Although not particularly accurate, specular reflection is a natural first guess at a boundary condition as it avoids the complex question of modelling the interactions of the particles with the fine microscopic structure of the wall.\nBounce-back: \\[\\begin{aligned}\n        f(x,v) = f(x,-v),\\quad x\\in\\partial X.\n        \\end{aligned}\\] Physically, this condition simply states that particles reflect from the boundary with a velocity opposite to their incident velocity. Although clearly not particularly realistic, it occasionally leads to more physical results than specular reflection as it allows for the transfer of some tangential momentum to the wall, which is not allowed by specular reflection.\nMaxwellian diffusion: \\[\\begin{aligned}\n    f(x,v) &= \\rho_-(x)M_w(v), \\quad v\\cdot n(x)&gt;0,\\\\\n     \\rho_-(x) &= \\int_{v\\cdot n\\lt0}f(x,v)|v\\cdot n| \\,\\mathrm{d}v,\\\\\n    M_w(v) &= \\frac{1}{(2\\pi)^{\\frac{d-1}{2}}T_w^{\\frac{d+1}{2}}}\\,\\mathrm{exp}\\left(-\\frac{|v|^2}{2T_w}\\right).\n    \\end{aligned}\\]"
  },
  {
    "objectID": "posts/bte1/index.html#collision-kernels",
    "href": "posts/bte1/index.html#collision-kernels",
    "title": "The Boltzmann Equation - 1. Introduction",
    "section": "",
    "text": "The collision kernel \\(B\\) is related to the physical cross section \\(\\Sigma(v-v_*,\\sigma)\\) by the identity\n\\[\\begin{aligned}\n    B = |v-v_*|\\Sigma\n\\end{aligned}\\]\nOn physical grounds (Galilean invariance) it is assumed that the collision kernel \\(B\\) depends only on the magnitude of the relative velocity \\(|v-v_*|\\) and the cosine of the deviation angle \\(\\mathrm{cos}\\:\\theta = \\left\\langle \\frac{v-v_*}{|v-v_*|},\\sigma\\right\\rangle\\). For this reason it is common to abuse notation by writing\n\\[\\begin{aligned}\n    B(v-v_*,\\sigma) = B(|v-v_*|,\\mathrm{cos}\\:\\theta)\n\\end{aligned}\\]\nto emphasise the specific forms that collision kernels can take. Maxwell showed that for a given impact parameter \\(p\\geq0\\) and relative velocity \\(z\\in\\mathbb{R}^3\\) the collision kernel is implicitly given by\n\\[\\begin{aligned}\n    B(|z|,\\mathrm{cos}\\:\\theta) = \\frac{p}{\\mathrm{sin}\\theta}\\frac{\\text{d}p}{\\text{d}\\theta}|z|.\n\\end{aligned}\\]\nThis can be made explicit in the crucially important example of a gas of hard spheres, where the gas particles are treated as spheres of fixed radius that interact via elastic collisions (‘billiard balls’). For hard spheres in \\(d = 3\\) the cross section is constant, and thus the collision kernel is given by\n\\[\\begin{aligned}\n    B = |v-v_*|.\n\\end{aligned}\\]\nIn the case of particles interacting by an inverse \\(s\\)-power law force the collision kernel factorises as\n\\[\\begin{aligned}\n\\begin{split}\n    B(v-v_*,\\cos\\theta) &= \\Psi(|v-v_*|)\\,b(\\cos\\theta)\\\\\n    &= |v-v_*|^{\\gamma}\\,b(\\cos\\theta),\n\\end{split}\n\\end{aligned}\\]\nwhere \\(\\Psi(|z|) = |z|^\\gamma\\) is conventionally called the kinetic collision kernel,\n\\(\\gamma = (s-5)/(s-1)\\) in dimension \\(d = 3\\) and \\(b(\\cos\\theta)\\) is conventionally called the angular collision kernel. The the function \\(b\\) is typically complicated, smooth away from \\(0\\) and is only known implicitly.\nSuch collision kernels are often further classified by the values of \\(\\gamma\\) as follows:\n\n\\(\\gamma &gt; 0\\) : hard potentials\n\\(\\gamma &lt; 0\\) : soft potentials\n\\(\\gamma = 0\\) : Maxwellian potentials\n\nThe edge case of Maxwellian potentials is interesting as in such cases the collision kernel has no kinetic part and only depends on the cosine of the deviation angle. This occurs for particles interacting via an inverse \\((2d-1)\\)-power law force in \\(\\mathbb{R}^d\\) (e.g. a force like \\(r^{-5}\\) in \\(\\mathbb{R}^3\\)). Such particles are called Maxwellian molecules and should only be considered a theoretical construction.\nA crucial assumption frequently made in analytical treatments of the Boltzmann equation is Grad’s angular cut-off, which simply assumes that the angular collision kernel \\(b\\) is integrable:\n\\[\\begin{aligned}\n\\int_{\\mathbb{S}^{d-1}}b(k\\cdot\\sigma)\\,\\text{d}\\sigma = \\left|\\mathbb{S}^{d-2}\\right|\\int_0^{\\pi}b(\\cos\\theta)\\sin^{d-2}\\theta\\,\\text{d}\\theta&lt;\\infty.\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/bte1/index.html#the-h-theorem",
    "href": "posts/bte1/index.html#the-h-theorem",
    "title": "The Boltzmann Equation - 1. Introduction",
    "section": "",
    "text": "For a probability density \\(f\\) on \\(X \\times \\mathbb{R}^d\\) define Boltzmann’s H functional by \\[\\begin{aligned}\n    H(f)\n= \\int_{X\\times\\mathbb{R}^d}f\\log f\\,\\text{d} x\\,\\text{d} v.\n\\end{aligned}\\]\n\\(H(f)\\) is well-defined in \\(\\mathbb{R}\\cup\\{\\infty\\}\\) provided that \\[\\int f(x,v)|v|^2\\,\\text{d} x\\,\\text{d} v &lt; \\infty,\\] i.e. if \\(f\\) has finite energy. We note that up to a change of sign the \\(H\\) functional is just the differential entropy of \\(f\\), to be defined in a future post.\nBoltzmann’s H theorem loosely states that \\(H(f)\\) is a quantity monotonically decreasing (non-increasing) in time, and is stationary if and only if \\(f\\) is a Maxwellian. Making this precise is a task more technical that it might first appear.\n\n\n\nLet \\((f_t)_{t\\geq0}\\) be a well-behaved (smooth) solution of the Boltzmann equation (in particular with finite entropy), with either periodic, bounce-back or specular boundary conditions. In the latter case assume further that \\(d = 2 \\text{ or } 3\\) and the spatial domain \\(X\\) has no axis of symmetry. Then:\n\n\\[\\frac{\\text{d}}{\\text{d} t} H(f_t) \\leq 0.\\] Moreover, one can define another functional \\(D\\) on \\(L^1\\left(\\mathbb{R}^d\\right)\\) called the such that \\[\\frac{\\text{d}}{\\text{d} t} H(f_t) = -\\int_X D(f_t(x,\\cdot\\,)\\,\\text{d} x.\\]\nAssume that \\(B(v-v_*,\\sigma)&gt;0\\) for a.e. \\((v,v_*,\\sigma)\\in\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}\\) (this is always true in cases of physical interest).\nLet \\(f(x,v)\\) be a probability density on \\(X\\times\\mathbb{R}^d\\) with finite energy, \\(\\int f(x,v)|v|^2\\,\\text{d} x\\,\\text{d} v &lt;\\infty.\\) Then:\n\\[\\int_X D(f(x,\\cdot\\,)\\text{d} x = 0\\,\\, \\Longleftrightarrow\\,\\, f\\; \\text{is a local Maxwellian}, \\text{ i.e. } f = M_{\\mathrm{loc}}^f.\\]\n\\[\\begin{aligned}\n        (f_t)_{t\\geq0} \\text{ is stationary }\\,\\,&\\Longleftrightarrow\\,\\, D(f_t(x,\\cdot\\,) = 0\\quad\\forall\\, t\\geq 0\\\\\n        &\\Longleftrightarrow\\,\\, f_t \\text{ is a global Maxwellian, i.e. } f_t(x,v) = M^f(v) \\quad\\forall\\, t\\geq 0.\n\\end{aligned}\\]\n\nProof.\nSee Theorem 1, section 1.1.2 in Rezakhanlou, Villani & Golse [3.]."
  },
  {
    "objectID": "posts/bte1/index.html#collision-invariants-and-conservation-laws",
    "href": "posts/bte1/index.html#collision-invariants-and-conservation-laws",
    "title": "The Boltzmann Equation - 1. Introduction",
    "section": "",
    "text": "The entropy dissipation functional \\(D\\) introduced in Theorem 1 can be expressed as\n\\[\\begin{aligned}\n   D(f) = \\frac{1}{4}\\int_{\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(f'f'_*-ff_*)\\mathrm{log}\\,\\frac{f'f'_*}{ff_*}\\,\\text{d}\\sigma\\,\\text{d} v_*\\,\\text{d} v.\n\\end{aligned}\\]\nAnd formally satisfies\n\\[\\frac{\\text{d}}{\\text{d} t} H(f_t) = - \\int D(f_t(x,\\cdot\\,)\\,\\text{d} x\\]\nin the spatially inhomogeneous case and\n\\[\\frac{\\text{d}}{\\text{d} t} H(f_t) = D(f_t),\\]\nin the spatially homogeneous case.\nThe derivation of the above formula for \\(D\\) relies on the following lemma used to symmetrize the Boltzmann operator \\(Q\\):\n\n\n\nThe change of variables interchanging the primed and unprimed velocities\n\\[\\begin{aligned}\n    (v,v_*,\\sigma) \\longmapsto (v',v'_*,k)\\\\\n\\end{aligned}\\]\nwhere\n\\[\\begin{aligned}\nk &= \\frac{v-v_*}{|v-v_*|}\\\\\n\\\\\n\\sigma &= \\frac{v'-v'_*}{|v'-v'_*|}\n\\end{aligned}\\]\nis involutive (self-inverse) and has unit Jacobian determinant.\nSimilarly, the change of variables interchanging the starred and unstarred velocities \\((v,v_*,v',v'_*)\\longmapsto(v_*,v,v'_*,v')\\) is also involutive with unit Jacobian.\n\nMorally, this lemma simply states than under an integral sign one can interchange primed and unprimed velocities \\((v,v_*) \\longmapsto (v',v'_*)\\) and starred and unstarred velocities \\((v,v_*) \\longmapsto (v_*,v)\\) at will. Physically, this property relies on the time- and space-reversal symmetry of the microscopic dynamics.\nRepeated application of Lemma 1 to an integral of the form \\(\\int Q(f,f)\\phi\\) for an arbitrary continuous function \\(\\phi\\) of velocity gives\n\\[\\begin{aligned}\n\\int_{\\mathbb{R}^d} Q(f,f)\\phi\\,dv &= \\int_{\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(f'f'_*-ff_*)\\phi\\;\\text{d}\\sigma\\,\\text{d}v_*\\text{d}v\\\\\n&=\\int_{\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(ff_*)(\\phi'-\\phi)\\;\\text{d}\\sigma\\,\\text{d}v_*\\text{d}v\\\\\n&=\\frac{1}{2}\\int_{\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(ff_*)(\\phi'+\\phi'_*-\\phi-\\phi_*)\\;\\text{d}\\sigma\\,\\text{d}v_*\\text{d}v\\\\\n&=-\\frac{1}{4}\\int_{\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(f'f'_*-ff_*)(\\phi'+\\phi'_*-\\phi-\\phi_*)\\;\\text{d}\\sigma\\,\\text{d}v_*\\text{d}v\\\\\n\\end{aligned}\\]\nTaking \\(\\phi = \\mathrm{log}\\,f\\) yields the above integral formula for \\(D\\). Moreover, we see from the last line of the above that for \\(\\phi\\) satisfying the functional equation\n\\[\\begin{aligned}\n    \\phi + \\phi_* = \\phi' + \\phi'_*\n\\end{aligned}\\]\nwe have, at least formally, the conservation law:\n\\[\\begin{aligned}\n        \\frac{\\text{d}}{\\text{d} t}\\int f(t,x,v)\\phi(v)\\,\\text{d} x\\,\\text{d} v = 0.\n\\end{aligned}\\]\nSuch functions \\(\\phi\\) are called collision invariants as they correspond to quantities conserved by the microscopic dynamics.\nThe functional equation \\(\\phi + \\phi_* = \\phi' + \\phi'_*\\) has been solved under progressively weaker and weaker assumptions on \\(\\phi\\), each time concluding that the most general solution is\n\\[\\phi(v) = A + B\\cdot v + C|v|^2\\]\nfor some \\(A,\\,C\\in\\mathbb{R}\\), \\(B\\in\\mathbb{R}^d.\\)\nThus, all collision invariants can be written as a linear combination of the so-called elementary collision invariants\n\\[\\begin{aligned}\n    \\phi(v) = 1, v_1,\\dots,v_d,\\frac{v^2}{2},\n\\end{aligned}\\]\nwhich correspond to the conservation of mass, the conservation of each of the \\(d\\) components of momentum and the conservation of energy respectively."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is made using Quarto, an open source technical publishing system that provides the functionality to render .ipynb files as blog posts, and hosted with GitHub pages.\nThe .ipynb files are written in Python3 using Jupyter Lab.\nThe mathematics posts are typically written in LaTeX and converted to markdown using Pandoc."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "On projected particles passing through the same point - S1-Q9-03\n\n\n\n\n\n\n\nTuition\n\n\nMathematics\n\n\nMechanics\n\n\nPython\n\n\nSymPy\n\n\n\n\nRead an in-depth solution of Q9 from STEP1 2003, including animations implemented in Python.\n\n\n\n\n\n\nApr 30, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nGenerative Adversarial Networks in PyTorch with PyTorch Lightning\n\n\n\n\n\n\n\nPython\n\n\nML\n\n\nDeep Learning\n\n\nPyTorch\n\n\nGenAI\n\n\n\n\nA Generative Adversarial Network (GAN) is implemented in PyTorch with PyTorch Lightning to generate greyscale 64 x 64 images of items of clothing. Several refinements have been made to improve the quality of the generated images, such as the inclusion of batch normalisation layers in the generator, leaky-ReLU activation in the hidden layers of the discriminator and data augmentation via affine transformations and reflections.\n\n\n\n\n\n\nApr 30, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nConvolutional Neural Networks in TensorFlow\n\n\n\n\n\n\n\nPython\n\n\nML\n\n\nDeep Learning\n\n\nTensorFlow\n\n\n\n\nConvolutional Neural Networks (CNNs) are implemented in TensorFlow as instances of built classes CNN and CNN_Reg that inherit from tensorflow.keras.Model. The resulting models are used in a multiclass classification problem, detecting how many fingers are raised on an image of a hand. By adding more convolutional layers, dense layers, and regularization we increase the test accuracy from 85.00% to 95.83% at the expense of increasing training time from ~15 seconds to ~40 minutes.\n\n\n\n\n\n\nApr 28, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nDeep Neural Networks in NumPy with BGD and Adam Optimizers\n\n\n\n\n\n\n\nPython\n\n\nML\n\n\nDeep Learning\n\n\n\n\nDeep, fully-connected neural networks with an arbitary number of layers are built from scratch (in NumPy). Both batch gradient descent (BGD) and Adam optimizers are implmented for training. Classes L_Layer_NN_GradientDescent and L_Layer_NN_AdamOptimizer are built that inherit from a common BaseModel. The resulting models are used to identify cats in an image classification problem.\n\n\n\n\n\n\nApr 22, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nVariational AutoEncoders in PyTorch\n\n\n\n\n\n\n\nPython\n\n\nML\n\n\nDeep Learning\n\n\nPyTorch\n\n\nGenAI\n\n\n\n\nVariational AutoEncoders (VAEs) are implemented in PyTorch. The focus is on the implementation and comparison of VAEs with either 1 or 2 hidden layers in the encoder and decoder. The VAEs are trained on the MNIST dataset and are then used to generate new examples of handwritten digits.\n\n\n\n\n\n\nApr 14, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nJoining Demographic Statistics onto a Dataset of Charity Donors with Python\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nfolium\n\n\nscikit-learn\n\n\nEmmanuel House\n\n\n\n\nLearn how to read ONS lookup tables into Pandas to convert postcodes into local demographic statistics, such as population density and Index of Multiple Deprivation (IMD) score. Learn how to use KNNImputer from scikit-learn to fill missing values. See how these statistics correlate with donation history for a dataset of charity donors.\n\n\n\n\n\n\nApr 8, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nThe Boltzmann Equation - 4. Maximum Entropy\n\n\n\n\n\n\n\nMathematics\n\n\nProbability Theory\n\n\nInformation Theory\n\n\nBoltzmann Equation\n\n\n\n\nWe prove Cover’s theorem from ‘Elements of Information Theory’ that the distribution function maximising the entropy over functions with given moment contraints takes the form of an exponential function. This result, combined with the H-theorem, provides a rigorous justification for our physical belief that the limiting form of a solution of the Boltzmann equation is a Maxwell-Boltzmann distribution.\n\n\n\n\n\n\nApr 4, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nDonor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning\n\n\n\n\n\n\n\nPython\n\n\nML\n\n\nPandas\n\n\nscikit-learn\n\n\nfolium\n\n\nKMeans\n\n\nGMM\n\n\nBIRCH\n\n\nClustering\n\n\nEmmanuel House\n\n\n\n\nRead a Jupyter notebook from my machine learning project for Emmanuel House Support Centre: ‘Donor segmentation using an ensemble of clustering algorithms’.\n\n\n\n\n\n\nApr 2, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nGeographic Analysis of Charity Donors - Latest Leaflet Maps\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nfolium\n\n\nGeoPy\n\n\nEmmanuel House\n\n\n\n\nExplore my latest interactive map displaying the geographic distribution of a artificial dataset of donors to a Nottingham charity. The geodesic distance from the donor to the charity is engineered using GeoPy.\n\n\n\n\n\n\nMar 31, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nThe Boltzmann Equation - 3. Information Theory\n\n\n\n\n\n\n\nMathematics\n\n\nProbability Theory\n\n\nInformation Theory\n\n\nBoltzmann Equation\n\n\n\n\nWe introduce key notions from information theory such as differential entropy and the Kullback-Liebler (KL) divergence and prove the information inequality.\n\n\n\n\n\n\nMar 23, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nModel Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction\n\n\n\n\n\n\n\nPython\n\n\nML\n\n\nPandas\n\n\nSeaborn\n\n\nscikit-learn\n\n\nTensorFlow\n\n\nXGBoost\n\n\nOptuna\n\n\n\n\nAfter cleaning and preprocessing a modified LendingClub dataset of loan applicants I implement an Optuna study for both model selection and hyperparameter tuning with cross-validation to choose a model to predict if an unseen applicant will repay their loan.\n\n\n\n\n\n\nMar 19, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nVisualising the Geographic Distribution of Charity Donors with Interactive Leaflet Maps\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nfolium\n\n\nEmmanuel House\n\n\n\n\nI illustrate my method for investigating the geographic distribution of individual donors to Emmanuel House Support Centre in Nottingham by constructing interactive leaflet maps in folium with a synthetic dataset.\n\n\n\n\n\n\nMar 17, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nGeocoding Postcodes in Python: pgeocode v ONS\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nfolium\n\n\nEmmanuel House\n\n\n\n\nI explain a problem encountered in voluntary work undertaken for Emmanuel House Support Centre in Nottingham related to encoding postcodes as coordinates, and the solution found.\n\n\n\n\n\n\nMar 16, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nThe Boltzmann Equation - 2. Probabilistic Preliminaries\n\n\n\n\n\n\n\nMathematics\n\n\nProbability Theory\n\n\nBoltzmann Equation\n\n\n\n\nBefore we can discuss Information Theory and its consequences for the Boltzmann equation, we first need to make some definitions from probability theory.\n\n\n\n\n\n\nFeb 13, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nThe Boltzmann Equation - 1. Introduction\n\n\n\n\n\n\n\nMathematics\n\n\nPDEs\n\n\nBoltzmann Equation\n\n\n\n\nI introduce the rigorous theory of the Boltzmann Transport Equation, following my undergraduate research project at Warwick Mathematics Institue: ‘The interplay between Information theory and the long-time behaviour of a dilute gas’.\n\n\n\n\n\n\nJan 29, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nSentiment Analysis with NLTK and Hugging Face Transformers\n\n\n\n\n\n\n\nPython\n\n\nML\n\n\nNLP\n\n\nNLTK\n\n\nTransformers\n\n\n\n\nSentiment analysis is performed on a dataset of Amazon reviews using NLTK’s VADER and a RoBERTa-base model from Hugging Face.\n\n\n\n\n\n\nJan 23, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nLogistic Regression with Gradient Descent and L2-Regularization\n\n\n\n\n\n\n\nPython\n\n\nML\n\n\n\n\nLogistic regression is implemented in NumPy and interpreted as a perceptron with sigmoid activation. The resulting model is used to detect cats in an image classification problem. Overfitting to the training data is counteracted by including a regularization term in the cost function. The regularization parameter is tuned to improve accuracy on the validation data.\n\n\n\n\n\n\nJan 16, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nForecasting Energy Consumption with XGBoost\n\n\n\n\n\n\n\nPython\n\n\nML\n\n\nPandas\n\n\nSeaborn\n\n\nXGBoost\n\n\n\n\nInformed by YouTube videos of Rob Mulla we use XGBoost to forecast energy consumption in the eastern US.\n\n\n\n\n\n\nDec 22, 2023\n\n\nDaniel J Smith\n\n\n\n\n\n\n  \n\n\n\n\nThe Spaceship Titanic with LightGBM\n\n\n\n\n\n\n\nPython\n\n\nML\n\n\nPandas\n\n\nSeaborn\n\n\nLightGBM\n\n\n\n\nA LightGBM classifier is trained with hyperparameters tuned using a random search to achieve &gt;80% classification accuracy on the Spaceship Titanic dataset.\n\n\n\n\n\n\nNov 23, 2023\n\n\nDaniel J Smith\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bte2/index.html",
    "href": "posts/bte2/index.html",
    "title": "The Boltzmann Equation - 2. Probabilistic Preliminaries",
    "section": "",
    "text": "Throughout we fix a probability space \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) and consider \\(\\mathbb{R}\\) equipped with Lebesgue measure \\(\\lambda\\) on the Borel \\(\\sigma\\)-algebra \\(\\mathcal{B}(\\mathbb{R}).\\) The reader is assumed to have taken a first course on measure theory.\nNote that a probability space \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) is a measure space with unit total mass\n\\[\\mathbb{P}(\\Omega)=1.\\]\n\n\n\nA random variable \\(X\\) is a measurable function \\(X:\\Omega\\longrightarrow\\mathbb{R}\\).\nThe cumulative distribution function (CDF) \\(F_X\\) of a random variable \\(X\\) is defined by \\(F_X(x) = \\mathbb{P}(X\\leq x).\\)\nA stochastic process is an indexed family of random variables \\({\\{X_t\\}_{t\\in T}}\\), where the indexing set \\(T\\) is not necessarily countable, and the index \\(t\\) is often interpreted as time.\n\n\n\n\n\\(X\\) is said to be a continuous random variable if its law \\(\\,\\mathbb{P}_X = \\mathbb{P}\\circ X^{-1}\\) is absolutely continuous with respect to the Lebesgue measure \\(\\lambda\\) as a measure on \\(\\mathbb{R}\\). That is, if\n\\[\\forall \\, N \\in \\mathcal{B}(\\mathbb{R}):\\] \\[\\lambda(N) = 0 \\Rightarrow \\mathbb{P}(X \\in N) = 0.\\]\nBy the Radon-Nikodym theorem, a random variable \\(X\\) is continuous if (and only if) there exists a measurable function \\(f_X : \\mathbb{R} \\longrightarrow [0,\\infty)\\) such that for all \\(\\, B \\in \\mathcal{B}(\\mathbb{R})\\),\n\\[\\mathbb{P}(X\\in B) = \\int_B f_X\\,\\text{d}\\lambda.\\]\nThe function \\(f\\) is called the probability density function (PDF) of \\(X\\) and is unique up to equality almost everywhere.\n\nUnless otherwise stated we now assume that a random variable \\(X\\) is continuous and has density \\(f\\).\n\n\n\nA random vector \\(\\mathbf{X}\\) is an n-tuple of random variables\n\\[\\mathbf{X}=(X_1,\\ldots,X_n) : \\Omega \\longrightarrow \\mathbb{R}^n.\\]\nThe joint CDF \\(F_{X,Y}\\) of a pair of random variables \\(X,\\,Y\\) is defined as\n\\[F_{X,Y}(x,y) = \\mathbb{P}(X\\leq x, Y\\leq y).\\]\nFrom the joint CDF \\(F_{X,Y}\\) we can recover the marginal CDFs \\(F_X\\), \\(F_Y\\) by sending the other variable to infinity:\n\\[\\begin{aligned}\nF_X(x) &= \\lim_{y\\to\\infty}F_{X,Y}(x,y),\\\\\n\\\\\nF_Y(y) &= \\lim_{x\\to\\infty}F_{X,Y}(x,y).\n\\end{aligned}\\]\nX and Y are jointly continuous if their joint law is absolutely continuous with respect to the two-dimensional Lebesgue measure \\(\\lambda_2\\) on \\((\\mathbb{R}^2,\\mathcal{B}(\\mathbb{R}^2)).\\) By the Radon-Nikodym theorem, X and Y are jointly continuous if (and only if) there exists a measurable function \\(f_{X,Y} : \\mathbb{R}^2 \\longrightarrow [0,\\infty)\\) such that for all \\(\\, B \\in \\mathcal{B}(\\mathbb{R}^2)\\),\n\\[\\mathbb{P}((X,Y)\\in B) = \\int_B f_{X,Y}\\,\\text{d}\\lambda_2,\\]\nwhere \\(f_{X,Y}\\) is called the joint PDF of X and Y. The marginal PDFs \\(f_X\\), \\(f_Y\\) can be obtained from \\(f_{X,Y}\\) by integrating out the other variable:\n\\[\\begin{aligned}\nf_X(x) &= \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)\\,\\text{d} y\\\\\n\\\\\nf_Y(y) &= \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)\\,\\text{d} x.\n\\end{aligned}\\]\nIn particular, jointly continuous random variables are automatically marginally continuous, although the converse is not true in general.\nAnalagously, \\(X_1,\\dots,X_n\\) are jointly continuous if their joint law is absolutely continuous with respect to n-dimensional Lebesgue measure \\(\\lambda_n\\), or equivalently if there exists a joint PDF for the random vector \\((X_1,\\dots,X_n)\\).\n\n\n\n\n\n\n\nThe expectation \\(\\mathbb{E}[X]\\) of a random variable \\(X\\) is simply its Lebesgue integral with respect to \\(\\mathbb{P}\\)\n\\[\\begin{aligned}\n    \\mathbb{E}[X] &= \\int_\\Omega X \\, \\text{d}\\mathbb{P}\\\\\n    &= \\int_\\mathbb{R} xf(x) \\,\\text{d} x.\n\\end{aligned}\\]\nThe \\(k^{th}\\) moment of \\(f\\) around the non-random value \\(c\\in\\mathbb{R}\\) is\n\\[\\mathbb{E}\\left[(X-c)^k\\right] = \\int_{-\\infty}^{\\infty} (x-c)^kf(x)\\,\\text{d} x.\\]\nThe \\(k^{th}\\) raw moment is the \\(k^{th}\\) moment around the origin, \\(c=0\\), \\[\\mathbb{E}[X^k] = \\int_{-\\infty}^{\\infty} x^kf(x)\\,\\text{d} x.\\]\nFor example, the \\(1^{st}\\) raw moment is the distribution’s mean, \\(\\mu = \\mathbb{E}[X]\\).\nThe \\(k^{th}\\) central moment is the \\(k^{th}\\) moment around the mean, \\(c = \\mu = \\mathbb{E}[X]\\),\n\\[\\mathbb{E}[(X-\\mu)^k] = \\int_{-\\infty}^{\\infty} (x-\\mu)^kf(x)\\,\\text{d} x.\\]\nFor example, the \\(2^{nd}\\) central moment is the distribution’s variance, \\(\\sigma^2 = \\mathbb{E}\\left[(X-\\mu)^2\\right]\\).\nBy abuse of language, it is common to refer to integrals of the form \\(\\int r(x)f(x)\\,\\text{d} x\\) as ‘moments’ even when the function \\(r\\) is not a polynomial in \\(x\\).\n\n\n\n\nGiven continuous random variables \\(X\\) and \\(Y\\) with \\(f_Y(y)&gt;0\\) we define:\n\nThe conditional CDF \\(F_{X|Y}\\) of \\(X\\) given Y by\n\\[F_{X|Y}(x|y) = \\frac{\\int_{-\\infty}^{x}f_{X,Y}(u,y)\\,\\text{d} u}{f_Y(y)}.\\]\nThe conditional PDF \\(f_{X|Y}\\) of \\(X\\) given \\(Y\\) by\n\\[f_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}.\\]\nThe conditional probability of the event \\(\\{ X\\in A\\}\\), \\(A\\in\\mathcal{B}(\\mathbb{R)}\\), given that \\(Y = y\\) by\n\\[\\mathbb{P}(X\\in A|Y = y) = \\int_A f_{X|Y}(x|y)\\,\\text{d} x.\\]\n\n\n\n\n\nRandom variables \\(X\\) and \\(Y\\) are said to be independent if their joint CDF \\(F_{X,Y}\\) factorises into the tensor product of the marginals:\n\\[F_{X,Y}(x,y) = F_X(x)F_Y(y).\\]\nEquivalently, if X and Y are jointly continuous then we say they are independent if their joint PDF \\(f_{X,Y}\\) factorises into the tensor product of the marginals:\n\\[f_{X,Y}(x,y) = f_X(x)f_Y(y)\\quad\\text{a.e.}\\]\nWhere for \\(f,g:E\\longrightarrow \\mathbb{R}\\) the tensor product \\(f\\otimes g : E^2\\longrightarrow \\mathbb{R}\\) is defined by \\(f\\otimes g(x,y) = f(x)g(y).\\)\n\n\n\n\nThis is not the most compact and elegant way of defining independence of random variables through the independence of the \\(\\sigma\\)-algebras they generate. However this definition is equivalent, more intuitive and will suffice for our purposes."
  },
  {
    "objectID": "posts/bte2/index.html#random-variables-and-random-vectors",
    "href": "posts/bte2/index.html#random-variables-and-random-vectors",
    "title": "The Boltzmann Equation - 2. Probabilistic Preliminaries",
    "section": "",
    "text": "Throughout we fix a probability space \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) and consider \\(\\mathbb{R}\\) equipped with Lebesgue measure \\(\\lambda\\) on the Borel \\(\\sigma\\)-algebra \\(\\mathcal{B}(\\mathbb{R}).\\) The reader is assumed to have taken a first course on measure theory.\nNote that a probability space \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) is a measure space with unit total mass\n\\[\\mathbb{P}(\\Omega)=1.\\]\n\n\n\nA random variable \\(X\\) is a measurable function \\(X:\\Omega\\longrightarrow\\mathbb{R}\\).\nThe cumulative distribution function (CDF) \\(F_X\\) of a random variable \\(X\\) is defined by \\(F_X(x) = \\mathbb{P}(X\\leq x).\\)\nA stochastic process is an indexed family of random variables \\({\\{X_t\\}_{t\\in T}}\\), where the indexing set \\(T\\) is not necessarily countable, and the index \\(t\\) is often interpreted as time.\n\n\n\n\n\\(X\\) is said to be a continuous random variable if its law \\(\\,\\mathbb{P}_X = \\mathbb{P}\\circ X^{-1}\\) is absolutely continuous with respect to the Lebesgue measure \\(\\lambda\\) as a measure on \\(\\mathbb{R}\\). That is, if\n\\[\\forall \\, N \\in \\mathcal{B}(\\mathbb{R}):\\] \\[\\lambda(N) = 0 \\Rightarrow \\mathbb{P}(X \\in N) = 0.\\]\nBy the Radon-Nikodym theorem, a random variable \\(X\\) is continuous if (and only if) there exists a measurable function \\(f_X : \\mathbb{R} \\longrightarrow [0,\\infty)\\) such that for all \\(\\, B \\in \\mathcal{B}(\\mathbb{R})\\),\n\\[\\mathbb{P}(X\\in B) = \\int_B f_X\\,\\text{d}\\lambda.\\]\nThe function \\(f\\) is called the probability density function (PDF) of \\(X\\) and is unique up to equality almost everywhere.\n\nUnless otherwise stated we now assume that a random variable \\(X\\) is continuous and has density \\(f\\).\n\n\n\nA random vector \\(\\mathbf{X}\\) is an n-tuple of random variables\n\\[\\mathbf{X}=(X_1,\\ldots,X_n) : \\Omega \\longrightarrow \\mathbb{R}^n.\\]\nThe joint CDF \\(F_{X,Y}\\) of a pair of random variables \\(X,\\,Y\\) is defined as\n\\[F_{X,Y}(x,y) = \\mathbb{P}(X\\leq x, Y\\leq y).\\]\nFrom the joint CDF \\(F_{X,Y}\\) we can recover the marginal CDFs \\(F_X\\), \\(F_Y\\) by sending the other variable to infinity:\n\\[\\begin{aligned}\nF_X(x) &= \\lim_{y\\to\\infty}F_{X,Y}(x,y),\\\\\n\\\\\nF_Y(y) &= \\lim_{x\\to\\infty}F_{X,Y}(x,y).\n\\end{aligned}\\]\nX and Y are jointly continuous if their joint law is absolutely continuous with respect to the two-dimensional Lebesgue measure \\(\\lambda_2\\) on \\((\\mathbb{R}^2,\\mathcal{B}(\\mathbb{R}^2)).\\) By the Radon-Nikodym theorem, X and Y are jointly continuous if (and only if) there exists a measurable function \\(f_{X,Y} : \\mathbb{R}^2 \\longrightarrow [0,\\infty)\\) such that for all \\(\\, B \\in \\mathcal{B}(\\mathbb{R}^2)\\),\n\\[\\mathbb{P}((X,Y)\\in B) = \\int_B f_{X,Y}\\,\\text{d}\\lambda_2,\\]\nwhere \\(f_{X,Y}\\) is called the joint PDF of X and Y. The marginal PDFs \\(f_X\\), \\(f_Y\\) can be obtained from \\(f_{X,Y}\\) by integrating out the other variable:\n\\[\\begin{aligned}\nf_X(x) &= \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)\\,\\text{d} y\\\\\n\\\\\nf_Y(y) &= \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)\\,\\text{d} x.\n\\end{aligned}\\]\nIn particular, jointly continuous random variables are automatically marginally continuous, although the converse is not true in general.\nAnalagously, \\(X_1,\\dots,X_n\\) are jointly continuous if their joint law is absolutely continuous with respect to n-dimensional Lebesgue measure \\(\\lambda_n\\), or equivalently if there exists a joint PDF for the random vector \\((X_1,\\dots,X_n)\\)."
  },
  {
    "objectID": "posts/bte2/index.html#expectation-moments-and-independence",
    "href": "posts/bte2/index.html#expectation-moments-and-independence",
    "title": "The Boltzmann Equation - 2. Probabilistic Preliminaries",
    "section": "",
    "text": "The expectation \\(\\mathbb{E}[X]\\) of a random variable \\(X\\) is simply its Lebesgue integral with respect to \\(\\mathbb{P}\\)\n\\[\\begin{aligned}\n    \\mathbb{E}[X] &= \\int_\\Omega X \\, \\text{d}\\mathbb{P}\\\\\n    &= \\int_\\mathbb{R} xf(x) \\,\\text{d} x.\n\\end{aligned}\\]\nThe \\(k^{th}\\) moment of \\(f\\) around the non-random value \\(c\\in\\mathbb{R}\\) is\n\\[\\mathbb{E}\\left[(X-c)^k\\right] = \\int_{-\\infty}^{\\infty} (x-c)^kf(x)\\,\\text{d} x.\\]\nThe \\(k^{th}\\) raw moment is the \\(k^{th}\\) moment around the origin, \\(c=0\\), \\[\\mathbb{E}[X^k] = \\int_{-\\infty}^{\\infty} x^kf(x)\\,\\text{d} x.\\]\nFor example, the \\(1^{st}\\) raw moment is the distribution’s mean, \\(\\mu = \\mathbb{E}[X]\\).\nThe \\(k^{th}\\) central moment is the \\(k^{th}\\) moment around the mean, \\(c = \\mu = \\mathbb{E}[X]\\),\n\\[\\mathbb{E}[(X-\\mu)^k] = \\int_{-\\infty}^{\\infty} (x-\\mu)^kf(x)\\,\\text{d} x.\\]\nFor example, the \\(2^{nd}\\) central moment is the distribution’s variance, \\(\\sigma^2 = \\mathbb{E}\\left[(X-\\mu)^2\\right]\\).\nBy abuse of language, it is common to refer to integrals of the form \\(\\int r(x)f(x)\\,\\text{d} x\\) as ‘moments’ even when the function \\(r\\) is not a polynomial in \\(x\\).\n\n\n\n\nGiven continuous random variables \\(X\\) and \\(Y\\) with \\(f_Y(y)&gt;0\\) we define:\n\nThe conditional CDF \\(F_{X|Y}\\) of \\(X\\) given Y by\n\\[F_{X|Y}(x|y) = \\frac{\\int_{-\\infty}^{x}f_{X,Y}(u,y)\\,\\text{d} u}{f_Y(y)}.\\]\nThe conditional PDF \\(f_{X|Y}\\) of \\(X\\) given \\(Y\\) by\n\\[f_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}.\\]\nThe conditional probability of the event \\(\\{ X\\in A\\}\\), \\(A\\in\\mathcal{B}(\\mathbb{R)}\\), given that \\(Y = y\\) by\n\\[\\mathbb{P}(X\\in A|Y = y) = \\int_A f_{X|Y}(x|y)\\,\\text{d} x.\\]\n\n\n\n\n\nRandom variables \\(X\\) and \\(Y\\) are said to be independent if their joint CDF \\(F_{X,Y}\\) factorises into the tensor product of the marginals:\n\\[F_{X,Y}(x,y) = F_X(x)F_Y(y).\\]\nEquivalently, if X and Y are jointly continuous then we say they are independent if their joint PDF \\(f_{X,Y}\\) factorises into the tensor product of the marginals:\n\\[f_{X,Y}(x,y) = f_X(x)f_Y(y)\\quad\\text{a.e.}\\]\nWhere for \\(f,g:E\\longrightarrow \\mathbb{R}\\) the tensor product \\(f\\otimes g : E^2\\longrightarrow \\mathbb{R}\\) is defined by \\(f\\otimes g(x,y) = f(x)g(y).\\)\n\n\n\n\nThis is not the most compact and elegant way of defining independence of random variables through the independence of the \\(\\sigma\\)-algebras they generate. However this definition is equivalent, more intuitive and will suffice for our purposes."
  },
  {
    "objectID": "posts/bte4/index.html",
    "href": "posts/bte4/index.html",
    "title": "The Boltzmann Equation - 4. Maximum Entropy",
    "section": "",
    "text": "Consider the following problem as posed by Cover[1]:\n\nMaximise the differential entropy \\(h(f)\\) over all densities \\(f\\) satisfying\n\n\\(f(x)\\geq0\\) with equality outside of a given set \\(S\\). \n\\(\\int_Sf(x)\\,\\text{d}x = 1\\) \n\\(\\int_S f(x)r_i(x)\\,\\text{d}x = \\alpha_i\\)\nfor given functions \\(r_i\\), \\(\\,i = 1,\\dots,m.\\)\n\n\nThat is, we wish to maximise the entropy over all probability distributions supported on the set \\(S\\) satisfying the \\(m\\) given moment constraints \\(\\mathbb{E}[r_i(X)] = \\alpha_i, \\,\\,i = 1,\\dots,m.\\)\nIt is natural to conjecture that the solution to this optimization problem takes the form of an exponential function. Indeed, we prove this in the following theorem by using the Information Inequality:\n\n\n\nFor \\(x\\in S\\) define\n\\[f^*(x) = \\exp{\\left[\\lambda_0 + \\sum_{i=0}^m \\lambda_i r_i(x)\\right]}\\]\nwhere \\(\\lambda_0, \\lambda_1,\\dots,\\lambda_m\\) are chosen such that\n\\[\n\\begin{aligned}\n\\int_Sf^*&=1\\\\\n\\quad\\int_Sf^*r_i&=\\alpha_i.\n\\end{aligned}\n\\]\nThen \\(f^*\\) uniquely maximises the entropy \\(h(f)\\) over all densities \\(f\\) satisfying conditions 1, 2 and 3 as stated above.\n\nProof.\nLet \\(g\\) satisfy conditions 1, 2 and 3. Then:\n\\[\\begin{align*}\nh(g) &= -\\int g\\log g\\\\\n&= -\\int g\\log\\frac{g}{f^*}f^*\\\\\n&= -D(g\\,||\\,f^*) -\\int g\\log f^*\\\\\n&\\leq -\\int g\\log f^*\\\\\n&= -\\int g\\left(\\lambda_0 + \\sum_{i=1}^m\\lambda_ir_i\\right)\\\\\n&= -\\int f^*\\left(\\lambda_0 + \\sum_{i=1}^m\\lambda_ir_i\\right)\\\\\n&= -\\int f^* \\log f^*\\\\\n&= h(f^*)\n\\end{align*}\\]\nin which we have equality iff we have equality in the information equality. i.e. iff \\(g = f^*\\) a.e.\n◻\n\n\n\n\n\nThe maximum entropy can be infinite. For example, consider \\(S = \\mathbb{R}\\) with constraint \\(\\mathbb{E}[X] = \\mu\\) some fixed \\(\\mu\\in\\mathbb{R}.\\) Then Gaussian distributions \\(X \\sim\\mathcal{N}(\\mu,\\sigma^2)\\) satisfy the constraint for any variance \\(\\sigma^2&gt;0\\) . By Example 2.1.2 from a previous post we have \\[h(X) = \\frac{1}{2}\\log2\\pi e\\sigma^2 \\xrightarrow{\\: \\sigma^2 \\to \\infty \\: }\\infty.\\]\nIn words, we can construct probability densities on \\(\\mathbb{R}\\) with arbitrarily large differential entropy satisfying the first moment constraint of having mean \\(\\mu\\) by considering Gaussian distributions \\(\\mathcal{N}(\\mu,\\sigma^2)\\) with fixed mean \\(\\mu\\) and increasing variance \\(\\sigma^2\\).\n\n\n\n\nEven if the maximum entropy is finite it need not be attained. That is, the constants \\(\\lambda_i\\) introduced in Theorem 1.1 need not exist.\nFor example, consider probability densities \\(f\\) on \\(S = \\mathbb{R}\\) with moment constraints up to third order: \\[\\begin{aligned}\n\\int_{-\\infty}^\\infty f(x) \\,\\text{d} x &= 1,\\\\\n\\int_{-\\infty}^\\infty x^if(x)\\,\\text{d} x &= \\alpha_i,\\quad i=1,2,3.\n\\end{aligned}\\]\nThen by Theorem 1.1 the maximum entropy distribution (if it exists) looks like\n\\[f(x) = \\exp\\left[\\lambda_0 + \\lambda_1x + \\lambda_2x^2 + \\lambda_3x^3\\right].\\]\nHowever, \\(f\\in L^1(\\mathbb{R})\\) only if \\(\\lambda_3 = 0.\\) Then we have four equations in three unknowns and thus it is in general not possible to determine the \\(\\lambda_i\\).\nThe failure of our technique in this case is simply explained, although \\(\\sup h(f) &lt;\\infty\\) there is not a probability density \\(f\\) satisfying our constraints that achieves this supremum.\n\n\n\n\nThe Maxwell-Boltzmann distribution \\(M^f\\) associated to a particle distribution function \\(f\\) is\n\\[\\begin{aligned}\n    M^f(v) = \\frac{\\rho}{(2\\pi T)^{d/2}}\\,\\mathrm{exp}\\left[-\\frac{1}{2T}|v-u|^2\\right],\n\\end{aligned}\\]\nwhere\n\\[\\begin{aligned}\n\\begin{split}\n    \\rho &= \\int_{\\mathbb{R}^d}f\\,\\text{d}v,\\\\\n    u &= \\frac{1}{\\rho}\\int_{\\mathbb{R}^d}vf\\,\\text{d}v,\\\\\n    T &= \\frac{1}{\\rho d}\\int_{\\mathbb{R}^d}|v-u|^2f\\,\\text{d}v.\n\\end{split}\n\\end{aligned}\\]\nThis is of the form of the maximum entropy distribution \\(f^*\\) given in Theorem 1.1 with respect to the moment contraints equivalent to fixed total energy and fixed particle number.\nBoltzmann’s \\(H\\)-theorem is an analytical assertion of the fact that the entoropy \\(h(f)\\) of such a distribution \\(f\\) is a quantity increasing with time.\nThus Theorem 1.1 combined with the \\(H\\)-theorem provide a rigorous mathematical underpinning for our physical intuition that the Maxwell-Boltzmann distribution \\(M^f\\) should be the candidate limit of a particle distribution function \\(f\\) as \\(t\\rightarrow\\infty\\), since this intuition aligns with the second law of thermodynamic’s assertion that the entropy of an isolated system will increase until it reaches a maximum at equilibrium."
  },
  {
    "objectID": "posts/bte4/index.html#theorem-1.1",
    "href": "posts/bte4/index.html#theorem-1.1",
    "title": "The Boltzmann Equation - 4. Maximum Entropy",
    "section": "",
    "text": "For \\(x\\in S\\) define\n\\[f^*(x) = \\exp{\\left[\\lambda_0 + \\sum_{i=0}^m \\lambda_i r_i(x)\\right]}\\]\nwhere \\(\\lambda_0, \\lambda_1,\\dots,\\lambda_m\\) are chosen such that\n\\[\n\\begin{aligned}\n\\int_Sf^*&=1\\\\\n\\quad\\int_Sf^*r_i&=\\alpha_i.\n\\end{aligned}\n\\]\nThen \\(f^*\\) uniquely maximises the entropy \\(h(f)\\) over all densities \\(f\\) satisfying conditions 1, 2 and 3 as stated above.\n\nProof.\nLet \\(g\\) satisfy conditions 1, 2 and 3. Then:\n\\[\\begin{align*}\nh(g) &= -\\int g\\log g\\\\\n&= -\\int g\\log\\frac{g}{f^*}f^*\\\\\n&= -D(g\\,||\\,f^*) -\\int g\\log f^*\\\\\n&\\leq -\\int g\\log f^*\\\\\n&= -\\int g\\left(\\lambda_0 + \\sum_{i=1}^m\\lambda_ir_i\\right)\\\\\n&= -\\int f^*\\left(\\lambda_0 + \\sum_{i=1}^m\\lambda_ir_i\\right)\\\\\n&= -\\int f^* \\log f^*\\\\\n&= h(f^*)\n\\end{align*}\\]\nin which we have equality iff we have equality in the information equality. i.e. iff \\(g = f^*\\) a.e.\n◻"
  },
  {
    "objectID": "posts/bte4/index.html#remark-1.2",
    "href": "posts/bte4/index.html#remark-1.2",
    "title": "The Boltzmann Equation - 4. Maximum Entropy",
    "section": "",
    "text": "The maximum entropy can be infinite. For example, consider \\(S = \\mathbb{R}\\) with constraint \\(\\mathbb{E}[X] = \\mu\\) some fixed \\(\\mu\\in\\mathbb{R}.\\) Then Gaussian distributions \\(X \\sim\\mathcal{N}(\\mu,\\sigma^2)\\) satisfy the constraint for any variance \\(\\sigma^2&gt;0\\) . By Example 2.1.2 from a previous post we have \\[h(X) = \\frac{1}{2}\\log2\\pi e\\sigma^2 \\xrightarrow{\\: \\sigma^2 \\to \\infty \\: }\\infty.\\]\nIn words, we can construct probability densities on \\(\\mathbb{R}\\) with arbitrarily large differential entropy satisfying the first moment constraint of having mean \\(\\mu\\) by considering Gaussian distributions \\(\\mathcal{N}(\\mu,\\sigma^2)\\) with fixed mean \\(\\mu\\) and increasing variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "posts/bte4/index.html#remark-1.3",
    "href": "posts/bte4/index.html#remark-1.3",
    "title": "The Boltzmann Equation - 4. Maximum Entropy",
    "section": "",
    "text": "Even if the maximum entropy is finite it need not be attained. That is, the constants \\(\\lambda_i\\) introduced in Theorem 1.1 need not exist.\nFor example, consider probability densities \\(f\\) on \\(S = \\mathbb{R}\\) with moment constraints up to third order: \\[\\begin{aligned}\n\\int_{-\\infty}^\\infty f(x) \\,\\text{d} x &= 1,\\\\\n\\int_{-\\infty}^\\infty x^if(x)\\,\\text{d} x &= \\alpha_i,\\quad i=1,2,3.\n\\end{aligned}\\]\nThen by Theorem 1.1 the maximum entropy distribution (if it exists) looks like\n\\[f(x) = \\exp\\left[\\lambda_0 + \\lambda_1x + \\lambda_2x^2 + \\lambda_3x^3\\right].\\]\nHowever, \\(f\\in L^1(\\mathbb{R})\\) only if \\(\\lambda_3 = 0.\\) Then we have four equations in three unknowns and thus it is in general not possible to determine the \\(\\lambda_i\\).\nThe failure of our technique in this case is simply explained, although \\(\\sup h(f) &lt;\\infty\\) there is not a probability density \\(f\\) satisfying our constraints that achieves this supremum."
  },
  {
    "objectID": "posts/bte4/index.html#remark-1.4",
    "href": "posts/bte4/index.html#remark-1.4",
    "title": "The Boltzmann Equation - 4. Maximum Entropy",
    "section": "",
    "text": "The Maxwell-Boltzmann distribution \\(M^f\\) associated to a particle distribution function \\(f\\) is\n\\[\\begin{aligned}\n    M^f(v) = \\frac{\\rho}{(2\\pi T)^{d/2}}\\,\\mathrm{exp}\\left[-\\frac{1}{2T}|v-u|^2\\right],\n\\end{aligned}\\]\nwhere\n\\[\\begin{aligned}\n\\begin{split}\n    \\rho &= \\int_{\\mathbb{R}^d}f\\,\\text{d}v,\\\\\n    u &= \\frac{1}{\\rho}\\int_{\\mathbb{R}^d}vf\\,\\text{d}v,\\\\\n    T &= \\frac{1}{\\rho d}\\int_{\\mathbb{R}^d}|v-u|^2f\\,\\text{d}v.\n\\end{split}\n\\end{aligned}\\]\nThis is of the form of the maximum entropy distribution \\(f^*\\) given in Theorem 1.1 with respect to the moment contraints equivalent to fixed total energy and fixed particle number.\nBoltzmann’s \\(H\\)-theorem is an analytical assertion of the fact that the entoropy \\(h(f)\\) of such a distribution \\(f\\) is a quantity increasing with time.\nThus Theorem 1.1 combined with the \\(H\\)-theorem provide a rigorous mathematical underpinning for our physical intuition that the Maxwell-Boltzmann distribution \\(M^f\\) should be the candidate limit of a particle distribution function \\(f\\) as \\(t\\rightarrow\\infty\\), since this intuition aligns with the second law of thermodynamic’s assertion that the entropy of an isolated system will increase until it reaches a maximum at equilibrium."
  },
  {
    "objectID": "posts/eh2/index.html",
    "href": "posts/eh2/index.html",
    "title": "Visualising the Geographic Distribution of Charity Donors with Interactive Leaflet Maps",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport folium\nfrom branca.colormap import LinearColormap\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg"
  },
  {
    "objectID": "posts/eh2/index.html#first-map",
    "href": "posts/eh2/index.html#first-map",
    "title": "Visualising the Geographic Distribution of Charity Donors with Interactive Leaflet Maps",
    "section": "First Map",
    "text": "First Map\nProducing a basic folium map with no extra features:\n\nm = folium.Map(location=[52.9548, -1.1581], zoom_start=12)\n\nfor index, row in df.iterrows():  \n    \n    folium.CircleMarker(\n        location=[row['Latitude'], row['Longitude']],\n        radius=5,\n        color='blue', \n        fill=True,\n        fill_color='blue',\n        fill_opacity=0.7,\n    ).add_to(m)\n    \nm.save('FakeDonorMap1.html')\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/eh2/index.html#adding-colour-labelling",
    "href": "posts/eh2/index.html#adding-colour-labelling",
    "title": "Visualising the Geographic Distribution of Charity Donors with Interactive Leaflet Maps",
    "section": "Adding Colour Labelling",
    "text": "Adding Colour Labelling\nWe can colour each point by the value of TotalDonated using LinearColormap from branca.colormap:\n\nm = folium.Map(location=[52.9548, -1.1581], zoom_start=12)\n\ncolors = ['green', 'yellow', 'orange', 'red', 'purple']\nlinear_colormap = LinearColormap(colors=colors,\n                                 index=[0, 100, 250, 500, 1000],\n                                 vmin=df['TotalDonated'].min(),\n                                 vmax=df['TotalDonated'].quantile(0.99025))\n\nfor index, row in df.iterrows():    \n    \n    total_don = row['TotalDonated']\n    color = linear_colormap(total_don)\n    \n    folium.CircleMarker(\n        location=[row['Latitude'], row['Longitude']],\n        radius=5,\n        color=color, \n        fill=True,\n        fill_color=color,\n        fill_opacity=1,\n    ).add_to(m)\n\nlinear_colormap.add_to(m)\n\nm.save('FakeDonorMap2.html')\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/eh2/index.html#adding-pop-ups",
    "href": "posts/eh2/index.html#adding-pop-ups",
    "title": "Visualising the Geographic Distribution of Charity Donors with Interactive Leaflet Maps",
    "section": "Adding Pop-Ups",
    "text": "Adding Pop-Ups\nNext, I added a pop-up to each data point showing the user the data of that donor by adding an argument popup to folium.CircleMarker:\n\nm = folium.Map(location=[52.9548, -1.1581], zoom_start=12)\n\ncolors = ['green', 'yellow', 'orange', 'red', 'purple']\nlinear_colormap = LinearColormap(colors=colors,\n                                 index=[0, 100, 250, 500, 1000],\n                                 vmin=df['TotalDonated'].min(),\n                                 vmax=df['TotalDonated'].quantile(0.99025))\n\nfor index, row in df.iterrows():    \n    num_don = row['NumberDonations']\n    total_don = row['TotalDonated']\n    news = bool(row['Newsletter'])\n    avg_don = row['AverageDonated']\n    \n    popup_text = f'&lt;div style=\"width: 175px;\"&gt;\\\n                  Total Donated: £{total_don:.2f}&lt;br&gt;\\\n                  Number of Donations: {num_don}&lt;br&gt;\\\n                  Average Donation: £{avg_don:.2f}&lt;br&gt;\\\n                  Subscribed to Newsletter: {news}\\\n                  &lt;/div&gt;'\n    \n    color = linear_colormap(total_don)\n    \n    folium.CircleMarker(\n        location=[row['Latitude'], row['Longitude']],\n        radius=5,\n        color=color, \n        fill=True,\n        fill_color=color,\n        fill_opacity=1,\n        popup=popup_text\n    ).add_to(m)\n\nlinear_colormap.add_to(m)\n\nm.save('FakeDonorMap_noLayerControl.html')\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nShowing a screenshot taken with ShareX using Matplotlib:\n\nplt.figure(figsize=(20,10))\nimg = mpimg.imread('FakeDonorMap_noLayerControl.jpg')\nimgplot = plt.imshow(img)\nplt.show()"
  },
  {
    "objectID": "posts/eh2/index.html#adding-layer-control",
    "href": "posts/eh2/index.html#adding-layer-control",
    "title": "Visualising the Geographic Distribution of Charity Donors with Interactive Leaflet Maps",
    "section": "Adding Layer Control",
    "text": "Adding Layer Control\nFinally, I added layer control to the map using folium.map.FeatureGroup and folium.LayerControl.\nThis results in buttons being added to the right of the UI, under the colour bar, allowing the user to show/hide the markers for donors in specific ranges of donation totals.\nI used Microsoft Copilot to assist with this step.\n\nm = folium.Map(location=[52.9548, -1.1581], zoom_start=12)\n\ncolors = ['green', 'yellow', 'orange', 'red', 'purple']\nlinear_colormap = LinearColormap(colors=colors,\n                                 index=[0, 100, 250, 500, 1000],\n                                 vmin=df['TotalDonated'].min(),\n                                 vmax=df['TotalDonated'].quantile(0.99025))\n\n# Create FeatureGroups\nfgroups = [folium.map.FeatureGroup(name=f\"Total Donated:  £{lower}-£{upper}\") for lower, upper in zip([0, 100, 250, 500, 750, 1000], [100, 250, 500, 750, 1000, float('inf')])]\n\nfor index, row in df.iterrows():    \n    num_don = row['NumberDonations']\n    total_don = row['TotalDonated']\n    news = bool(row['Newsletter'])\n    avg_don = row['AverageDonated']\n    \n    popup_text = f'&lt;div style=\"width: 175px;\"&gt;\\\n                  Total Donated: £{total_don:.2f}&lt;br&gt;\\\n                  Number of Donations: {num_don}&lt;br&gt;\\\n                  Average Donation: £{avg_don:.2f}&lt;br&gt;\\\n                  Subscribed to Newsletter: {news}\\\n                  &lt;/div&gt;'\n    \n    color = linear_colormap(total_don)\n    \n    marker = folium.CircleMarker(\n        location=[row['Latitude'], row['Longitude']],\n        radius=5,\n        color=color, \n        fill=True,\n        fill_color=color,\n        fill_opacity=1,\n        popup=popup_text\n    )\n    \n    # Add the marker to the appropriate FeatureGroup\n    for fgroup, (lower, upper) in zip(fgroups, zip([0, 100, 250, 500, 750, 1000], [100, 250, 500, 750, 1000, float('inf')])):\n        if lower &lt;= total_don &lt; upper:\n            fgroup.add_child(marker)\n            break\n\n# Add the FeatureGroups to the map\nfor fgroup in fgroups:\n    m.add_child(fgroup)\n\nlinear_colormap.add_to(m)\nm.add_child(folium.LayerControl())\n\n\nm.save('FakeDonorMap_withLayerControl.html')\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nShowing a screenshot taken with ShareX using Matplotlib:\n\nplt.figure(figsize=(20,10))\nimg = mpimg.imread('FakeDonorMap_withLayerControl.jpg')\nimgplot = plt.imshow(img)\nplt.show()"
  },
  {
    "objectID": "posts/eh4/index.html",
    "href": "posts/eh4/index.html",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "",
    "text": "An artificial dataset of charity donors spanning 3 years is preprocessed for clustering. Preprocessing steps includes feature selection by correlation analysis and dimensionality reduction using PCA. A random search is used to tune the hyperparameters of KMeans, GMM and BIRCH models for k=4 and k=5 clusters. The models are combined into an ensemble with hard voting to cluster donors into groups with similar donation patterns. The results of each clustering experiment are geographically presented through interactive maps and judged through visual inspection and comparing silhouette scores.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nimport folium\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans, Birch\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score, make_scorer\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom collections import Counter\npd.set_option('display.max_columns', None)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndef drop_column(df, col_name):\n    if col_name in df.columns:\n        df = df.drop(columns=[col_name])\n    return df\ndf = pd.read_csv('../eh3/data.csv')\ndf['Transactions_DateOfFirstGift'] = pd.to_datetime(df['Transactions_DateOfFirstGift'])\ndf['Transactions_DateOfLastGift'] = pd.to_datetime(df['Transactions_DateOfLastGift'])"
  },
  {
    "objectID": "posts/eh4/index.html#features",
    "href": "posts/eh4/index.html#features",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "FEATURES",
    "text": "FEATURES\n\nunscaled_features = ['Transactions_LifetimeGiftsAmount',\n                    'Transactions_LifetimeGiftsNumber',\n                    'Transactions_AverageGiftAmount',\n\n                    'Transactions_Months1To12GiftsAmount',\n                    'Transactions_Months1To12GiftsNumber',\n                    'Transactions_Months13To24GiftsAmount',\n                    'Transactions_Months13To24GiftsNumber',\n                    'Transactions_Months25To36GiftsAmount',\n                    'Transactions_Months25To36GiftsNumber',\n                    'Transactions_FirstGiftAmount',\n                    'Transactions_LastGiftAmount',\n                    'monthsSinceFirstDonation',\n                    'monthsSinceLastDonation',\n                    'activeMonths',\n                    'Transactions_HighestGiftAmount',\n                    'Transactions_LowestGiftAmount',\n                   ]\n\n\nscaled_features = ['DonationFrequency',\n                   'DonationFrequencyActive',]\n\n\nbinary_features = []\n\n\nfeatures = unscaled_features + scaled_features + binary_features\n\n\nlen(features)\n\n18"
  },
  {
    "objectID": "posts/eh4/index.html#correlation-analysis",
    "href": "posts/eh4/index.html#correlation-analysis",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "CORRELATION ANALYSIS",
    "text": "CORRELATION ANALYSIS\n\nplt.figure(figsize=(12, 8))\n\n# Compute the correlation matrix\ncorr = df[features].corr(numeric_only=True)\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool), k=0)\n\n# Draw the heatmap with the mask\nsns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='magma', \n            cbar_kws={\"shrink\": .8}, linewidths=.5)\n\n# Rotate the y labels (ticks) to horizontal\nplt.yticks(rotation=0)\nplt.show()\n\n\n\n\n\nprint('15 most highly correlated pairs of features (in absolute value):\\n')\nprint(corr.abs().where(~mask).stack().sort_values(ascending=False).head(15))\n\n15 most highly correlated pairs of features (in absolute value):\n\nTransactions_Months13To24GiftsNumber  Transactions_LifetimeGiftsNumber       0.949797\nTransactions_LastGiftAmount           Transactions_AverageGiftAmount         0.934479\nTransactions_LowestGiftAmount         Transactions_AverageGiftAmount         0.928709\n                                      Transactions_LastGiftAmount            0.915917\nTransactions_FirstGiftAmount          Transactions_AverageGiftAmount         0.905685\nTransactions_Months1To12GiftsNumber   Transactions_LifetimeGiftsNumber       0.899897\nTransactions_Months13To24GiftsAmount  Transactions_LifetimeGiftsAmount       0.890478\nTransactions_Months1To12GiftsAmount   Transactions_LifetimeGiftsAmount       0.868661\nTransactions_LowestGiftAmount         Transactions_FirstGiftAmount           0.855994\nDonationFrequency                     Transactions_Months1To12GiftsNumber    0.834842\nTransactions_Months13To24GiftsNumber  Transactions_Months1To12GiftsNumber    0.823872\nTransactions_HighestGiftAmount        Transactions_AverageGiftAmount         0.816823\nTransactions_Months25To36GiftsNumber  Transactions_LifetimeGiftsNumber       0.811658\nTransactions_LastGiftAmount           Transactions_FirstGiftAmount           0.787498\nTransactions_HighestGiftAmount        Transactions_FirstGiftAmount           0.769152\ndtype: float64"
  },
  {
    "objectID": "posts/eh4/index.html#new-features-after-removal",
    "href": "posts/eh4/index.html#new-features-after-removal",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "NEW FEATURES AFTER REMOVAL",
    "text": "NEW FEATURES AFTER REMOVAL\n\nunscaled_features = ['Transactions_LifetimeGiftsAmount',\n                    'Transactions_LifetimeGiftsNumber',\n                    'Transactions_AverageGiftAmount',\n\n\n\n\n                    'Transactions_Months25To36GiftsAmount',\n                     \n                    'Transactions_FirstGiftAmount',\n                    'monthsSinceFirstDonation',\n                    'monthsSinceLastDonation',\n                    'activeMonths',\n\n                   ]\n\n\nscaled_features = ['DonationFrequency',\n                   'DonationFrequencyActive',]\n\n\nbinary_features = []\n\n\nfeatures = unscaled_features + scaled_features + binary_features\n\n\nlen(features)\n\n10"
  },
  {
    "objectID": "posts/eh4/index.html#transforming-scaling-pca-outliers",
    "href": "posts/eh4/index.html#transforming-scaling-pca-outliers",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "TRANSFORMING, SCALING, PCA, OUTLIERS",
    "text": "TRANSFORMING, SCALING, PCA, OUTLIERS\n\nLog transforming £ features\n\ndf_log = df[features].copy()\n\n# Logarithmically scaling down features representing donation totals\n# np.log1p is the function x |--&gt; ln(1+x)\n\ndf_log['Transactions_Months1To12GiftsAmount'] = np.log1p(df['Transactions_Months1To12GiftsAmount'])\ndf_log['Transactions_Months13To24GiftsAmount'] = np.log1p(df['Transactions_Months13To24GiftsAmount'])\ndf_log['Transactions_Months25To36GiftsAmount'] = np.log1p(df['Transactions_Months25To36GiftsAmount'])\n\ndf_log['Transactions_FirstGiftAmount'] = np.log1p(df['Transactions_FirstGiftAmount'])\ndf_log['Transactions_LastGiftAmount'] = np.log1p(df['Transactions_LastGiftAmount'])\n\ndf_log['Transactions_HighestGiftAmount'] = np.log1p(df['Transactions_HighestGiftAmount'])\ndf_log['Transactions_LowestGiftAmount'] = np.log1p(df['Transactions_LowestGiftAmount'])\n\ndf_log['Transactions_LifetimeGiftsAmount'] = np.log1p(df['Transactions_LifetimeGiftsAmount'])\n\ndf_log['Transactions_AverageGiftAmount'] = np.log1p(df['Transactions_LifetimeGiftsAmount'])/df['Transactions_LifetimeGiftsNumber']\n\n\ndf_log = df_log[features]\n\n\ndf_log.sample(10)\n\n\n\n\n\n\n\n\nTransactions_LifetimeGiftsAmount\nTransactions_LifetimeGiftsNumber\nTransactions_AverageGiftAmount\nTransactions_Months25To36GiftsAmount\nTransactions_FirstGiftAmount\nmonthsSinceFirstDonation\nmonthsSinceLastDonation\nactiveMonths\nDonationFrequency\nDonationFrequencyActive\n\n\n\n\n391\n4.418841\n3\n1.472947\n3.419037\n3.447763\n27\n3\n25\n0.107143\n0.120000\n\n\n305\n5.931529\n3\n1.977176\n4.624581\n4.593401\n29\n17\n13\n0.100000\n0.230769\n\n\n485\n4.115780\n2\n2.057890\n0.000000\n3.444257\n15\n15\n1\n0.125000\n2.000000\n\n\n533\n4.565805\n4\n1.141451\n3.477541\n3.429137\n27\n4\n24\n0.142857\n0.166667\n\n\n467\n6.584087\n36\n0.182891\n5.380634\n3.043093\n36\n1\n36\n0.972973\n1.000000\n\n\n143\n4.389250\n2\n2.194625\n0.000000\n3.716738\n14\n14\n1\n0.133333\n2.000000\n\n\n147\n3.741946\n2\n1.870973\n0.000000\n3.031099\n6\n6\n1\n0.285714\n2.000000\n\n\n667\n4.145038\n5\n0.829008\n3.088311\n3.088311\n27\n3\n25\n0.178571\n0.200000\n\n\n623\n5.312023\n2\n2.656012\n4.632396\n4.663722\n27\n27\n1\n0.071429\n2.000000\n\n\n97\n5.546466\n10\n0.554647\n4.467172\n3.001714\n31\n1\n31\n0.312500\n0.322581\n\n\n\n\n\n\n\n\n\nStandardScaler\n\n# Scaling non-binary features\nscaler = StandardScaler()\ndf_scaled = pd.DataFrame(scaler.fit_transform(df[unscaled_features]), columns=unscaled_features)\ndf_concat = pd.concat([df_scaled, df[scaled_features], df[binary_features]], axis=1)\n\n\n\nPCA\n\n# Choosing n_components by producing an explained variance plot\npca = PCA()\ndf_pca = pca.fit_transform(df_concat)\n\n# Calculate the cumulative explained variance\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\n\n# Plot the elbow graph\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\nplt.title('Explained Variance by Components')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\n\n# Add gridlines\nplt.grid(True)\n\n# Ensure non-fractional x labels\nax = plt.gca()\nax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n\nplt.show()\n\n\n\n\n\n# Apply PCA with chosen number of components\npca = PCA(n_components=7)\ndf_pca = pd.DataFrame(pca.fit_transform(df_concat))\n\n\n\nREMOVING OUTLIERS\nWe declare a donor to be an outlier if they have any of Transactions_LifetimeGiftsAmount, Transactions_AverageGiftAmount or Transactions_LifetimeGiftsNumber at more than 3 standard deviations above the mean.\nOutliers are removed before clustering to improve interpretability of results.\n\n# Define thresholds at which to cutoff outliers and remove such rows from the dataframe\namnt_thres = df['Transactions_LifetimeGiftsAmount'].mean() + 3*df['Transactions_LifetimeGiftsAmount'].std()\navg_thres = df['Transactions_AverageGiftAmount'].mean() + 3*df['Transactions_AverageGiftAmount'].std()\nnum_thres = df['Transactions_LifetimeGiftsNumber'].mean() + 3*df['Transactions_LifetimeGiftsNumber'].std()\nmask = ~((df['Transactions_LifetimeGiftsAmount'] &gt; amnt_thres) | (df['Transactions_AverageGiftAmount'] &gt; avg_thres) | (df['Transactions_LifetimeGiftsNumber'] &gt; num_thres))\ndf_pca_without_outliers = df_pca[mask]"
  },
  {
    "objectID": "posts/eh4/index.html#k-4",
    "href": "posts/eh4/index.html#k-4",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "k = 4",
    "text": "k = 4\n\nTUNING K MEANS\n\nk = 4\n\n# Define the parameter grid\nparam_grid = {\n    'init': ['k-means++', 'random'],\n    'n_init': list(range(10, 200)), \n    'algorithm': ['auto', 'lloyd', 'elkan'],\n    'max_iter': list(range(100, 500)),\n    'tol': [1e-4, 1e-3, 1e-2, 1e-1, 1],\n}\n\n\n# Create a KMeans instance with a fixed number of clusters\nkmeans = KMeans(n_clusters=k)\n\n# Create a silhouette scorer\nsilhouette_scorer = make_scorer(silhouette_score)\n\n# Create a RandomizedSearchCV instance\nrandom_search = RandomizedSearchCV(kmeans, \n                                   param_grid, \n                                   n_iter=50,\n                                   cv=3, \n                                   random_state=594,\n                                   scoring=silhouette_scorer,\n                                  )\n\n# Fit the RandomizedSearchCV instance \nrandom_search.fit(df_pca_without_outliers)\n\n# Get the best parameters\nbest_params_kmeans_4 = random_search.best_params_\n\n# Print the best parameters\nprint(f'Best params for KMeans with k={k}:\\n')\nprint(best_params_kmeans_4)\n\nBest params for KMeans with k=4:\n\n{'tol': 0.0001, 'n_init': 129, 'max_iter': 238, 'init': 'k-means++', 'algorithm': 'elkan'}\n\n\n\n\nTUNING GMM\n\nk = 4\n\n# Define the parameter grid for GMM\nparam_grid_gmm = {\n    'n_init': list(range(1, 21)), \n    'covariance_type': ['full', 'tied', 'diag', 'spherical'],\n    'tol': [1e-3, 1e-4, 1e-5, 1e-6],\n    'reg_covar': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],\n    'max_iter': list(range(100, 500)) \n}\n\n\n# Create a GMM instance\ngmm = GaussianMixture(n_components=k)\n\n# Create a silhouette scorer\nsilhouette_scorer = make_scorer(silhouette_score)\n\n# Create a RandomizedSearchCV instance for GMM\nrandom_search_gmm = RandomizedSearchCV(gmm, \n                                       param_grid_gmm, \n                                       n_iter=50, \n                                       cv=3, \n                                       random_state=594,\n                                       scoring=silhouette_scorer,\n                                      )\n\n# Fit the RandomizedSearchCV instance to your data\nrandom_search_gmm.fit(df_pca_without_outliers)\n\n# Get the best parameters for GMM\nbest_params_gmm_4 = random_search_gmm.best_params_\n\n# Print the best parameters for GMM\nprint(f'Best params for GMM with k={k}:\\n')\nprint(best_params_gmm_4)\n\nBest params for GMM with k=4:\n\n{'tol': 1e-06, 'reg_covar': 1, 'n_init': 20, 'max_iter': 376, 'covariance_type': 'diag'}\n\n\n\n\nTUNING BIRCH\n\nk = 4\n\n# Define the parameter grid for BIRCH\nparam_grid_birch = {\n    'threshold': [0.1, 0.3, 0.5, 0.7, 0.9],\n    'branching_factor': list(range(20, 100))\n}\n\n# Create a BIRCH instance\nbirch = Birch(n_clusters=k)\n\n# Create a silhouette scorer\nsilhouette_scorer = make_scorer(silhouette_score)\n\n# Create a RandomizedSearchCV instance for BIRCH\nrandom_search_birch = RandomizedSearchCV(birch, \n                                         param_grid_birch, \n                                         n_iter=50, \n                                         cv=3, \n                                         random_state=594,\n                                         scoring=silhouette_scorer,\n                                        )\n\n# Fit the RandomizedSearchCV instance to your data\nrandom_search_birch.fit(df_pca_without_outliers)\n\n# Get the best parameters for BIRCH\nbest_params_birch_4 = random_search_birch.best_params_\n\n# Print the best parameters for BIRCH\nprint(f'Best params for BIRCH with k={k}:\\n')\nprint(best_params_birch_4)\n\nBest params for BIRCH with k=4:\n\n{'threshold': 0.7, 'branching_factor': 51}"
  },
  {
    "objectID": "posts/eh4/index.html#k-5",
    "href": "posts/eh4/index.html#k-5",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "k = 5",
    "text": "k = 5\n\nTUNING K MEANS\n\nk = 5\n\nparam_grid = {\n    'init': ['k-means++', 'random'],\n    'n_init': list(range(10, 200)), \n    'algorithm': ['auto', 'lloyd', 'elkan'],\n    'max_iter': list(range(100, 500)),\n    'tol': [1e-4, 1e-3, 1e-2, 1e-1, 1],\n}\n\n\n# Create a KMeans instance with n clusters\nkmeans = KMeans(n_clusters=k)\n\n# Create a silhouette scorer\nsilhouette_scorer = make_scorer(silhouette_score)\n\n# Create a RandomizedSearchCV instance\nrandom_search = RandomizedSearchCV(kmeans, \n                                   param_grid, \n                                   n_iter=50,\n                                   cv=3, \n                                   random_state=594,\n                                   scoring=silhouette_scorer,\n                                  )\n\n# Fit the RandomizedSearchCV instance\nrandom_search.fit(df_pca_without_outliers)\n\n# Get the best parameters\nbest_params_kmeans_5 = random_search.best_params_\n\n# Print the best parameters\nprint(f'Best params for KMeans with k={k}:\\n')\nprint(best_params_kmeans_5)\n\nBest params for KMeans with k=5:\n\n{'tol': 0.0001, 'n_init': 129, 'max_iter': 238, 'init': 'k-means++', 'algorithm': 'elkan'}\n\n\n\n\nTUNING GMM\n\nk = 5\n\n# Define the parameter grid for GMM\nparam_grid_gmm = {\n    'n_init': list(range(1, 21)), \n    'covariance_type': ['full', 'tied', 'diag', 'spherical'],\n    'tol': [1e-3, 1e-4, 1e-5, 1e-6],\n    'reg_covar': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],\n    'max_iter': list(range(100, 500)) \n}\n\n# Create a GMM instance\ngmm = GaussianMixture(n_components=k)\n\n# Create a silhouette scorer\nsilhouette_scorer = make_scorer(silhouette_score)\n\n# Create a RandomizedSearchCV instance for GMM\nrandom_search_gmm = RandomizedSearchCV(gmm, \n                                       param_grid_gmm, \n                                       n_iter=50, \n                                       cv=3, \n                                       random_state=594,\n                                       scoring=silhouette_scorer,\n                                      )\n\n# Fit the RandomizedSearchCV instance to your data\nrandom_search_gmm.fit(df_pca_without_outliers)\n\n# Get the best parameters for GMM\nbest_params_gmm_5 = random_search_gmm.best_params_\n\n# Print the best parameters for GMM\nprint(f'Best params for GMM with k={k}:\\n')\nprint(best_params_gmm_5)\n\nBest params for GMM with k=5:\n\n{'tol': 1e-06, 'reg_covar': 1, 'n_init': 20, 'max_iter': 376, 'covariance_type': 'diag'}\n\n\n\n\nTUNING BIRCH\n\nk = 5\n\n# Define the parameter grid for BIRCH\nparam_grid_birch = {\n    'threshold': np.arange(0.05, 1.05, 0.05),\n    'branching_factor': list(range(20, 200))\n}\n\n# Create a BIRCH instance\nbirch = Birch(n_clusters=k)\n\n# Create a silhouette scorer\nsilhouette_scorer = make_scorer(silhouette_score)\n\n# Create a RandomizedSearchCV instance for BIRCH\nrandom_search_birch = RandomizedSearchCV(birch, \n                                         param_grid_birch, \n                                         n_iter=50, \n                                         cv=3, \n                                         random_state=594,\n                                         scoring=silhouette_scorer,\n                                        )\n\n# Fit the RandomizedSearchCV instance to your data\nrandom_search_birch.fit(df_pca_without_outliers)\n\n# Get the best parameters for BIRCH\nbest_params_birch_5 = random_search_birch.best_params_\n\n# Print the best parameters for BIRCH\nprint(f'Best params for KMeans with k={k}:\\n')\nprint(best_params_birch_5)\n\nBest params for KMeans with k=5:\n\n{'threshold': 0.7500000000000001, 'branching_factor': 80}"
  },
  {
    "objectID": "posts/eh4/index.html#k-4-1",
    "href": "posts/eh4/index.html#k-4-1",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "k = 4",
    "text": "k = 4\n\nENSEMBLE: k=4, UNTUNED\n\nk = 4\n\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594),\n\n    \n    Birch(n_clusters=k),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.32390613387277556\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n3    230\n0    227\n1    202\n2     59\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nENSEMBLE: k=4, KMEANS TUNED\n\nk = 4\n\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594,\n          **best_params_kmeans_4),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594),\n\n    \n    Birch(n_clusters=k),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.2949886894806673\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n2    231\n3    196\n1    184\n0    107\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nENSEMBLE: k=4, KMEANS AND GMM TUNED\n\nk = 4\n\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594,\n          **best_params_kmeans_4),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594,\n                **best_params_gmm_4),\n\n    \n    Birch(n_clusters=k),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.15953504480250802\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n2    327\n0    219\n1    155\n3     17\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nENSEMBLE: k=4, ALL TUNED\n\nk = 4\n\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594,\n          **best_params_kmeans_4),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594,\n                **best_params_gmm_4),\n\n    \n    Birch(n_clusters=k,\n          **best_params_birch_4),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.21204320844898658\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n2    327\n1    188\n0    185\n3     18\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/eh4/index.html#k-5-1",
    "href": "posts/eh4/index.html#k-5-1",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "k = 5",
    "text": "k = 5\n\nENSEMBLE: k=5, UNTUNED\n\nk = 5\n\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594),\n\n    \n    Birch(n_clusters=k),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.3549508024992924\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n0    224\n3    188\n1    165\n4     76\n2     65\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nENSEMBLE: k=5, KMEANS TUNED\n\nk = 4\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594,\n          **best_params_kmeans_5),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594),\n\n    \n    Birch(n_clusters=k),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.2949886894806673\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n2    231\n3    196\n1    184\n0    107\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nENSEMBLE: k=5, KMEANS & GMM TUNED\n\nk = 5\n\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594,\n          **best_params_kmeans_5),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594,\n                **best_params_gmm_5),\n\n    \n    Birch(n_clusters=k),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.39211366933147423\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n2    269\n1    175\n4    148\n0     81\n3     45\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nENSEMBLE: k=5, ALL TUNED\n\nk = 5\n\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594,\n          **best_params_kmeans_5),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594,\n                **best_params_gmm_5),\n\n    \n    Birch(n_clusters=k,\n          **best_params_birch_5),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.37188413554296684\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n2    281\n1    177\n4    148\n0     81\n3     31\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/k1/index.html",
    "href": "posts/k1/index.html",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import randint, uniform\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nfrom lightgbm import LGBMClassifier\nimport sys\nprint(\"Python version:\")\nprint(sys.version)\n\nPython version:\n3.11.4 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 13:38:37) [MSC v.1916 64 bit (AMD64)]"
  },
  {
    "objectID": "posts/k1/index.html#filling-homeplanet-destination-and-vip",
    "href": "posts/k1/index.html#filling-homeplanet-destination-and-vip",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "Filling HomePlanet, Destination and VIP",
    "text": "Filling HomePlanet, Destination and VIP\n\ndf_train['HomePlanet'].value_counts()\n\nHomePlanet\nEarth     4602\nEuropa    2131\nMars      1759\nName: count, dtype: int64\n\n\n\ndf_test['HomePlanet'].value_counts()\n\nHomePlanet\nEarth     2263\nEuropa    1002\nMars       925\nName: count, dtype: int64\n\n\nThe mode for HomePlanet for both the train and test sets is “Earth”, so we use this to fill the null values\n\ndata['HomePlanet'] = data['HomePlanet'].fillna('Earth')\n\n\ndf_train['Destination'].value_counts()\n\nDestination\nTRAPPIST-1e      5915\n55 Cancri e      1800\nPSO J318.5-22     796\nName: count, dtype: int64\n\n\n\ndf_test['Destination'].value_counts()\n\nDestination\nTRAPPIST-1e      2956\n55 Cancri e       841\nPSO J318.5-22     388\nName: count, dtype: int64\n\n\nThe mode for Destination for both the train and test sets is “TRAPPIST-1e”, so we use this to fill the null values\n\ndata['Destination'] = data['Destination'].fillna('TRAPPIST-1e')\n\n\ndf_train['VIP'].value_counts()\n\nVIP\nFalse    8291\nTrue      199\nName: count, dtype: int64\n\n\n\ndf_test['VIP'].value_counts()\n\nVIP\nFalse    4110\nTrue       74\nName: count, dtype: int64\n\n\n\ndata['VIP'] = data['VIP'].fillna(False)\n\n\ndata.isna().sum()\n\nPassengerId            0\nHomePlanet             0\nCryoSleep              0\nCabin                299\nDestination            0\nAge                  270\nVIP                    0\nRoomService          170\nFoodCourt            180\nShoppingMall         175\nSpa                  177\nVRDeck               177\nName                 294\nTransported         4277\nTotalExpenditure       0\ndtype: int64"
  },
  {
    "objectID": "posts/k1/index.html#filling-age-and-the-expenditure-features",
    "href": "posts/k1/index.html#filling-age-and-the-expenditure-features",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "Filling Age and the expenditure features",
    "text": "Filling Age and the expenditure features\nTo fill the remaining null values in Age and Expenses_features we will use the median, to reduce the influence of outliers. This requires seperating data back into constituent train and test sets, to avoid data leakage.\n\ntrain = data[:len(df_train)]\ntest = data[len(df_train):].drop('Transported', axis=1)\n\n\nprint(len(train) == len(df_train))\n\nTrue\n\n\n\ntrain.loc[:, 'Age'] = train['Age'].fillna(train['Age'].median())\ntest.loc[:, 'Age'] = test['Age'].fillna(test['Age'].median())\n\n\ntrain.loc[:,Expenses_features] = train[Expenses_features].fillna(train[Expenses_features].median())\ntest.loc[:,Expenses_features] = test[Expenses_features].fillna(test[Expenses_features].median())\n\n\nprint('Remaining null values in train:\\n')\nprint(train.isna().sum())\nprint('\\nRemaining null values in test:\\n')\nprint(test.isna().sum())\n\nRemaining null values in train:\n\nPassengerId           0\nHomePlanet            0\nCryoSleep             0\nCabin               199\nDestination           0\nAge                   0\nVIP                   0\nRoomService           0\nFoodCourt             0\nShoppingMall          0\nSpa                   0\nVRDeck                0\nName                200\nTransported           0\nTotalExpenditure      0\ndtype: int64\n\nRemaining null values in test:\n\nPassengerId           0\nHomePlanet            0\nCryoSleep             0\nCabin               100\nDestination           0\nAge                   0\nVIP                   0\nRoomService           0\nFoodCourt             0\nShoppingMall          0\nSpa                   0\nVRDeck                0\nName                 94\nTotalExpenditure      0\ndtype: int64\n\n\nRedefine data as the concatenation of train and test\n\ndata = pd.concat([train,test], axis=0)"
  },
  {
    "objectID": "posts/k1/index.html#new-features---agegroup-cabinside-and-groupsize",
    "href": "posts/k1/index.html#new-features---agegroup-cabinside-and-groupsize",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "New features - AgeGroup, CabinSide and GroupSize",
    "text": "New features - AgeGroup, CabinSide and GroupSize\nCreate a new feature AgeGroup by binning the Age feature into 8 different categories.\n\ndata['Age'].max()\n\n79.0\n\n\n\ndata['AgeGroup'] = 0\nfor i in range(8):\n    data.loc[(data.Age &gt;= 10*i) & (data.Age &lt; 10*(i + 1)), 'AgeGroup'] = i\n\n\ndata['AgeGroup'].value_counts()\n\nAgeGroup\n2    4460\n3    2538\n1    2235\n4    1570\n0     980\n5     809\n6     312\n7      66\nName: count, dtype: int64\n\n\nCreate a dummy feature Group by extracting the first character from the PassengerId column. Use Group to define a new feature GroupSize indicating how many people are in the passengers group. Drop the feature Group as it has too many values to be useful.\n\ndata['Group'] = data['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\ndata['GroupSize'] = data['Group'].map(lambda x: data['Group'].value_counts()[x])\ndata = data.drop('Group', axis=1)\n\nCreate a new boolean feature Solo, indicating if a passenger is in a group just by themselves\n\ndata['Solo'] = (data['GroupSize'] == 1).astype(int)\n\nWe won’t use Cabin directly, but we engineer a new feature CabinSide by taking the last character of Cabin. “P” for port and “S” for starboard. To implement this we fill Cabin with a placeholder value.\n\ndata['Cabin'] = data['Cabin'].fillna('T/0/P')\n\n\ndata['CabinSide'] = data['Cabin'].apply(lambda x: x.split('/')[-1])"
  },
  {
    "objectID": "posts/k1/index.html#finishing-preprocessing---dropping-features-and-splitting-into-train-and-test-sets",
    "href": "posts/k1/index.html#finishing-preprocessing---dropping-features-and-splitting-into-train-and-test-sets",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "Finishing preprocessing - dropping features and splitting into train and test sets",
    "text": "Finishing preprocessing - dropping features and splitting into train and test sets\n\ndata = data.drop(['PassengerId','Cabin','Name'], axis=1)\n\n\ndata.isna().sum()\n\nHomePlanet             0\nCryoSleep              0\nDestination            0\nAge                    0\nVIP                    0\nRoomService            0\nFoodCourt              0\nShoppingMall           0\nSpa                    0\nVRDeck                 0\nTransported         4277\nTotalExpenditure       0\nAgeGroup               0\nGroupSize              0\nSolo                   0\nCabinSide              0\ndtype: int64\n\n\n\ndata\n\n\n\n\n\n\n\n\nHomePlanet\nCryoSleep\nDestination\nAge\nVIP\nRoomService\nFoodCourt\nShoppingMall\nSpa\nVRDeck\nTransported\nTotalExpenditure\nAgeGroup\nGroupSize\nSolo\nCabinSide\n\n\n\n\n0\nEuropa\nFalse\nTRAPPIST-1e\n39.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nFalse\n0.0\n3\n1\n1\nP\n\n\n1\nEarth\nFalse\nTRAPPIST-1e\n24.0\nFalse\n109.0\n9.0\n25.0\n549.0\n44.0\nTrue\n736.0\n2\n1\n1\nS\n\n\n2\nEuropa\nFalse\nTRAPPIST-1e\n58.0\nTrue\n43.0\n3576.0\n0.0\n6715.0\n49.0\nFalse\n10383.0\n5\n2\n0\nS\n\n\n3\nEuropa\nFalse\nTRAPPIST-1e\n33.0\nFalse\n0.0\n1283.0\n371.0\n3329.0\n193.0\nFalse\n5176.0\n3\n2\n0\nS\n\n\n4\nEarth\nFalse\nTRAPPIST-1e\n16.0\nFalse\n303.0\n70.0\n151.0\n565.0\n2.0\nTrue\n1091.0\n1\n1\n1\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4272\nEarth\nTrue\nTRAPPIST-1e\n34.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n0.0\n3\n2\n0\nS\n\n\n4273\nEarth\nFalse\nTRAPPIST-1e\n42.0\nFalse\n0.0\n847.0\n17.0\n10.0\n144.0\nNaN\n1018.0\n4\n1\n1\nP\n\n\n4274\nMars\nTrue\n55 Cancri e\n26.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n0.0\n2\n1\n1\nP\n\n\n4275\nEuropa\nFalse\nTRAPPIST-1e\n26.0\nFalse\n0.0\n2680.0\n0.0\n0.0\n523.0\nNaN\n3203.0\n2\n1\n1\nP\n\n\n4276\nEarth\nTrue\nPSO J318.5-22\n43.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n0.0\n4\n1\n1\nS\n\n\n\n\n12970 rows × 16 columns\n\n\n\n\ntrain = data[:len(df_train)]\ntest = data[len(df_train):].drop('Transported', axis=1)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nHomePlanet\nCryoSleep\nDestination\nAge\nVIP\nRoomService\nFoodCourt\nShoppingMall\nSpa\nVRDeck\nTransported\nTotalExpenditure\nAgeGroup\nGroupSize\nSolo\nCabinSide\n\n\n\n\n0\nEuropa\nFalse\nTRAPPIST-1e\n39.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nFalse\n0.0\n3\n1\n1\nP\n\n\n1\nEarth\nFalse\nTRAPPIST-1e\n24.0\nFalse\n109.0\n9.0\n25.0\n549.0\n44.0\nTrue\n736.0\n2\n1\n1\nS\n\n\n2\nEuropa\nFalse\nTRAPPIST-1e\n58.0\nTrue\n43.0\n3576.0\n0.0\n6715.0\n49.0\nFalse\n10383.0\n5\n2\n0\nS\n\n\n3\nEuropa\nFalse\nTRAPPIST-1e\n33.0\nFalse\n0.0\n1283.0\n371.0\n3329.0\n193.0\nFalse\n5176.0\n3\n2\n0\nS\n\n\n4\nEarth\nFalse\nTRAPPIST-1e\n16.0\nFalse\n303.0\n70.0\n151.0\n565.0\n2.0\nTrue\n1091.0\n1\n1\n1\nS\n\n\n\n\n\n\n\n\ntest.head()\n\n\n\n\n\n\n\n\nHomePlanet\nCryoSleep\nDestination\nAge\nVIP\nRoomService\nFoodCourt\nShoppingMall\nSpa\nVRDeck\nTotalExpenditure\nAgeGroup\nGroupSize\nSolo\nCabinSide\n\n\n\n\n0\nEarth\nTrue\nTRAPPIST-1e\n27.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n1\n1\nS\n\n\n1\nEarth\nFalse\nTRAPPIST-1e\n19.0\nFalse\n0.0\n9.0\n0.0\n2823.0\n0.0\n2832.0\n1\n1\n1\nS\n\n\n2\nEuropa\nTrue\n55 Cancri e\n31.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3\n1\n1\nS\n\n\n3\nEuropa\nFalse\nTRAPPIST-1e\n38.0\nFalse\n0.0\n6652.0\n0.0\n181.0\n585.0\n7418.0\n3\n1\n1\nS\n\n\n4\nEarth\nFalse\nTRAPPIST-1e\n20.0\nFalse\n10.0\n0.0\n635.0\n0.0\n0.0\n645.0\n2\n1\n1\nS\n\n\n\n\n\n\n\nThese are our final dataframes for the train and test set. We have engineered new features TotalExpenditure, AgeGroup, GroupSize, Solo and CabinSide. We have filled all null values, and are now nearly ready to train a model"
  },
  {
    "objectID": "posts/ng1/index.html",
    "href": "posts/ng1/index.html",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nfrom PIL import Image\n\n%matplotlib inline\nprint(\"Python version:\")\n!python --version\n\nPython version:\nPython 3.11.4\nThe data and basic architecture of this post is taken from an exercise in the first course of Andrew Ng’s Deep Learning Specialization:"
  },
  {
    "objectID": "posts/ng1/index.html#mathematical-expression-of-the-algorithm",
    "href": "posts/ng1/index.html#mathematical-expression-of-the-algorithm",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "Mathematical expression of the algorithm",
    "text": "Mathematical expression of the algorithm\nFor one training example \\(\\left(x^{(i)},y^{(i)}\\right)\\in \\mathbb{R}^n \\times \\{0,1\\}\\) in the training set \\(\\left\\{ \\left(x^{(i)} , y^{(i)} \\right) \\right\\} _{i=1}^{m}\\) and a choice of parameters \\(w\\in\\mathbb{R}^{n}\\,\\), \\(b\\in\\mathbb{R}\\), forward propagation consists of computing the prediction \\(\\hat y^{(i)}\\in(0,1)\\) of the model as \\(x \\mapsto \\left(w\\cdot x + b\\right) \\mapsto \\sigma\\left( w\\cdot x + b \\right)\\). That is,\n\\[z^{(i)} = w \\cdot x^{(i)} + b\\]\n\\[\\hat{y}^{(i)} = a^{(i)} = \\sigma\\left(z^{(i)}\\right)\\]\nwhere \\(\\sigma: \\mathbb{R} \\to \\left(0,1\\right)\\) is the sigmoid defined above, \\(n\\) is the number of features (length of any of the vectors \\(x\\)) and \\(m = m_{\\text{train}}\\) is the number of training examples. The scalar \\(a^{(i)}\\) is the activation of the perceptron, which is numerically equal to the output \\(\\hat y^{(i)}\\) for a neural network with no hidden layers such as logistic regression.\nThe prediction \\(\\hat{y}^{(i)}\\in \\left(0,1\\right)\\) is interpreted as the probability that \\(x^{(i)}\\) is in class 1 (i.e. is an image of a cat)\n\\[\\hat{y}^{(i)} = \\mathbb{P}\\left( y^{(i)}=1 \\,|\\, x^{(i)} \\,; \\,w, b \\right)\\]\nWe can extract a binary prediction in \\(\\{0,1\\} \\equiv \\{\\text{non-cat},\\text{cat}\\}\\) from the prediction \\(\\hat{y}^{(i)}\\) by applying a threshold \\[y^{(i)}_{\\text{pred}} = \\mathbb{1} {\\left\\{a^{(i)} &gt; 0.5\\right\\}} = \\begin{cases}\n      1 & \\text{if}\\ a^{(i)} &gt; 0.5 \\\\\n      0 & \\text{otherwise}\n    \\end{cases}\n\\]\nSuch a threshold can be implemented in code using, for example, numpy.round.\nTraining the model consists of using the training data to find parameters \\(w,\\, b\\) solving the optimization problem \\[\n\\min_{w\\in\\mathbb{R}^n,\\,b\\in\\mathbb{R}} J(w, b)\n\\] where \\(J = J(w,b): \\mathbb{R}^n \\times \\mathbb{R} \\rightarrow [0,\\infty)\\) is the cost function defined in terms of a loss function \\(\\mathcal{L}\\). The loss function \\(\\mathcal{L}\\) measures the error between the model’s prediction \\(\\hat y^{(i)} = a^{(i)}\\in(0,1)\\) for one of the training examples \\(x^{(i)}\\in\\mathbb{R}^n\\) and the true label \\(y^{(i)}\\in\\{0,1\\}.\\)\nWe use the binary cross entropy (negative log-loss) loss function \\(\\mathcal{L}\\), defined as\n\\[\\begin{align*}\n\\mathcal{L}\\left(a, y\\right) &=  - y  \\log a - \\left(1-y \\right)  \\log\\left(1-a\\right)\\\\[0.2cm]\n&= \\begin{cases}\n- \\log\\left(a\\right) & \\text{if } y = 1 \\\\\n- \\log\\left(1 - a\\right) & \\text{if } y = 0\n\\end{cases}\n\\end{align*}\\]\nThe (unregularized) cost \\(J\\) is then computed by summing over all training examples:\n\\[\\begin{align*}\nJ(w,b) &= \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}\\left(a^{(i)}, y^{(i)}\\right)\\\\[0.2cm]\n  &= -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)}  \\log\\left(a^{(i)}\\right) + \\left(1-y^{(i)} \\right)  \\log\\left(1-a^{(i)}\\right) \\right]\n\\end{align*}\\]\nTo counteract overfitting of the model to the training data one can include a regularization term in the cost function to penalise large weights \\(w\\). For example, with an \\(L_2\\)-regularization term the cost \\(J\\) becomes\n\\[J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}\\left(a^{(i)}, y^{(i)}\\right) + \\frac{\\lambda}{2m}||w||_2^2\\]\nwhere \\(\\lambda \\geq 0\\) is the regularization parameter and \\(||w||_2^2 = w_1^2 + w_2^2 + \\dots + w_n^2 = w \\cdot w\\) denotes the squared Euclidean norm of \\(w\\).\nBackpropagation consists of computing the derivatives\n\\[\\frac{\\partial J (w,b)}{\\partial w_j},\\, \\frac{\\partial J(w,b)}{\\partial b}\\]\nfor use when optimizing \\((w,b)\\) using gradient descent. It is easy to compute that in the unregularized case:\n\\[\\begin{align*}\n\\frac{\\partial J (w,b)}{\\partial w_j} &= \\frac{1}{m}\\sum_{i=1}^m \\left(a^{(i)} - y^{(i)}\\right)x_j^{(i)}\\\\\n\\frac{\\partial J(w,b)}{\\partial b} &= \\frac{1}{m}\\sum_{i=1}^m \\left(a^{(i)} - y^{(i)}\\right)\n\\end{align*}\\]\nWhile in the \\(L_2\\)-regularized case:\n\\[\\begin{align*}\n\\frac{\\partial J (w,b)}{\\partial w_j} &= \\frac{1}{m}\\sum_{i=1}^m \\left(a^{(i)} - y^{(i)}\\right)x_j^{(i)} +\\frac{\\lambda}{m}w_j  \\\\\n\\frac{\\partial J(w,b)}{\\partial b} &= \\frac{1}{m}\\sum_{i=1}^m \\left(a^{(i)} - y^{(i)}\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/ng1/index.html#vectorization",
    "href": "posts/ng1/index.html#vectorization",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "Vectorization",
    "text": "Vectorization\nLooping over all the \\(m\\) training examples \\(\\left (x^{(i)},y^{(i)} \\right)\\) in turn to calculate \\(\\hat{y}^{(i)} = a^{(i)} = \\sigma\\left(z^{(i)}\\right) = \\sigma\\left( w \\cdot x^{(i)} + b\\right)\\) and \\(\\mathcal{L}\\left(a^{(i)}, y^{(i)}\\right)\\) is computationally inefficient if \\(m\\) is large \\(\\left(\\text{e.g.}\\,\\, m\\sim10^6\\right)\\) as is common in modern industry applications.\nBy turning to a so called vectorized implementation we can take advantage of NumPy’s powerful numerical linear algebra capabilities to implement forward propagation more efficiently.\nDefine vectors \\(Z = \\left( z^{(1)}, z^{(2)}, \\dots, z^{(m)} \\right) \\in \\mathbb{R}^m\\) and \\(A = \\left( a^{(1)}, a^{(2)}, \\dots, a^{(m)} \\right) \\in \\mathbb{R}^m\\). Define the \\(n\\,\\times\\,m\\) matrix \\(X\\) with \\(i^{\\text{th}}\\) column \\(x^{(i)}\\). That is,\n\\[\\begin{equation}\nX = \\begin{bmatrix}\n    | & | & \\cdots & | \\\\\n    x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n    | & | & \\cdots & |\n\\end{bmatrix}\\in \\mathcal{M}_{n,m} \\left(\\mathbb{R}\\right)\n\\end{equation}\\]\nThen\n\\[\\begin{align*}\nw^T X + \\left(b,b,\\dots,b\\right) &= \\left( w^T x^{(1)}+b, \\,w^T x^{(2)}+b, \\dots ,\\,w^T x^{(n)}+b \\right)\\\\[0.2cm]\n                                 &= \\left( z^{(1)}, z^{(2)}, \\dots, z^{(m)} \\right)\\\\[0.2cm]\n                                 &= Z\n\\end{align*}\\]\nSo if \\(\\mathbf{b} = \\left(b,b,\\dots,b\\right)\\) then \\(Z = w^T X + \\mathbf{b}\\).\nWe can implement this in code as Z = np.dot(w.T,X) + b where we have taken advantage of python broadcasting to add the scalar b to the array np.dot(w.T,X). NumPy then interprets this addition as element-wise. We then have \\[A = \\left( a^{(1)}, a^{(2)}, \\dots, a^{(m)} \\right) = \\sigma (Z)\\] since the sigmoid \\(\\sigma\\) acts on arrays element-wise.\nA = sigmoid(np.dot(w.T,X) + b) is a computationally efficient implementation of forward propagation across the entire training set at once. In particular, this is more efficient than using a for loop to iterate over each training example in turn."
  },
  {
    "objectID": "posts/ng1/index.html#gradient-descent",
    "href": "posts/ng1/index.html#gradient-descent",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nThe optimization problem \\[\n\\min_{w\\in\\mathbb{R}^n,\\,b\\in\\mathbb{R}} J(w, b)\n\\]\nis numerically solved by gradient descent. For our purposes, gradient descent comprises of iteratively and simultaneously updating \\(b\\) and \\(w\\) according to\n\\[\\begin{align*}\nw_j &\\mapsto w_j - \\alpha \\frac{\\partial J(w,b)}{\\partial w_j}\\\\[0.1cm]\nb &\\mapsto b - \\alpha \\frac{\\partial J(w,b)}{\\partial b}\n\\end{align*}\\]\nwhere \\(\\alpha &lt;&lt; 1\\) is a fixed hyperparameter called the learning rate. Another hyperparameter introduced with gradient descent is the number of iterations num_interations to repeat this updating process."
  },
  {
    "objectID": "posts/ng1/index.html#helper-functions",
    "href": "posts/ng1/index.html#helper-functions",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "Helper Functions",
    "text": "Helper Functions\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    Create a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (int)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias) of type float\n    \"\"\"\n    w = np.zeros((dim,1))\n    b = float(0)\n    \n    return w, b\n\n\ndef forward_propagate(w, b, X):\n    \"\"\"\n    Implements forward propogation across the training set X, computing the activation matrix A \n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n\n    Returns:\n    A -- activation of the neuron, numpy array of size (num_px * num_px * 3, number of examples)\n    \"\"\"\n    A = sigmoid(np.dot(w.T,X)+b)\n    return A\n\n\ndef compute_cost(w, b, X, Y):\n    '''\n    Computes the negative log-likelihood cost J(w,b) across the training set    \n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    \n    Returns:\n    cost -- negative log-likelihood cost for logistic regression\n    '''\n    m = X.shape[1]\n    A = forward_propagate(w, b, X)\n    cost = (-1/m) * (np.dot(Y, np.log(A).T) + np.dot((1 - Y), np.log(1 - A).T))\n    cost = np.squeeze(np.array(cost))  \n    return cost\n\n\ndef backward_propagate(w, b, X, Y):\n    '''\n    Calculates the gradient of the cost function J with respect to the parameters w, b\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    \n    Returns:\n    grads -- dictionary containing the gradients of J w.r.t. the weights and bias\n            (dw -- gradient of the loss with respect to w, thus same shape as w)\n            (db -- gradient of the loss with respect to b, thus same shape as b)\n    '''\n    m = X.shape[1]\n    \n    A = forward_propagate(w, b, X)\n    \n    dw = (1/m)*np.dot(X,(A-Y).T)\n    db = (1/m)*np.sum(A-Y)\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads\n\n\ndef optimize(w, b, X, Y, num_iterations=3000, learning_rate=0.005, print_cost=False):\n    \"\"\"\n    Optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        cost = compute_cost(w, b, X, Y)\n        grads = backward_propagate(w, b, X, Y)\n        \n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n \n        if i % 100 == 0:\n            costs.append(cost)\n        \n            # Print the cost every 100 training iterations\n            if print_cost:\n                print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs\n\n\ndef predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) outputted from `optimize`\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1, m))\n    \n    w = w.reshape(X.shape[0], 1)\n\n    A = sigmoid(np.dot(w.T,X)+b) \n\n    for i in range(A.shape[1]):        \n        Y_prediction[0,i] = np.round(A[0,i]) # Applies a threshold of 0.5\n    \n    return Y_prediction"
  },
  {
    "objectID": "posts/ng1/index.html#combining-into-logistic-regression",
    "href": "posts/ng1/index.html#combining-into-logistic-regression",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "Combining into logistic regression",
    "text": "Combining into logistic regression\nCombining the previously defined functions intialize_with_zeros, propagate, optimize and predict into the logistic regression model\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations=3000, learning_rate=0.005, print_cost=False):\n    \"\"\"\n    Combines the helper functions to construct the unregularized logistic regression model\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to True to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    w, b = initialize_with_zeros(X_train.shape[0])\n    \n    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost=False)\n    \n    w = params['w']\n    b = params['b']\n    \n    Y_prediction_test = predict(w,b,X_test)\n    Y_prediction_train = predict(w,b,X_train)\n\n    # Print train/test Errors\n    if print_cost:\n        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d\n\n\nlogistic_regression_model = model(train_set_x, \n                                  train_set_y, \n                                  test_set_x,\n                                  test_set_y,\n                                  num_iterations=3000,\n                                  learning_rate=0.005,\n                                  print_cost=True)\n\ntrain accuracy: 99.52153110047847 %\ntest accuracy: 68.0 %\n\n\nThe model accuractely classified &gt;99% of the images in the training set and 70% of the images in the test set, suggesting that we are experiences overfitting to our training data."
  },
  {
    "objectID": "posts/ng1/index.html#errors-and-learning-curves",
    "href": "posts/ng1/index.html#errors-and-learning-curves",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "Errors and learning curves",
    "text": "Errors and learning curves\n\n# Example of a 'cat' that was inaccuractely classified as 'not-cat' (False Negative)\nindex = 10\nplt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", predicted as \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 1, predicted as \"non-cat\" picture.\n\n\n\n\n\n\n# Example of a 'not-cat' that was inaccuractely classified as 'cat' (False Positive)\nindex = 34\nplt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", predicted as \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 0, predicted as \"cat\" picture.\n\n\n\n\n\n\n# Plot learning curve (with costs)\ncosts = np.squeeze(logistic_regression_model['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate = \" + str(logistic_regression_model[\"learning_rate\"]))\nplt.show()"
  },
  {
    "objectID": "posts/ng1/index.html#animal-testing",
    "href": "posts/ng1/index.html#animal-testing",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "Animal Testing",
    "text": "Animal Testing\n\ndef is_cat(image_str):\n    '''\n    Applies the trained logistic regression model to predict if an inputted image is a cat (y=1) or a non-cat (y=0)\n    \n    Arguments:\n    image_str - a string encoding the file name of the .jpg file, \n                e.g. 'cat.jpg' if cat.jpg is the file name of an image saved in the same directory as this notebook.\n    Returns:\n    None\n    '''\n    # Read the original image\n    original_image = np.array(Image.open(image_str))\n    \n    # Show the original image\n    plt.subplot(1, 2, 1)\n    plt.imshow(original_image)\n    plt.title(\"Original Image\")\n    \n    # Resize the image to 64x64\n    resized_image = np.array(Image.open(image_str).resize((num_px, num_px)))\n    \n    # Show the resized image\n    plt.subplot(1, 2, 2)\n    plt.imshow(resized_image)\n    plt.title(\"Resized Image (64x64)\")\n    \n    # Standardize and flatten the resized image \n    image = resized_image / 255.\n    image = image.reshape((1, num_px * num_px * 3)).T\n    \n    # Predict label using training logistic regression model\n    my_predicted_image = predict(logistic_regression_model[\"w\"], logistic_regression_model[\"b\"], image)\n    \n    # Print the prediction for the resized image\n    print(\"y = \" + str(int(np.squeeze(my_predicted_image))) + \", the model predicts this is a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n\nLet’s test the function is_cat on an image of my own cat, William.\nHe’s middle-aged and overweight.\n\nis_cat('img/william.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\nUnsuprising. He’s always been a disappointment.\nObserving on some other images in my camera roll:\n\nis_cat('img/flora.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat('img/ginger_greek_cat.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\n\nis_cat('img/william_yawning.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat('img/cambridge_cat.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat('img/lexi.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\n\nis_cat('img/toby.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat('img/mr_president.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\n\nis_cat('img/bojack.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat('img/american_football.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\nOh dear. Time to do some tuning."
  },
  {
    "objectID": "posts/ng1/index.html#regularizing-the-helper-functions",
    "href": "posts/ng1/index.html#regularizing-the-helper-functions",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "Regularizing the helper functions",
    "text": "Regularizing the helper functions\n\ndef compute_cost_regularized(w, b, X, Y, lambda_):\n    '''\n    Computes the L2-regularized negative log-likelihood cost J(w,b) across the training set\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    lambda_ -- regularization hyperparameter\n    \n    Returns:\n    cost -- L2-regularized negative log-likelihood cost for logistic regression\n    '''\n    m = X.shape[1]\n    reg_cost = compute_cost(w, b, X, Y) + (lambda_/m)*(np.linalg.norm(w)**2)\n    \n    return reg_cost\n\n\ndef backward_propagate_regularized(w, b, X, Y, lambda_):\n    '''\n    Calculates the gradient of the L2-regularized cost function J with respect to the parameters w, b\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    lambda_ -- regularization hyperparameter\n    \n    Returns:\n    grads -- dictionary containing the gradients of J w.r.t. the weights and bias\n            (dw -- gradient of the loss with respect to w, thus same shape as w)\n            (db -- gradient of the loss with respect to b, thus same shape as b)\n    '''\n    m = X.shape[1]\n    grads = backward_propagate(w, b, X, Y)\n    grads['dw'] = grads['dw'] + (lambda_/m)*w\n    \n    return grads\n\n\ndef optimize_regularized(w, b, X, Y, num_iterations=3000, learning_rate=0.005, lambda_=0, print_cost=False):\n    \"\"\"\n    Optimizes w and b by running a gradient descent algorithm on the regularized cost function\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    lambda_ -- regularization hyperparameter\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        cost = compute_cost_regularized(w, b, X, Y, lambda_)\n        grads = backward_propagate_regularized(w, b, X, Y, lambda_)\n        \n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n \n        if i % 100 == 0:\n            costs.append(cost)\n        \n            # Print the cost every 100 training iterations\n            if print_cost:\n                print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs"
  },
  {
    "objectID": "posts/ng1/index.html#combining-into-regularized-logistic-regression",
    "href": "posts/ng1/index.html#combining-into-regularized-logistic-regression",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "Combining into regularized logistic regression",
    "text": "Combining into regularized logistic regression\n\ndef regularized_model(X_train, Y_train, X_test, Y_test, num_iterations=3000, learning_rate=0.5, lambda_=0, print_cost=False):\n    \"\"\"\n    Combines the helper functions to construct the regularized model\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    lambda_ -- regularization hyperparameter\n    print_cost -- Set to True to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    w, b = initialize_with_zeros(X_train.shape[0])\n    \n    params, grads, costs = optimize_regularized(w, b, X_train, Y_train, num_iterations, learning_rate, lambda_, print_cost=False)\n    \n    w = params['w']\n    b = params['b']\n    \n    Y_prediction_test = predict(w,b,X_test)\n    Y_prediction_train = predict(w,b,X_train)\n\n    # Print train/test Errors\n    if print_cost:\n        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d\n\n\nlogistic_regression_model_regularized = regularized_model(train_set_x,\n                                                          train_set_y,\n                                                          test_set_x,\n                                                          test_set_y,\n                                                          num_iterations=3000,\n                                                          learning_rate=0.005,\n                                                          lambda_=100,\n                                                          print_cost=True)\n\ntrain accuracy: 89.47368421052632 %\ntest accuracy: 74.0 %"
  },
  {
    "objectID": "posts/ng1/index.html#tuning-the-regularization-parameter",
    "href": "posts/ng1/index.html#tuning-the-regularization-parameter",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "Tuning the regularization parameter",
    "text": "Tuning the regularization parameter\n\ndef tune_lambda(lambdas):\n    '''\n    Trains the regularized model with a choice of different regularization hyperparameters lambda_\n    \n    Arguments:\n    lambdas - a list of regularization hyperparameters lambda_\n    \n    Returns:\n    None\n    '''\n    for lambda_ in lambdas:\n        print(f\"Training a model with regularization parameter lambda = {lambda_}\")\n        regularized_model(train_set_x,\n                          train_set_y,\n                          test_set_x,\n                          test_set_y,\n                          num_iterations=3000,\n                          learning_rate=0.005,\n                          lambda_=lambda_,\n                          print_cost=True)\n        print(\"\\n\") \n\n\nlambdas = [0, 50, 100, 150, 200, 250]\ntune_lambda(lambdas)\n\nTraining a model with regularization parameter lambda = 0\ntrain accuracy: 99.52153110047847 %\ntest accuracy: 68.0 %\n\n\nTraining a model with regularization parameter lambda = 50\ntrain accuracy: 95.69377990430623 %\ntest accuracy: 74.0 %\n\n\nTraining a model with regularization parameter lambda = 100\ntrain accuracy: 89.47368421052632 %\ntest accuracy: 74.0 %\n\n\nTraining a model with regularization parameter lambda = 150\ntrain accuracy: 84.21052631578948 %\ntest accuracy: 80.0 %\n\n\nTraining a model with regularization parameter lambda = 200\ntrain accuracy: 75.59808612440192 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 250\ntrain accuracy: 72.72727272727273 %\ntest accuracy: 82.0 %\n\n\n\n\n\nlambdas = [130, 140, 150, 160, 170]\ntune_lambda(lambdas)\n\nTraining a model with regularization parameter lambda = 130\ntrain accuracy: 87.08133971291866 %\ntest accuracy: 80.0 %\n\n\nTraining a model with regularization parameter lambda = 140\ntrain accuracy: 87.08133971291866 %\ntest accuracy: 80.0 %\n\n\nTraining a model with regularization parameter lambda = 150\ntrain accuracy: 84.21052631578948 %\ntest accuracy: 80.0 %\n\n\nTraining a model with regularization parameter lambda = 160\ntrain accuracy: 81.33971291866028 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 170\ntrain accuracy: 80.38277511961722 %\ntest accuracy: 82.0 %\n\n\n\n\n\nlambdas = [150, 152, 154, 156, 158, 160]\ntune_lambda(lambdas)\n\nTraining a model with regularization parameter lambda = 150\ntrain accuracy: 84.21052631578948 %\ntest accuracy: 80.0 %\n\n\nTraining a model with regularization parameter lambda = 152\ntrain accuracy: 83.73205741626793 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 154\ntrain accuracy: 83.25358851674642 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 156\ntrain accuracy: 83.25358851674642 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 158\ntrain accuracy: 82.29665071770334 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 160\ntrain accuracy: 81.33971291866028 %\ntest accuracy: 82.0 %"
  },
  {
    "objectID": "posts/ng1/index.html#more-animal-testing",
    "href": "posts/ng1/index.html#more-animal-testing",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "More animal testing",
    "text": "More animal testing\n\ntuned_regularized_model = regularized_model(train_set_x,\n                                            train_set_y,\n                                            test_set_x,\n                                            test_set_y,\n                                            num_iterations=3000,\n                                            learning_rate=0.005,\n                                            lambda_=152,\n                                            print_cost=True)\n\ntrain accuracy: 83.73205741626793 %\ntest accuracy: 82.0 %\n\n\n\ndef is_cat_tuned_regularized(image_str):\n    '''\n    Applies the trained regularized & tuned logistic regression model to predict if an inputted image is a cat (y=1) or a non-cat (y=0)\n    \n    Arguments:\n    image_str - a string encoding the file name of the .jpg file, \n                e.g. 'cat.jpg' if cat.jpg is the file name of an image saved in the same directory as this notebook.\n    Returns:\n    None\n    '''\n    # Read the original image\n    original_image = np.array(Image.open(image_str))\n    \n    # Show the original image\n    plt.subplot(1, 2, 1)\n    plt.imshow(original_image)\n    plt.title(\"Original Image\")\n    \n    # Resize the image to 64x64\n    resized_image = np.array(Image.open(image_str).resize((num_px, num_px)))\n    \n    # Show the resized image\n    plt.subplot(1, 2, 2)\n    plt.imshow(resized_image)\n    plt.title(\"Resized Image (64x64)\")\n    \n    # Standardize and flatten the resized image \n    image = resized_image / 255.\n    image = image.reshape((1, num_px * num_px * 3)).T\n    \n    # Predict label using training logistic regression model\n    my_predicted_image = predict(tuned_regularized_model[\"w\"], tuned_regularized_model[\"b\"], image)\n    \n    # Print the prediction for the resized image\n    print(\"y = \" + str(int(np.squeeze(my_predicted_image))) + \", the model predicts this is a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n\n\nis_cat_tuned_regularized('img/william.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('img/flora.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('img/ginger_greek_cat.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('img/william_yawning.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('img/cambridge_cat.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('img/lexi.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('img/toby.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('img/mr_president.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('img/bojack.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('img/american_football.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture."
  },
  {
    "objectID": "posts/ng3/index.html",
    "href": "posts/ng3/index.html",
    "title": "Convolutional Neural Networks in TensorFlow",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom sklearn.model_selection import train_test_split\nfrom utils import *\nversions()\n\n+------------+---------+\n| Component  | Version |\n+------------+---------+\n|   Python   |  3.12.2 |\n+------------+---------+\n| TensorFlow |  2.16.1 |\n+------------+---------+"
  },
  {
    "objectID": "posts/ng3/index.html#model-1---convx2",
    "href": "posts/ng3/index.html#model-1---convx2",
    "title": "Convolutional Neural Networks in TensorFlow",
    "section": "Model 1 - CONVx2",
    "text": "Model 1 - CONVx2\nThe architecture of the first model is:\n\nInput Layer: The input layer accepts images of shape (64, 64, 3), which corresponds to a 64x64 RGB image.\nConvolutional Layer 1: The first convolutional layer has 8 filters, each of size (4, 4). The stride is 1, and the padding strategy is ‘same’, which means that zero-padding is used to preserve the spatial dimensions of the input.\nReLU Activation Function 1: The first ReLU activation function introduces non-linearity after the first convolutional layer.\nMax Pooling Layer 1: The first max pooling layer uses a pooling window of size (4, 4) and a stride of 1. The padding strategy is ‘same’.\nConvolutional Layer 2: The second convolutional layer has 16 filters, each of size (2, 2). The stride is 1, and the padding strategy is ‘same’.\nReLU Activation Function 2: The second ReLU activation function introduces non-linearity after the second convolutional layer.\nMax Pooling Layer 2: The second max pooling layer uses a pooling window of size (8, 8) and a stride of 1. The padding strategy is ‘same’.\nFlatten Layer: The flatten layer reshapes the 3D output of the previous layer into a 1D vector.\nOutput Layer: The output layer is a dense layer with 6 neurons. The activation function is linear, but when calculating the loss during training, a softmax activation function is applied to the logits because from_logits=True is used in the loss function. This means that the model’s output is expected to be logits and the softmax activation function is applied to convert these logits to probabilities when calculating the loss.\n\n This architecture can be represented as:\n\n[CONV2D -&gt; RELU -&gt; MAXPOOL] * 2 -&gt; FLATTEN -&gt; DENSE (Softmax activation from logits)\n\n\nconv_params = [\n    {'filters': 8, 'kernel_size': (4, 4), 'strides': 1, 'padding':'same', 'pool_size': (4, 4)},\n    {'filters': 16, 'kernel_size': (2, 2), 'strides': 1, 'padding':'same', 'pool_size': (8, 8)}\n]\ndense_layer_dims = [6]  # 1 output layer with 6 neurons\n\nmodel = CNN(input_shape=(64, 64, 3), conv_params=conv_params, dense_layer_dims=dense_layer_dims)\n\n\n%%time     \n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,  ## DEFAULT HYPERPARAMS ## \n                                                 beta_1=0.9,\n                                                 beta_2=0.999,\n                                                 epsilon=1e-07,),\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nmodel1_history = model.fit(train_dataset, epochs=20, validation_data=val_dataset)\n\nprint('\\nTraining Complete.\\n')\n\nEpoch 1/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 51ms/step - accuracy: 0.1624 - loss: 4.5708 - val_accuracy: 0.1358 - val_loss: 2.1793\nEpoch 2/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - accuracy: 0.2397 - loss: 2.0271 - val_accuracy: 0.3210 - val_loss: 1.6034\nEpoch 3/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - accuracy: 0.4177 - loss: 1.5144 - val_accuracy: 0.5556 - val_loss: 1.2332\nEpoch 4/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - accuracy: 0.6214 - loss: 1.1302 - val_accuracy: 0.7593 - val_loss: 0.9770\nEpoch 5/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 46ms/step - accuracy: 0.7430 - loss: 0.8014 - val_accuracy: 0.8148 - val_loss: 0.7882\nEpoch 6/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - accuracy: 0.7978 - loss: 0.6133 - val_accuracy: 0.7901 - val_loss: 0.6515\nEpoch 7/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 47ms/step - accuracy: 0.8492 - loss: 0.4740 - val_accuracy: 0.8333 - val_loss: 0.5664\nEpoch 8/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - accuracy: 0.8797 - loss: 0.3754 - val_accuracy: 0.8148 - val_loss: 0.5370\nEpoch 9/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - accuracy: 0.9048 - loss: 0.3061 - val_accuracy: 0.8210 - val_loss: 0.5078\nEpoch 10/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 46ms/step - accuracy: 0.9223 - loss: 0.2731 - val_accuracy: 0.8580 - val_loss: 0.4567\nEpoch 11/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - accuracy: 0.9344 - loss: 0.2344 - val_accuracy: 0.8704 - val_loss: 0.4783\nEpoch 12/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - accuracy: 0.9333 - loss: 0.1945 - val_accuracy: 0.8519 - val_loss: 0.4340\nEpoch 13/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - accuracy: 0.9783 - loss: 0.1474 - val_accuracy: 0.8827 - val_loss: 0.4232\nEpoch 14/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 46ms/step - accuracy: 0.9740 - loss: 0.1254 - val_accuracy: 0.8765 - val_loss: 0.4111\nEpoch 15/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 44ms/step - accuracy: 0.9835 - loss: 0.1086 - val_accuracy: 0.8704 - val_loss: 0.4078\nEpoch 16/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - accuracy: 0.9909 - loss: 0.0949 - val_accuracy: 0.8827 - val_loss: 0.4102\nEpoch 17/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - accuracy: 0.9914 - loss: 0.0825 - val_accuracy: 0.9012 - val_loss: 0.3916\nEpoch 18/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - accuracy: 0.9874 - loss: 0.0786 - val_accuracy: 0.9012 - val_loss: 0.3793\nEpoch 19/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - accuracy: 0.9897 - loss: 0.0721 - val_accuracy: 0.8951 - val_loss: 0.3809\nEpoch 20/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 1s 45ms/step - accuracy: 0.9890 - loss: 0.0725 - val_accuracy: 0.8765 - val_loss: 0.4653\n\nTraining Complete.\n\nCPU times: total: 2min 7s\nWall time: 14.5 s\n\n\n\nplot_model_history(model1_history)\n\n\n\n\n\ntest_loss, test_accuracy = model.evaluate(test_dataset)\n\nprint(f\"\\nTest accuracy: {test_accuracy*100:.2f}%\")\n\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step - accuracy: 0.8479 - loss: 0.4076\n\nTest accuracy: 85.00%\n\n\n\npred = np.argmax(model.predict(X_test), axis=1)\nprint_mislabeled_images(classes, X_test, Y_test_orig, pred, number=5)\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step"
  },
  {
    "objectID": "posts/ng3/index.html#model-2---convx2-densex3",
    "href": "posts/ng3/index.html#model-2---convx2-densex3",
    "title": "Convolutional Neural Networks in TensorFlow",
    "section": "Model 2 - CONVx2 + DENSEx3",
    "text": "Model 2 - CONVx2 + DENSEx3\nThe architecture of the second model is:\n\nInput Layer: The input layer accepts images of shape (64, 64, 3), which corresponds to a 64x64 RGB image.\nConvolutional Layer 1: The first convolutional layer has 8 filters, each of size (4, 4). The stride is 1, and the padding strategy is ‘same’, which means that zero-padding is used to preserve the spatial dimensions of the input.\nReLU Activation Function 1: The first ReLU activation function introduces non-linearity after the first convolutional layer.\nMax Pooling Layer 1: The first max pooling layer uses a pooling window of size (4, 4) and a stride of 1. The padding strategy is ‘same’.\nConvolutional Layer 2: The second convolutional layer has 16 filters, each of size (2, 2). The stride is 1, and the padding strategy is ‘same’.\nReLU Activation Function 2: The second ReLU activation function introduces non-linearity after the second convolutional layer.\nMax Pooling Layer 2: The second max pooling layer uses a pooling window of size (8, 8) and a stride of 1. The padding strategy is ‘same’.\nFlatten Layer: The flatten layer reshapes the 3D output of the previous layer into a 1D vector.\nDense Layer 1: The first dense layer has 128 neurons and uses a ReLU activation function.\nDense Layer 2: The second dense layer has 64 neurons and uses a ReLU activation function.\nDense Layer 3: The third dense layer has 32 neurons and uses a ReLU activation function.\nOutput Layer: The output layer is a dense layer with 6 neurons. The activation function is linear, but when calculating the loss during training, a softmax activation function is applied to the logits because from_logits=True is used in the loss function. This means that the model’s output is expected to be logits and the softmax activation function is applied to convert these logits to probabilities when calculating the loss.\n\n This architecture can be represented as:\n\n[CONV2D -&gt; RELU -&gt; MAXPOOL] * 2 -&gt; FLATTEN -&gt; [DENSE (ReLU activation)] * 3 -&gt; DENSE (Softmax activation from logits)\n\n\nconv_params = [\n    {'filters': 8, 'kernel_size': (4, 4), 'strides': 1, 'padding':'same', 'pool_size': (4, 4)},\n    {'filters': 16, 'kernel_size': (2, 2), 'strides': 1, 'padding':'same', 'pool_size': (8, 8)}\n]\ndense_layer_dims = [128, 64, 32, 6]\n\nmodel2 = CNN(input_shape=(64, 64, 3), conv_params=conv_params, dense_layer_dims=dense_layer_dims)\n\n\n%%time  \n\nmodel2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,  ## DEFAULT HYPERPARAMS ## \n                                                  beta_1=0.9,\n                                                  beta_2=0.999,\n                                                  epsilon=1e-07,),\n               loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n               metrics=['accuracy'])\n\nmodel2_history = model2.fit(train_dataset, epochs=20, validation_data=val_dataset)\n\nprint('\\nTraining Complete.\\n')\n\nEpoch 1/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 3s 109ms/step - accuracy: 0.1684 - loss: 2.7840 - val_accuracy: 0.1296 - val_loss: 1.8596\nEpoch 2/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 103ms/step - accuracy: 0.2148 - loss: 1.7787 - val_accuracy: 0.3457 - val_loss: 1.7083\nEpoch 3/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 101ms/step - accuracy: 0.3261 - loss: 1.6809 - val_accuracy: 0.3765 - val_loss: 1.5629\nEpoch 4/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 104ms/step - accuracy: 0.3701 - loss: 1.5516 - val_accuracy: 0.4444 - val_loss: 1.3756\nEpoch 5/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 103ms/step - accuracy: 0.4248 - loss: 1.4038 - val_accuracy: 0.4074 - val_loss: 1.2754\nEpoch 6/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 109ms/step - accuracy: 0.4994 - loss: 1.1962 - val_accuracy: 0.6420 - val_loss: 1.0361\nEpoch 7/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 107ms/step - accuracy: 0.6326 - loss: 0.9407 - val_accuracy: 0.6111 - val_loss: 0.8805\nEpoch 8/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 110ms/step - accuracy: 0.6985 - loss: 0.7753 - val_accuracy: 0.7901 - val_loss: 0.7639\nEpoch 9/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 103ms/step - accuracy: 0.7606 - loss: 0.6323 - val_accuracy: 0.7716 - val_loss: 0.7495\nEpoch 10/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 104ms/step - accuracy: 0.8253 - loss: 0.5350 - val_accuracy: 0.8025 - val_loss: 0.6187\nEpoch 11/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 104ms/step - accuracy: 0.8897 - loss: 0.3794 - val_accuracy: 0.8333 - val_loss: 0.5593\nEpoch 12/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 108ms/step - accuracy: 0.8975 - loss: 0.3293 - val_accuracy: 0.8519 - val_loss: 0.5415\nEpoch 13/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 107ms/step - accuracy: 0.9072 - loss: 0.2789 - val_accuracy: 0.7840 - val_loss: 0.5942\nEpoch 14/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 102ms/step - accuracy: 0.9140 - loss: 0.2676 - val_accuracy: 0.8827 - val_loss: 0.4759\nEpoch 15/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 104ms/step - accuracy: 0.9328 - loss: 0.2261 - val_accuracy: 0.8457 - val_loss: 0.5378\nEpoch 16/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 103ms/step - accuracy: 0.9141 - loss: 0.2515 - val_accuracy: 0.8765 - val_loss: 0.4160\nEpoch 17/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 105ms/step - accuracy: 0.9368 - loss: 0.1952 - val_accuracy: 0.8457 - val_loss: 0.5423\nEpoch 18/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 102ms/step - accuracy: 0.9613 - loss: 0.1689 - val_accuracy: 0.8395 - val_loss: 0.5287\nEpoch 19/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 104ms/step - accuracy: 0.9651 - loss: 0.1352 - val_accuracy: 0.9074 - val_loss: 0.3936\nEpoch 20/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 2s 105ms/step - accuracy: 0.9449 - loss: 0.1510 - val_accuracy: 0.9074 - val_loss: 0.4150\n\nTraining Complete.\n\nCPU times: total: 4min 52s\nWall time: 32.7 s\n\n\n\nplot_model_history(model2_history)\n\n\n\n\n\ntest_loss, test_accuracy = model2.evaluate(test_dataset)\n\nprint(f\"\\nTest accuracy: {test_accuracy*100:.2f}%\")\n\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.8747 - loss: 0.3343\n\nTest accuracy: 86.67%\n\n\n\npred = np.argmax(model2.predict(X_test), axis=1)\nprint_mislabeled_images(classes, X_test, Y_test_orig, pred, number=5)\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step"
  },
  {
    "objectID": "posts/ng3/index.html#model-3---convx4-densex3",
    "href": "posts/ng3/index.html#model-3---convx4-densex3",
    "title": "Convolutional Neural Networks in TensorFlow",
    "section": "Model 3 - CONVx4 + DENSEx3",
    "text": "Model 3 - CONVx4 + DENSEx3\nThe architecture of the third model is:\n\nInput Layer: The input layer accepts images of shape (64, 64, 3), which corresponds to a 64x64 RGB image.\nConvolutional Layer 1: The first convolutional layer has 8 filters, each of size (4, 4). The stride is 1, and the padding strategy is ‘same’, which means that zero-padding is used to preserve the spatial dimensions of the input.\nReLU Activation Function 1: The first ReLU activation function introduces non-linearity after the first convolutional layer.\nMax Pooling Layer 1: The first max pooling layer uses a pooling window of size (4, 4) and a stride of 1. The padding strategy is ‘same’.\nConvolutional Layer 2: The second convolutional layer has 16 filters, each of size (2, 2). The stride is 1, and the padding strategy is ‘same’.\nReLU Activation Function 2: The second ReLU activation function introduces non-linearity after the second convolutional layer.\nMax Pooling Layer 2: The second max pooling layer uses a pooling window of size (8, 8) and a stride of 1. The padding strategy is ‘same’.\nConvolutional Layer 3: The third convolutional layer has 32 filters, each of size (2, 2). The stride is 1, and the padding strategy is ‘same’.\nReLU Activation Function 3: The third ReLU activation function introduces non-linearity after the third convolutional layer.\nMax Pooling Layer 3: The third max pooling layer uses a pooling window of size (4, 4) and a stride of 1. The padding strategy is ‘same’.\nConvolutional Layer 4: The fourth convolutional layer has 64 filters, each of size (2, 2). The stride is 1, and the padding strategy is ‘same’.\nReLU Activation Function 4: The fourth ReLU activation function introduces non-linearity after the fourth convolutional layer.\nMax Pooling Layer 4: The fourth max pooling layer uses a pooling window of size (4, 4) and a stride of 1. The padding strategy is ‘same’.\nFlatten Layer: The flatten layer reshapes the 3D output of the previous layer into a 1D vector.\nDense Layer 1: The first dense layer has 128 neurons and uses a ReLU activation function.\nDense Layer 2: The second dense layer has 64 neurons and uses a ReLU activation function.\nDense Layer 3: The third dense layer has 32 neurons and uses a ReLU activation function.\nOutput Layer: The output layer is a dense layer with 6 neurons. The activation function is linear, but when calculating the loss during training, a softmax activation function is applied to the logits because from_logits=True is used in the loss function. This means that the model’s output is expected to be logits and the softmax activation function is applied to convert these logits to probabilities when calculating the loss.\n\n This architecture can be represented as:\n\n[CONV2D -&gt; RELU -&gt; MAXPOOL] * 4 -&gt; FLATTEN -&gt; [DENSE (ReLU activation)] * 3 -&gt; DENSE (Softmax activation from logits)\n\n\nconv_params = [\n    {'filters': 8, 'kernel_size': (4, 4), 'strides': 1, 'padding':'same', 'pool_size': (4, 4)},\n    {'filters': 16, 'kernel_size': (2, 2), 'strides': 1, 'padding':'same', 'pool_size': (8, 8)},\n    {'filters': 32, 'kernel_size': (2, 2), 'strides': 1, 'padding':'same', 'pool_size': (4, 4)},\n    {'filters': 64, 'kernel_size': (2, 2), 'strides': 1, 'padding':'same', 'pool_size': (4, 4)},\n]\ndense_layer_dims = [128, 64, 32, 6]\n\nmodel3 = CNN(input_shape=(64, 64, 3), conv_params=conv_params, dense_layer_dims=dense_layer_dims)\n\n\n%%time\n\nmodel3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,  ## DEFAULT HYPERPARAMS ## \n                                                  beta_1=0.9,\n                                                  beta_2=0.999,\n                                                  epsilon=1e-07,),\n               loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n               metrics=['accuracy'])\n\nmodel3_history = model3.fit(train_dataset, epochs=20, validation_data=val_dataset)\n\nprint('\\nTraining Complete.\\n')\n\nEpoch 1/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 8s 401ms/step - accuracy: 0.1967 - loss: 3.1120 - val_accuracy: 0.1358 - val_loss: 1.7858\nEpoch 2/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 403ms/step - accuracy: 0.2096 - loss: 1.7440 - val_accuracy: 0.2654 - val_loss: 1.7415\nEpoch 3/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 398ms/step - accuracy: 0.3801 - loss: 1.5042 - val_accuracy: 0.4383 - val_loss: 1.3922\nEpoch 4/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 392ms/step - accuracy: 0.5176 - loss: 1.2117 - val_accuracy: 0.5556 - val_loss: 1.1236\nEpoch 5/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 393ms/step - accuracy: 0.5856 - loss: 1.0351 - val_accuracy: 0.6667 - val_loss: 0.9586\nEpoch 6/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 408ms/step - accuracy: 0.6690 - loss: 0.8851 - val_accuracy: 0.7284 - val_loss: 0.7769\nEpoch 7/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 400ms/step - accuracy: 0.7404 - loss: 0.6485 - val_accuracy: 0.7716 - val_loss: 0.6890\nEpoch 8/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 390ms/step - accuracy: 0.8220 - loss: 0.4962 - val_accuracy: 0.7593 - val_loss: 0.6425\nEpoch 9/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 393ms/step - accuracy: 0.8452 - loss: 0.4354 - val_accuracy: 0.8827 - val_loss: 0.4200\nEpoch 10/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 390ms/step - accuracy: 0.8666 - loss: 0.3775 - val_accuracy: 0.8827 - val_loss: 0.3574\nEpoch 11/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 394ms/step - accuracy: 0.9215 - loss: 0.2436 - val_accuracy: 0.9012 - val_loss: 0.3768\nEpoch 12/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 390ms/step - accuracy: 0.9291 - loss: 0.2042 - val_accuracy: 0.8951 - val_loss: 0.3472\nEpoch 13/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 389ms/step - accuracy: 0.9311 - loss: 0.2238 - val_accuracy: 0.7901 - val_loss: 0.4974\nEpoch 14/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 390ms/step - accuracy: 0.9552 - loss: 0.1670 - val_accuracy: 0.9321 - val_loss: 0.2744\nEpoch 15/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 389ms/step - accuracy: 0.9671 - loss: 0.0944 - val_accuracy: 0.9259 - val_loss: 0.2453\nEpoch 16/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 391ms/step - accuracy: 0.9792 - loss: 0.0825 - val_accuracy: 0.9259 - val_loss: 0.2370\nEpoch 17/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 402ms/step - accuracy: 0.9820 - loss: 0.0452 - val_accuracy: 0.9259 - val_loss: 0.2395\nEpoch 18/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 433ms/step - accuracy: 0.9895 - loss: 0.0272 - val_accuracy: 0.9444 - val_loss: 0.1991\nEpoch 19/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 400ms/step - accuracy: 1.0000 - loss: 0.0240 - val_accuracy: 0.9198 - val_loss: 0.2222\nEpoch 20/20\n15/15 ━━━━━━━━━━━━━━━━━━━━ 6s 421ms/step - accuracy: 0.9991 - loss: 0.0154 - val_accuracy: 0.9136 - val_loss: 0.2425\n\nTraining Complete.\n\nCPU times: total: 20min 11s\nWall time: 2min 1s\n\n\n\nplot_model_history(model3_history)\n\n\n\n\n\ntest_loss, test_accuracy = model3.evaluate(test_dataset)\n\nprint(f\"\\nTest accuracy: {test_accuracy*100:.2f}%\")\n\n2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - accuracy: 0.8965 - loss: 0.2692\n\nTest accuracy: 89.17%\n\n\n\npred = np.argmax(model3.predict(X_test), axis=1)\nprint_mislabeled_images(classes, X_test, Y_test_orig, pred, number=5)\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 1s 37ms/step"
  },
  {
    "objectID": "posts/ng3/index.html#model-4---convx5-densex4-regularization",
    "href": "posts/ng3/index.html#model-4---convx5-densex4-regularization",
    "title": "Convolutional Neural Networks in TensorFlow",
    "section": "Model 4 - CONVx5 + DENSEx4 + REGULARIZATION",
    "text": "Model 4 - CONVx5 + DENSEx4 + REGULARIZATION\nThe architecture of the fourth model is:\n\nInput Layer: The input layer accepts images of shape (64, 64, 3), which corresponds to a 64x64 RGB image.\nConvolutional Layer 1: The first convolutional layer has 8 filters, each of size (4, 4). The stride is 1, and the padding strategy is ‘same’. After the convolutional layer, a ReLU activation function introduces non-linearity, and a dropout layer is applied with a rate of 0.25.\nMax Pooling Layer 1: The first max pooling layer uses a pooling window of size (4, 4).\nConvolutional Layer 2: The second convolutional layer has 16 filters, each of size (4, 4). The stride is 1, and the padding strategy is ‘same’. After the convolutional layer, a ReLU activation function introduces non-linearity, and a dropout layer is applied with a rate of 0.25.\nMax Pooling Layer 2: The second max pooling layer uses a pooling window of size (8, 8).\nConvolutional Layer 3: The third convolutional layer has 32 filters, each of size (2, 2). The stride is 1, and the padding strategy is ‘same’. After the convolutional layer, a ReLU activation function introduces non-linearity, and a dropout layer is applied with a rate of 0.25.\nMax Pooling Layer 3: The third max pooling layer uses a pooling window of size (4, 4).\nConvolutional Layer 4: The fourth convolutional layer has 64 filters, each of size (2, 2). The stride is 1, and the padding strategy is ‘same’. After the convolutional layer, a ReLU activation function introduces non-linearity, and a dropout layer is applied with a rate of 0.25.\nMax Pooling Layer 4: The fourth max pooling layer uses a pooling window of size (4, 4).\nConvolutional Layer 5: The fifth convolutional layer has 128 filters, each of size (2, 2). The stride is 1, and the padding strategy is ‘same’. After the convolutional layer, a ReLU activation function introduces non-linearity, and a dropout layer is applied with a rate of 0.25.\nMax Pooling Layer 5: The fifth max pooling layer uses a pooling window of size (4, 4).\nFlatten Layer: The flatten layer reshapes the 3D output of the previous layer into a 1D vector.\nDense Layer 1: The first dense layer has 256 neurons. After the dense layer, a ReLU activation function introduces non-linearity, and a dropout layer is applied with a rate of 0.5. This layer also has L2 regularization with a factor of 0.001.\nDense Layer 2: The second dense layer has 128 neurons. After the dense layer, a ReLU activation function introduces non-linearity, and a dropout layer is applied with a rate of 0.5. This layer also has L2 regularization with a factor of 0.001.\nDense Layer 3: The third dense layer has 64 neurons. After the dense layer, a ReLU activation function introduces non-linearity, and a dropout layer is applied with a rate of 0.5. This layer also has L2 regularization with a factor of 0.001.\nDense Layer 4: The fourth dense layer has 32 neurons. After the dense layer, a ReLU activation function introduces non-linearity, and a dropout layer is applied with a rate of 0.5. This layer also has L2 regularization with a factor of 0.001.\nOutput Layer: The output layer is a dense layer with 6 neurons. There is no dropout after this layer. The activation function is linear, but when calculating the loss during training, a softmax activation function is applied to the logits because from_logits=True is used in the loss function. This means that the model’s output is expected to be logits and the softmax activation function is applied to convert these logits to probabilities when calculating the loss. This layer also has L2 regularization with a factor of 0.001.\n\nThis architecture can be represented as:\n\n[CONV2D -&gt; RELU -&gt; DROPOUT -&gt; MAXPOOL] * 5 -&gt; FLATTEN -&gt; [DENSE (ReLU activation) -&gt; DROPOUT] * 4 -&gt; DENSE (Softmax activation from logits)\n\n\nconv_params = [\n    {'filters': 8, 'kernel_size': (4, 4), 'strides': 1, 'padding':'same', 'pool_size': (4, 4), 'dropout': 0.25},\n    {'filters': 16, 'kernel_size': (4, 4), 'strides': 1, 'padding':'same', 'pool_size': (8, 8), 'dropout': 0.25},\n    {'filters': 32, 'kernel_size': (2, 2), 'strides': 1, 'padding':'same', 'pool_size': (4, 4), 'dropout': 0.25},\n    {'filters': 64, 'kernel_size': (2, 2), 'strides': 1, 'padding':'same', 'pool_size': (4, 4), 'dropout': 0.25},\n    {'filters': 128, 'kernel_size': (2, 2), 'strides': 1, 'padding':'same', 'pool_size': (4, 4), 'dropout': 0.25}\n]\n\ndense_params = [\n    {'units': 256, 'dropout': 0.5, 'l2_reg': 0.001},\n    {'units': 128, 'dropout': 0.5, 'l2_reg': 0.001},\n    {'units': 64, 'dropout': 0.5, 'l2_reg': 0.001},\n    {'units': 32, 'dropout': 0.5, 'l2_reg': 0.001},\n    {'units': 6, 'dropout': 0, 'l2_reg': 0.001},  # No dropout for the output layer\n]\n\nmodel4 = CNN_Reg(input_shape=(64, 64, 3), conv_params=conv_params, dense_params=dense_params)\n\n\nWith a regularized model we will train for far longer, epochs=100.\nThe hope is that the increased training time will give the model more chance to learn the intricacies of the data distribution while the regularization will stop the model from memorizing the distribution of the training data (overfitting).\n\n\n%%time\n\nmodel4.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,  ## DEFAULT HYPERPARAMS ## \n                                                  beta_1=0.9,\n                                                  beta_2=0.999,\n                                                  epsilon=1e-07,),\n               loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n               metrics=['accuracy'])\n\nmodel4_history = model4.fit(train_dataset, epochs=100, validation_data=val_dataset)\n\nprint('\\nTraining Complete.\\n')\n\nEpoch 1/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 27s 2s/step - accuracy: 0.1620 - loss: 6.2356 - val_accuracy: 0.1358 - val_loss: 2.9833\nEpoch 2/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.2168 - loss: 2.9632 - val_accuracy: 0.2654 - val_loss: 2.7816\nEpoch 3/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.3516 - loss: 2.6073 - val_accuracy: 0.3580 - val_loss: 2.4996\nEpoch 4/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.4670 - loss: 2.1334 - val_accuracy: 0.4938 - val_loss: 1.9900\nEpoch 5/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.5698 - loss: 1.8123 - val_accuracy: 0.7222 - val_loss: 1.5858\nEpoch 6/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.7270 - loss: 1.4288 - val_accuracy: 0.7284 - val_loss: 1.4681\nEpoch 7/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.7642 - loss: 1.2207 - val_accuracy: 0.6049 - val_loss: 1.6744\nEpoch 8/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.7288 - loss: 1.2981 - val_accuracy: 0.8395 - val_loss: 1.1413\nEpoch 9/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.8214 - loss: 1.0202 - val_accuracy: 0.7469 - val_loss: 1.3735\nEpoch 10/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.8570 - loss: 0.9302 - val_accuracy: 0.7716 - val_loss: 1.0934\nEpoch 11/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.8699 - loss: 0.7897 - val_accuracy: 0.7407 - val_loss: 1.3671\nEpoch 12/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.8535 - loss: 0.8504 - val_accuracy: 0.7593 - val_loss: 1.1791\nEpoch 13/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.8650 - loss: 0.7723 - val_accuracy: 0.8395 - val_loss: 0.8838\nEpoch 14/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9140 - loss: 0.6719 - val_accuracy: 0.8765 - val_loss: 0.8008\nEpoch 15/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9479 - loss: 0.6024 - val_accuracy: 0.7222 - val_loss: 1.1934\nEpoch 16/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.8504 - loss: 0.7278 - val_accuracy: 0.8765 - val_loss: 0.7511\nEpoch 17/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9326 - loss: 0.5967 - val_accuracy: 0.8148 - val_loss: 0.7815\nEpoch 18/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9287 - loss: 0.5644 - val_accuracy: 0.9012 - val_loss: 0.6538\nEpoch 19/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9085 - loss: 0.6052 - val_accuracy: 0.9136 - val_loss: 0.6378\nEpoch 20/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9701 - loss: 0.4528 - val_accuracy: 0.9321 - val_loss: 0.5749\nEpoch 21/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9734 - loss: 0.4292 - val_accuracy: 0.9012 - val_loss: 0.6929\nEpoch 22/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9564 - loss: 0.4333 - val_accuracy: 0.9074 - val_loss: 0.5624\nEpoch 23/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9379 - loss: 0.5011 - val_accuracy: 0.8827 - val_loss: 0.7382\nEpoch 24/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9576 - loss: 0.4716 - val_accuracy: 0.9321 - val_loss: 0.5930\nEpoch 25/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9772 - loss: 0.4066 - val_accuracy: 0.9383 - val_loss: 0.5381\nEpoch 26/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9643 - loss: 0.3814 - val_accuracy: 0.9074 - val_loss: 0.5819\nEpoch 27/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9487 - loss: 0.4560 - val_accuracy: 0.8333 - val_loss: 0.9663\nEpoch 28/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.6337 - loss: 1.3982 - val_accuracy: 0.5679 - val_loss: 1.6912\nEpoch 29/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.6333 - loss: 1.4273 - val_accuracy: 0.4630 - val_loss: 1.8655\nEpoch 30/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.5850 - loss: 1.6849 - val_accuracy: 0.6852 - val_loss: 1.4429\nEpoch 31/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.7212 - loss: 1.2814 - val_accuracy: 0.7778 - val_loss: 1.1745\nEpoch 32/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.8276 - loss: 0.9652 - val_accuracy: 0.8395 - val_loss: 1.0079\nEpoch 33/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.8792 - loss: 0.7353 - val_accuracy: 0.8457 - val_loss: 0.9016\nEpoch 34/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9199 - loss: 0.6312 - val_accuracy: 0.8827 - val_loss: 0.8751\nEpoch 35/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9174 - loss: 0.5758 - val_accuracy: 0.8827 - val_loss: 0.9350\nEpoch 36/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.8909 - loss: 0.6174 - val_accuracy: 0.8519 - val_loss: 0.8674\nEpoch 37/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9246 - loss: 0.5637 - val_accuracy: 0.7778 - val_loss: 0.9864\nEpoch 38/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9070 - loss: 0.5787 - val_accuracy: 0.8086 - val_loss: 1.0043\nEpoch 39/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9190 - loss: 0.5501 - val_accuracy: 0.8272 - val_loss: 0.9312\nEpoch 40/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9457 - loss: 0.4871 - val_accuracy: 0.8951 - val_loss: 0.7946\nEpoch 41/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9617 - loss: 0.4359 - val_accuracy: 0.8580 - val_loss: 0.8824\nEpoch 42/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9674 - loss: 0.4178 - val_accuracy: 0.8395 - val_loss: 1.0029\nEpoch 43/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9426 - loss: 0.4555 - val_accuracy: 0.9012 - val_loss: 0.7421\nEpoch 44/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9667 - loss: 0.3693 - val_accuracy: 0.8457 - val_loss: 0.9235\nEpoch 45/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9493 - loss: 0.4499 - val_accuracy: 0.8765 - val_loss: 0.7632\nEpoch 46/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9771 - loss: 0.3512 - val_accuracy: 0.8889 - val_loss: 0.8511\nEpoch 47/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9709 - loss: 0.3821 - val_accuracy: 0.8704 - val_loss: 0.8721\nEpoch 48/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9677 - loss: 0.3612 - val_accuracy: 0.8642 - val_loss: 0.9992\nEpoch 49/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9719 - loss: 0.3600 - val_accuracy: 0.8827 - val_loss: 0.8648\nEpoch 50/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9799 - loss: 0.3303 - val_accuracy: 0.8951 - val_loss: 0.7907\nEpoch 51/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9564 - loss: 0.3879 - val_accuracy: 0.8148 - val_loss: 1.0517\nEpoch 52/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9041 - loss: 0.5492 - val_accuracy: 0.7963 - val_loss: 0.9532\nEpoch 53/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9091 - loss: 0.5380 - val_accuracy: 0.8642 - val_loss: 0.8355\nEpoch 54/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9588 - loss: 0.4090 - val_accuracy: 0.8827 - val_loss: 0.7720\nEpoch 55/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9659 - loss: 0.3630 - val_accuracy: 0.8889 - val_loss: 0.8303\nEpoch 56/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9751 - loss: 0.3360 - val_accuracy: 0.8951 - val_loss: 0.7807\nEpoch 57/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9888 - loss: 0.3062 - val_accuracy: 0.8889 - val_loss: 0.8557\nEpoch 58/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9904 - loss: 0.2811 - val_accuracy: 0.9074 - val_loss: 0.8780\nEpoch 59/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9956 - loss: 0.2600 - val_accuracy: 0.9383 - val_loss: 0.6578\nEpoch 60/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9989 - loss: 0.2386 - val_accuracy: 0.9259 - val_loss: 0.6584\nEpoch 61/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 1.0000 - loss: 0.2242 - val_accuracy: 0.9383 - val_loss: 0.6327\nEpoch 62/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9968 - loss: 0.2223 - val_accuracy: 0.9198 - val_loss: 0.6559\nEpoch 63/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9974 - loss: 0.2152 - val_accuracy: 0.8827 - val_loss: 0.7064\nEpoch 64/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9996 - loss: 0.2086 - val_accuracy: 0.9198 - val_loss: 0.5703\nEpoch 65/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9979 - loss: 0.1999 - val_accuracy: 0.9012 - val_loss: 0.5444\nEpoch 66/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9993 - loss: 0.1922 - val_accuracy: 0.9198 - val_loss: 0.5736\nEpoch 67/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9991 - loss: 0.1885 - val_accuracy: 0.9012 - val_loss: 0.6271\nEpoch 68/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9879 - loss: 0.2147 - val_accuracy: 0.9012 - val_loss: 0.7182\nEpoch 69/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9819 - loss: 0.2304 - val_accuracy: 0.9136 - val_loss: 0.6253\nEpoch 70/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9541 - loss: 0.3188 - val_accuracy: 0.8457 - val_loss: 0.8789\nEpoch 71/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9420 - loss: 0.3629 - val_accuracy: 0.8210 - val_loss: 0.9289\nEpoch 72/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.8632 - loss: 0.6289 - val_accuracy: 0.7901 - val_loss: 1.0566\nEpoch 73/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9324 - loss: 0.4914 - val_accuracy: 0.9136 - val_loss: 0.5650\nEpoch 74/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9736 - loss: 0.3409 - val_accuracy: 0.9074 - val_loss: 0.6690\nEpoch 75/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9793 - loss: 0.3261 - val_accuracy: 0.8580 - val_loss: 0.7094\nEpoch 76/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9793 - loss: 0.3011 - val_accuracy: 0.8951 - val_loss: 0.7572\nEpoch 77/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9957 - loss: 0.2720 - val_accuracy: 0.8827 - val_loss: 0.8553\nEpoch 78/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9896 - loss: 0.2698 - val_accuracy: 0.9136 - val_loss: 0.6873\nEpoch 79/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9723 - loss: 0.2737 - val_accuracy: 0.9383 - val_loss: 0.5609\nEpoch 80/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9912 - loss: 0.2419 - val_accuracy: 0.8765 - val_loss: 0.8657\nEpoch 81/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9685 - loss: 0.2911 - val_accuracy: 0.9012 - val_loss: 0.7114\nEpoch 82/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9603 - loss: 0.3087 - val_accuracy: 0.8642 - val_loss: 0.9624\nEpoch 83/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9635 - loss: 0.3166 - val_accuracy: 0.9074 - val_loss: 0.6693\nEpoch 84/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9850 - loss: 0.2649 - val_accuracy: 0.8951 - val_loss: 0.6677\nEpoch 85/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9735 - loss: 0.2888 - val_accuracy: 0.9012 - val_loss: 0.6874\nEpoch 86/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9725 - loss: 0.2862 - val_accuracy: 0.8827 - val_loss: 0.8227\nEpoch 87/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9795 - loss: 0.2853 - val_accuracy: 0.9136 - val_loss: 0.7170\nEpoch 88/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9894 - loss: 0.2422 - val_accuracy: 0.9259 - val_loss: 0.5987\nEpoch 89/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9946 - loss: 0.2328 - val_accuracy: 0.8889 - val_loss: 0.8390\nEpoch 90/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9794 - loss: 0.2786 - val_accuracy: 0.9321 - val_loss: 0.5397\nEpoch 91/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9923 - loss: 0.2222 - val_accuracy: 0.9074 - val_loss: 0.6283\nEpoch 92/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 0.9956 - loss: 0.2238 - val_accuracy: 0.9321 - val_loss: 0.5519\nEpoch 93/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 1.0000 - loss: 0.1990 - val_accuracy: 0.9506 - val_loss: 0.6280\nEpoch 94/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 1.0000 - loss: 0.1881 - val_accuracy: 0.9383 - val_loss: 0.5802\nEpoch 95/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 1.0000 - loss: 0.1786 - val_accuracy: 0.9568 - val_loss: 0.5699\nEpoch 96/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 1.0000 - loss: 0.1711 - val_accuracy: 0.9568 - val_loss: 0.5285\nEpoch 97/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 1.0000 - loss: 0.1645 - val_accuracy: 0.9568 - val_loss: 0.5148\nEpoch 98/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 1.0000 - loss: 0.1589 - val_accuracy: 0.9630 - val_loss: 0.5089\nEpoch 99/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 1.0000 - loss: 0.1540 - val_accuracy: 0.9630 - val_loss: 0.5027\nEpoch 100/100\n15/15 ━━━━━━━━━━━━━━━━━━━━ 24s 2s/step - accuracy: 1.0000 - loss: 0.1497 - val_accuracy: 0.9568 - val_loss: 0.4960\n\nTraining Complete.\n\nCPU times: total: 7h 35min 32s\nWall time: 39min 57s\n\n\n\nplot_model_history(model4_history, epochs=100)\n\n\n\n\n\ntest_loss, test_accuracy = model4.evaluate(test_dataset)\n\nprint(f\"\\nTest accuracy: {test_accuracy*100:.2f}%\")\n\n2/2 ━━━━━━━━━━━━━━━━━━━━ 1s 305ms/step - accuracy: 0.9722 - loss: 0.3679\n\nTest accuracy: 95.83%\n\n\n\npred = np.argmax(model4.predict(X_test), axis=1)\nprint_mislabeled_images(classes, X_test, Y_test_orig, pred, number=5)\n\n4/4 ━━━━━━━━━━━━━━━━━━━━ 1s 120ms/step"
  },
  {
    "objectID": "posts/yt1/index.html",
    "href": "posts/yt1/index.html",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\ncolor_pal = sns.color_palette('magma')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit\n\nimport xgboost as xgb\nprint(\"Python version:\")\n!python --version\n\nPython version:\nPython 3.11.4"
  },
  {
    "objectID": "posts/yt1/index.html#training-using-cross-validation",
    "href": "posts/yt1/index.html#training-using-cross-validation",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "Training using Cross-Validation",
    "text": "Training using Cross-Validation\nThe following code trains an XGBRegressor on each of the above 5 folds, saving the score (RMSE) in a list scores.\n\nfold = 0\npreds = []\nscores = []\nfor train_idx, val_idx in tss.split(df):\n    train = df.iloc[train_idx]\n    test = df.iloc[val_idx]\n\n    train = create_features(train)\n    test = create_features(test)\n\n    FEATURES = ['dayofyear', 'hour', 'dayofweek', 'quarter', 'month','year', 'lag1', 'lag2', 'lag3']\n    TARGET = 'PJME_MW'\n\n    X_train = train[FEATURES]\n    y_train = train[TARGET]\n\n    X_test = test[FEATURES]\n    y_test = test[TARGET]\n\n    reg = xgb.XGBRegressor(base_score=0.5, \n                           booster='gbtree',    \n                           n_estimators=1000,\n                           early_stopping_rounds=50,\n                           objective='reg:squarederror',\n                           max_depth=3,\n                           learning_rate=0.01)\n    \n    reg.fit(X_train, \n            y_train,\n            eval_set=[(X_train, y_train), (X_test, y_test)],\n            verbose=100)\n\n    y_pred = reg.predict(X_test)\n    preds.append(y_pred)\n    score = np.sqrt(mean_squared_error(y_test, y_pred))\n    scores.append(score)\n\n[0] validation_0-rmse:32732.50147   validation_1-rmse:31956.66494\n[100]   validation_0-rmse:12532.10915   validation_1-rmse:11906.70125\n[200]   validation_0-rmse:5739.78666    validation_1-rmse:5352.86754\n[300]   validation_0-rmse:3868.29390    validation_1-rmse:3891.32148\n[400]   validation_0-rmse:3428.85875    validation_1-rmse:3753.95996\n[456]   validation_0-rmse:3349.18480    validation_1-rmse:3761.64093\n[0] validation_0-rmse:32672.16154   validation_1-rmse:32138.88680\n[100]   validation_0-rmse:12513.25338   validation_1-rmse:12222.97626\n[200]   validation_0-rmse:5755.14393    validation_1-rmse:5649.54800\n[300]   validation_0-rmse:3909.18294    validation_1-rmse:3930.98277\n[400]   validation_0-rmse:3477.91771    validation_1-rmse:3603.77859\n[500]   validation_0-rmse:3356.63775    validation_1-rmse:3534.18452\n[600]   validation_0-rmse:3299.24378    validation_1-rmse:3495.69013\n[700]   validation_0-rmse:3258.86466    validation_1-rmse:3470.24780\n[800]   validation_0-rmse:3222.68998    validation_1-rmse:3446.36557\n[900]   validation_0-rmse:3195.04645    validation_1-rmse:3438.00845\n[999]   validation_0-rmse:3169.68251    validation_1-rmse:3434.35289\n[0] validation_0-rmse:32631.19070   validation_1-rmse:31073.24659\n[100]   validation_0-rmse:12498.56469   validation_1-rmse:11133.47932\n[200]   validation_0-rmse:5749.48268    validation_1-rmse:4812.56835\n[300]   validation_0-rmse:3915.69493    validation_1-rmse:3552.97165\n[400]   validation_0-rmse:3493.17887    validation_1-rmse:3492.55244\n[415]   validation_0-rmse:3467.76622    validation_1-rmse:3500.17489\n[0] validation_0-rmse:32528.44140   validation_1-rmse:31475.37803\n[100]   validation_0-rmse:12461.95683   validation_1-rmse:12016.24890\n[200]   validation_0-rmse:5736.08201    validation_1-rmse:5800.02075\n[300]   validation_0-rmse:3913.36576    validation_1-rmse:4388.02984\n[400]   validation_0-rmse:3495.35688    validation_1-rmse:4177.05330\n[500]   validation_0-rmse:3380.70922    validation_1-rmse:4123.43863\n[600]   validation_0-rmse:3321.42955    validation_1-rmse:4110.84393\n[700]   validation_0-rmse:3280.93068    validation_1-rmse:4096.40531\n[800]   validation_0-rmse:3249.14336    validation_1-rmse:4095.30547\n[809]   validation_0-rmse:3246.14826    validation_1-rmse:4094.38398\n[0] validation_0-rmse:32462.05402   validation_1-rmse:31463.86930\n[100]   validation_0-rmse:12445.22753   validation_1-rmse:11954.79556\n[200]   validation_0-rmse:5750.85887    validation_1-rmse:5616.16472\n[300]   validation_0-rmse:3949.92308    validation_1-rmse:4154.55799\n[400]   validation_0-rmse:3538.33857    validation_1-rmse:3996.70155\n[448]   validation_0-rmse:3471.50174    validation_1-rmse:4005.60241\n\n\n\nscores\n\n[3753.2775219986684,\n 3434.3528874818867,\n 3475.9138463312997,\n 4093.3608331481823,\n 3996.298054855067]\n\n\n\nprint(f'Mean score across folds: {np.mean(scores):0.4f}')\nprint(f'Fold scores:\\n{scores}')\n\nMean score across folds: 3750.6406\nFold scores:\n[3753.2775219986684, 3434.3528874818867, 3475.9138463312997, 4093.3608331481823, 3996.298054855067]"
  },
  {
    "objectID": "posts/yt1/index.html#fitting-an-xgbregressor",
    "href": "posts/yt1/index.html#fitting-an-xgbregressor",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "Fitting an XGBRegressor",
    "text": "Fitting an XGBRegressor\n\nmodel = xgb.XGBRegressor(base_score=0.5, \n                         booster='gbtree',    \n                         n_estimators=1000,\n                         early_stopping_rounds=50,\n                         objective='reg:squarederror',\n                         max_depth=3,\n                         learning_rate=0.01)\n\n\nmodel.fit(X_train, \n          y_train, \n          eval_set=[(X_train,y_train),(X_test,y_test)],\n          verbose=100)\n\n[0] validation_0-rmse:32462.05402   validation_1-rmse:31463.86930\n[100]   validation_0-rmse:12445.22753   validation_1-rmse:11954.79556\n[200]   validation_0-rmse:5750.85887    validation_1-rmse:5616.16472\n[300]   validation_0-rmse:3949.92308    validation_1-rmse:4154.55799\n[400]   validation_0-rmse:3538.33857    validation_1-rmse:3996.70155\n[447]   validation_0-rmse:3472.46884    validation_1-rmse:4004.66554\n\n\nXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=50,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=50,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)"
  },
  {
    "objectID": "posts/yt1/index.html#feature-importances",
    "href": "posts/yt1/index.html#feature-importances",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "Feature Importances",
    "text": "Feature Importances\n\nfi = pd.DataFrame(data=model.feature_importances_,\n                  index=model.feature_names_in_,\n                  columns=['Importance'])\n\n\nfi.sort_values('Importance').plot(kind='barh',title='Feature Importance',color='blue',legend=False)\n\n&lt;Axes: title={'center': 'Feature Importance'}&gt;"
  },
  {
    "objectID": "posts/yt1/index.html#predictions",
    "href": "posts/yt1/index.html#predictions",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "Predictions",
    "text": "Predictions\n\ntrain\n\n\n\n\n\n\n\n\nPJME_MW\nhour\ndayofweek\nquarter\nmonth\nyear\ndayofyear\nlag1\nlag2\nlag3\n\n\nDatetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2002-01-01 01:00:00\n30393.0\n1\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 02:00:00\n29265.0\n2\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 03:00:00\n28357.0\n3\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 04:00:00\n27899.0\n4\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 05:00:00\n28057.0\n5\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2017-08-01 20:00:00\n45090.0\n20\n1\n3\n8\n2017\n213\n41056.0\n46225.0\n43934.0\n\n\n2017-08-01 21:00:00\n43843.0\n21\n1\n3\n8\n2017\n213\n40151.0\n44510.0\n42848.0\n\n\n2017-08-01 22:00:00\n41850.0\n22\n1\n3\n8\n2017\n213\n38662.0\n42467.0\n40861.0\n\n\n2017-08-01 23:00:00\n38473.0\n23\n1\n3\n8\n2017\n213\n35583.0\n38646.0\n37361.0\n\n\n2017-08-02 00:00:00\n35126.0\n0\n2\n3\n8\n2017\n214\n32181.0\n34829.0\n33743.0\n\n\n\n\n136567 rows × 10 columns\n\n\n\n\ntest\n\n\n\n\n\n\n\n\nPJME_MW\nhour\ndayofweek\nquarter\nmonth\nyear\ndayofyear\nlag1\nlag2\nlag3\n\n\nDatetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-08-03 01:00:00\n29189.0\n1\n3\n3\n8\n2017\n215\n28809.0\n29952.0\n28465.0\n\n\n2017-08-03 02:00:00\n27584.0\n2\n3\n3\n8\n2017\n215\n27039.0\n27934.0\n26712.0\n\n\n2017-08-03 03:00:00\n26544.0\n3\n3\n3\n8\n2017\n215\n25881.0\n26659.0\n25547.0\n\n\n2017-08-03 04:00:00\n26012.0\n4\n3\n3\n8\n2017\n215\n25300.0\n25846.0\n24825.0\n\n\n2017-08-03 05:00:00\n26187.0\n5\n3\n3\n8\n2017\n215\n25412.0\n25898.0\n24927.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2018-08-02 20:00:00\n44057.0\n20\n3\n3\n8\n2018\n214\n42256.0\n41485.0\n38804.0\n\n\n2018-08-02 21:00:00\n43256.0\n21\n3\n3\n8\n2018\n214\n41210.0\n40249.0\n38748.0\n\n\n2018-08-02 22:00:00\n41552.0\n22\n3\n3\n8\n2018\n214\n39525.0\n38698.0\n37330.0\n\n\n2018-08-02 23:00:00\n38500.0\n23\n3\n3\n8\n2018\n214\n36490.0\n35406.0\n34552.0\n\n\n2018-08-03 00:00:00\n35486.0\n0\n4\n3\n8\n2018\n215\n33539.0\n32094.0\n31695.0\n\n\n\n\n8760 rows × 10 columns\n\n\n\n\ntest['prediction'] = model.predict(X_test)\ndf = df.merge(test[['prediction']],how='left',left_index=True,right_index=True)\n\n\ntest['prediction']\n\nDatetime\n2017-08-03 01:00:00    27884.035156\n2017-08-03 02:00:00    27147.710938\n2017-08-03 03:00:00    26344.050781\n2017-08-03 04:00:00    25737.550781\n2017-08-03 05:00:00    25737.550781\n                           ...     \n2018-08-02 20:00:00    40988.347656\n2018-08-02 21:00:00    40045.542969\n2018-08-02 22:00:00    38405.371094\n2018-08-02 23:00:00    36211.242188\n2018-08-03 00:00:00    30370.074219\nName: prediction, Length: 8760, dtype: float32\n\n\n\ntest['prediction'].describe()\n\ncount     8760.000000\nmean     30520.908203\nstd       5277.272949\nmin      21005.292969\n25%      26730.913086\n50%      30010.917969\n75%      33361.808594\nmax      46170.230469\nName: prediction, dtype: float64\n\n\nWe can visualise the predicted energy consumption for a particular week:\n\nstart_date = '04-01-2018'\nend_date = '04-08-2018'\nfiltered_df = df.loc[(df.index &gt; start_date) & (df.index &lt; end_date)]\n\nplt.figure(figsize=(15, 5))\nax = sns.lineplot(data=filtered_df, x=filtered_df.index, y='PJME_MW', label='Truth')\nsns.scatterplot(data=filtered_df, x=filtered_df.index, y='prediction', label='Prediction', marker='.',color='orange')\nplt.title(f'Predicted vs. Actual Energy Consumption: {start_date} to {end_date}')\n\nText(0.5, 1.0, 'Predicted vs. Actual Energy Consumption: 04-01-2018 to 04-08-2018')"
  },
  {
    "objectID": "posts/yt3/index.html",
    "href": "posts/yt3/index.html",
    "title": "Variational AutoEncoders in PyTorch",
    "section": "",
    "text": "import torch\nfrom torch import nn\n\nimport torchvision.datasets as datasets\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm   # Progress Bar\nfrom utils import versions\nversions()\n\n+-------------+------------+\n|  Component  |  Version   |\n+-------------+------------+\n|    Python   |   3.12.2   |\n+-------------+------------+\n|   pytorch   | 2.2.2+cpu  |\n+-------------+------------+\n| torchvision | 0.17.2+cpu |\n+-------------+------------+"
  },
  {
    "objectID": "posts/yt3/index.html#initialising-single-layer-vae-model",
    "href": "posts/yt3/index.html#initialising-single-layer-vae-model",
    "title": "Variational AutoEncoders in PyTorch",
    "section": "Initialising Single-Layer VAE model",
    "text": "Initialising Single-Layer VAE model\n\n# Initialise Model\n# Create an instance of the VariationalAutoEncoder class with the specified input, hidden, and latent dimensions.\n# The model is moved to the specified device (either CPU or GPU) for computation.\nmodel = VariationalAutoEncoder(INPUT_DIM, H_DIM, Z_DIM).to(DEVICE)\n\n# Optimizer\n# The Adam optimizer is used for updating the parameters of the model.\n# The learning rate (LR) is a hyperparameter that controls how much the weights of the network will change in response to the gradient.\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\n# Loss Function\n# The Binary Cross Entropy (BCE) loss function is used as the reconstruction loss for the VAE.\n# This loss function measures the error between the reconstructed image and the original image.\n# The 'sum' reduction mode is used, which means the losses are summed over all elements in the output.\nloss_fn = nn.BCELoss(reduction=\"sum\")"
  },
  {
    "objectID": "posts/yt3/index.html#the-dataset",
    "href": "posts/yt3/index.html#the-dataset",
    "title": "Variational AutoEncoders in PyTorch",
    "section": "The Dataset",
    "text": "The Dataset\n\nMNIST\nThe MNIST dataset is a large database of handwritten digits that is commonly used for training various image processing systems. It contains 60,000 training images and 10,000 testing images. Each image is a grayscale 28x28 pixel representation of a digit.\n\nFor more details see the MNIST wikipedia page:\n\nhttps://en.wikipedia.org/wiki/MNIST_database\n\n\n\nData Loading\nThe dataset is loaded from a local directory named dataset/. If the dataset is not available in this directory, it will be downloaded. The images from the dataset are transformed into tensors using the transforms.ToTensor() function from PyTorch. This converts the images from a PIL Image or numpy ndarray into tensor format.\nAfter loading the dataset, we use PyTorch’s DataLoader to shuffle and batch the data. Shuffling the data ensures that the model’s performance is not affected by the order of the data in the dataset. The batch_size parameter for the DataLoader is set to BATCH_SIZE.\n\n# Dataset loading\n# Load the MNIST dataset. If it's not available in the local directory, download it.\n# The images are transformed to tensors using transforms.ToTensor().\ndataset = datasets.MNIST(root=\"dataset/\",\n                         train=True,\n                         transform=transforms.ToTensor(),\n                         download=True)\n\n# DataLoader is used to shuffle and batch the data.\n# Shuffling helps to make sure that the model is not affected by the order of the data in the dataset.\ntrain_loader = DataLoader(dataset=dataset,\n                          batch_size=BATCH_SIZE,\n                          shuffle=True)\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset/MNIST\\raw\\train-images-idx3-ubyte.gz\nExtracting dataset/MNIST\\raw\\train-images-idx3-ubyte.gz to dataset/MNIST\\raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST\\raw\\train-labels-idx1-ubyte.gz\nExtracting dataset/MNIST\\raw\\train-labels-idx1-ubyte.gz to dataset/MNIST\\raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST\\raw\\t10k-images-idx3-ubyte.gz\nExtracting dataset/MNIST\\raw\\t10k-images-idx3-ubyte.gz to dataset/MNIST\\raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\nExtracting dataset/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to dataset/MNIST\\raw\n\n\n\n100%|██████████| 9912422/9912422 [00:01&lt;00:00, 5711216.86it/s]\n100%|██████████| 28881/28881 [00:00&lt;00:00, 28869326.46it/s]\n100%|██████████| 1648877/1648877 [00:00&lt;00:00, 6409941.87it/s]\n100%|██████████| 4542/4542 [00:00&lt;?, ?it/s]"
  },
  {
    "objectID": "posts/yt3/index.html#training",
    "href": "posts/yt3/index.html#training",
    "title": "Variational AutoEncoders in PyTorch",
    "section": "Training",
    "text": "Training\n\nTraining Function train_vae\nThe train_vae function is implemented and used to train the VAE model.\n\nTraining Loop:\n\nThe model is set to training mode at the start.\nFor each epoch, the function initializes the total loss for the epoch (epoch_loss).\nIt then loops over the training data. For each batch, it performs a forward pass, computes the loss, performs backpropagation, and updates the model parameters. \n\nLoss Function:\n\nThe loss function consists of two parts: the reconstruction loss and the Kullback-Leibler (KL) divergence.\nThe reconstruction loss (reconstruction_loss) measures how well the model can reconstruct the input.\nThe KL divergence (kl_div) measures how much the latent distribution deviates from a standard normal distribution. It is computed as -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()).\n\n\nFor a precise definition of the KL divergence, see my previous blog post:\n\n“The Boltzmann Equation - 3. Information Theory”\n\n\n\nCode\ndef train_vae(model, train_loader, optimizer, loss_fn, num_epochs, device):\n    \"\"\"\n    Train a Variational AutoEncoder (VAE) model.\n\n    Args:\n    model (nn.Module): The VAE model.\n    train_loader (DataLoader): The DataLoader for the training data.\n    optimizer (Optimizer): The optimizer for the model.\n    loss_fn (Loss): The loss function to use.\n    num_epochs (int): The number of epochs to train for.\n    device (torch.device): The device to train the model on.\n    \"\"\"\n    # Ensure model is in training mode\n    model.train()\n\n    # Start Training\n    for epoch in range(num_epochs):\n        # Initialize the loss for this epoch\n        epoch_loss = 0.0\n\n        # tqdm is used to show a progress bar for the training loop\n        loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n        for i, (x, _) in loop:\n            # Forward Pass:\n            # Move the input to the device and reshape it\n            x = x.to(device).view(x.shape[0], -1)\n            # Pass the input through the model to get the reconstructed output and the parameters of the latent distribution\n            x_reconstructed, mu, logvar = model(x)\n\n            # Compute Loss:\n            # Compute the reconstruction loss (how well the model can reconstruct the input)\n            reconstruction_loss = loss_fn(x_reconstructed, x)\n            # Compute the KL divergence loss (how much the latent distribution deviates from a standard normal distribution)\n            kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n            # Backpropagation:\n            # Compute the total loss, perform backpropagation to compute gradients, and update the model parameters\n            loss = reconstruction_loss + kl_div \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Add the loss for this batch to the total loss for this epoch\n            epoch_loss += loss.item()\n\n            # Update the progress bar with the current loss\n            loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n            loop.set_postfix(loss=epoch_loss / (i+1))\n\n    print(\"Training completed.\")\n\n\n\ntrain_vae(model, train_loader, optimizer, loss_fn, NUM_EPOCHS, DEVICE)\n\n  0%|          | 0/1875 [00:00&lt;?, ?it/s]Epoch [1/10]: 100%|██████████| 1875/1875 [00:21&lt;00:00, 89.18it/s, loss=5.79e+3]\nEpoch [2/10]: 100%|██████████| 1875/1875 [00:20&lt;00:00, 89.30it/s, loss=4.45e+3]\nEpoch [3/10]: 100%|██████████| 1875/1875 [00:20&lt;00:00, 91.68it/s, loss=4.1e+3] \nEpoch [4/10]: 100%|██████████| 1875/1875 [00:20&lt;00:00, 89.58it/s, loss=3.89e+3]\nEpoch [5/10]: 100%|██████████| 1875/1875 [00:20&lt;00:00, 90.01it/s, loss=3.77e+3]\nEpoch [6/10]: 100%|██████████| 1875/1875 [00:21&lt;00:00, 87.77it/s, loss=3.68e+3]\nEpoch [7/10]: 100%|██████████| 1875/1875 [00:21&lt;00:00, 88.52it/s, loss=3.62e+3]\nEpoch [8/10]: 100%|██████████| 1875/1875 [00:20&lt;00:00, 89.39it/s, loss=3.58e+3]\nEpoch [9/10]: 100%|██████████| 1875/1875 [00:21&lt;00:00, 87.81it/s, loss=3.54e+3]\nEpoch [10/10]: 100%|██████████| 1875/1875 [00:21&lt;00:00, 86.45it/s, loss=3.52e+3]\n\n\nTraining completed."
  },
  {
    "objectID": "posts/yt3/index.html#visualising-results",
    "href": "posts/yt3/index.html#visualising-results",
    "title": "Variational AutoEncoders in PyTorch",
    "section": "Visualising Results",
    "text": "Visualising Results\n\n\nCode\ndef inference(model, digit, num_examples=1, plot=False):\n    \"\"\"\n    Generate `num_examples` of the specified `digit` using the provided VAE `model`.\n\n    Args:\n    model (nn.Module): The Variational AutoEncoder model.\n    digit (int): The digit to generate (0-9).\n    num_examples (int, optional): The number of examples to generate. Defaults to 1.\n    plot (bool, optional): Whether to plot the generated images. Defaults to False.\n\n    Returns:\n    list of Tensor: The generated images.\n    \"\"\"\n    # Ensure model is in evaluation mode\n    model.eval()\n\n    # Extract an example of the specified digit from the dataset\n    image = None\n    for x, y in train_loader.dataset:\n        if y == digit:\n            image = x\n            break\n\n    # Get the mean and logvar for the specified digit\n    mu, logvar = model.encode(image.view(1, 784).to(DEVICE))\n\n    # Create a list to store the output images\n    out_images = []\n\n    for example in range(num_examples):\n        # Create a latent vector\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std).to(DEVICE)\n        z = mu + eps * std\n\n        # Decode the latent vector\n        with torch.no_grad():\n            out = model.decode(z)\n\n        # Reshape the output and move it to CPU\n        out = out.view(-1, 1, 28, 28).cpu()\n\n        # Add the output image to the list\n        out_images.append(out)\n\n        # Plot the image if plot is True\n        if plot:\n            plt.imshow(out[0].detach().numpy().reshape(28, 28), cmap='gray')\n            plt.axis('off')\n            plt.show()\n            return None\n\n    return out_images\n\n\n\ninference(model, digit=1, plot=True)\n\n\n\n\n\ninference(model, digit=6, plot=True)\n\n\n\n\n\n\nCode\ndef visualise_digits(model):\n    \"\"\"\n    Generate and plot an example of each digit (0-9) using the provided VAE `model`.\n\n    Args:\n    model (nn.Module): The Variational AutoEncoder model.\n    \"\"\"\n    # Ensure model is in evaluation mode\n    model.eval()\n\n    # Create a subplot for each digit in a 5x2 grid\n    fig, axs = plt.subplots(5, 2, figsize=(5, 10))\n    fig.subplots_adjust(hspace=0.5, wspace=0)\n\n    for digit in range(10):\n        # Generate an example of the digit\n        out_images = inference(model, digit, num_examples=1)\n\n        # Display the output image with a title\n        ax = axs[digit // 2, digit % 2]\n        ax.imshow(out_images[0][0].detach().numpy().reshape(28, 28), cmap='gray')\n        ax.axis('off')\n        ax.set_title(f'Generated Digit: {digit}')\n\n    plt.show()\n\n\n\nvisualise_digits(model)"
  },
  {
    "objectID": "posts/yt3/index.html#hyperparameters",
    "href": "posts/yt3/index.html#hyperparameters",
    "title": "Variational AutoEncoders in PyTorch",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\n# Configuration\n# Set the device for computation, prefer GPU (cuda) if available, else use CPU\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# The dimension of the input data. For MNIST, images are 28x28 pixels, so the input dimension is 784.\nINPUT_DIM = 784\n\n# The dimension of the first hidden layer in the VAE.\nH_DIM1 = 400\n\n# The dimension of the second hidden layer in the VAE.\nH_DIM2 = 200\n\n# The dimension of the latent space in the VAE. This is the dimension of the random variable z in the VAE.\nZ_DIM = 50\n\n# The number of epochs to train the model. An epoch is one complete pass through the entire training dataset.\nNUM_EPOCHS = 10\n\n# The number of samples per batch. The model parameters are updated once per batch.\nBATCH_SIZE = 32\n\n# The learning rate for the Adam optimizer.\nLR = 3e-4"
  },
  {
    "objectID": "posts/yt3/index.html#initialising-double-layer-vae-model",
    "href": "posts/yt3/index.html#initialising-double-layer-vae-model",
    "title": "Variational AutoEncoders in PyTorch",
    "section": "Initialising Double-Layer VAE model",
    "text": "Initialising Double-Layer VAE model\n\n# Initialise Model\n# Create an instance of the VariationalAutoEncoder_nlayer class with the specified input, hidden, and latent dimensions. Set num_layers=2.\n# The model is moved to the specified device (either CPU or GPU) for computation.\nmodel_2layers = VariationalAutoEncoder_nlayers(INPUT_DIM, \n                                               H_DIM1, \n                                               H_DIM2, \n                                               Z_DIM, \n                                               num_layers=2).to(DEVICE)\n\n# Optimizer\n# The Adam optimizer is used for updating the parameters of the model.\n# The learning rate (LR) is a hyperparameter that controls how much the weights of the network will change in response to the gradient.\noptimizer = torch.optim.Adam(model_2layers.parameters(), lr=LR)\n\n# Loss Function\n# The Binary Cross Entropy (BCE) loss function is used as the reconstruction loss for the VAE.\n# This loss function measures the error between the reconstructed image and the original image.\n# The 'sum' reduction mode is used, which means the losses are summed over all elements in the output.\nloss_fn = nn.BCELoss(reduction=\"sum\")"
  },
  {
    "objectID": "posts/yt3/index.html#training-1",
    "href": "posts/yt3/index.html#training-1",
    "title": "Variational AutoEncoders in PyTorch",
    "section": "Training",
    "text": "Training\nUsing the train_vae model as defined for the Single-Hidden-Layer VAE model:\n\ntrain_vae(model_2layers, train_loader, optimizer, loss_fn, NUM_EPOCHS, DEVICE)\n\nEpoch [1/10]: 100%|██████████| 1875/1875 [00:27&lt;00:00, 67.16it/s, loss=5.66e+3]\nEpoch [2/10]: 100%|██████████| 1875/1875 [00:28&lt;00:00, 66.65it/s, loss=4.33e+3]\nEpoch [3/10]: 100%|██████████| 1875/1875 [00:27&lt;00:00, 67.01it/s, loss=3.89e+3]\nEpoch [4/10]: 100%|██████████| 1875/1875 [00:28&lt;00:00, 66.40it/s, loss=3.68e+3]\nEpoch [5/10]: 100%|██████████| 1875/1875 [00:27&lt;00:00, 66.99it/s, loss=3.56e+3]\nEpoch [6/10]: 100%|██████████| 1875/1875 [00:28&lt;00:00, 65.96it/s, loss=3.49e+3]\nEpoch [7/10]: 100%|██████████| 1875/1875 [00:28&lt;00:00, 66.65it/s, loss=3.44e+3]\nEpoch [8/10]: 100%|██████████| 1875/1875 [00:28&lt;00:00, 65.94it/s, loss=3.4e+3] \nEpoch [9/10]: 100%|██████████| 1875/1875 [00:28&lt;00:00, 64.93it/s, loss=3.37e+3]\nEpoch [10/10]: 100%|██████████| 1875/1875 [00:28&lt;00:00, 64.87it/s, loss=3.35e+3]\n\n\nTraining completed."
  },
  {
    "objectID": "posts/yt3/index.html#visualing-results",
    "href": "posts/yt3/index.html#visualing-results",
    "title": "Variational AutoEncoders in PyTorch",
    "section": "Visualing Results",
    "text": "Visualing Results\nUsing the inference and visualize_digits functions as defined for the Single-Hidden-Layer VAE model:\n\ninference(model_2layers, digit=1, plot=True)\n\n\n\n\n\ninference(model_2layers, digit=6, plot=True)\n\n\n\n\n\nvisualise_digits(model_2layers)"
  }
]