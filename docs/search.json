[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This portfolio is made using Quarto, an open source technical publishing system that provides the functionality to render .ipynb and .md files as blog posts.\nThe .ipynb files are written in Python 3 using Jupyter Lab.\nThe mathematics posts are typically written in LaTeX and converted to .md using Pandoc.\nI am using GitHub pages to host the site."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "The Boltzmann Equation - 3. Information Theory\n\n\n\n\n\n\n\nMathematics\n\n\nProbability Theory\n\n\nInformation Theory\n\n\nBoltzmann Equation\n\n\n\n\nWe introduce key notions from information theory such as differential entropy and the Kullback-Liebler (KL) divergence and prove the information inequality.\n\n\n\n\n\n\nMar 23, 2024\n\n\nDaniel Smith\n\n\n\n\n\n\n  \n\n\n\n\nModel Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nSeaborn\n\n\nTensorFlow\n\n\nOptuna\n\n\n\n\nAfter cleaning and preprocessing a modified LendingClub dataset of loan applicants I implement an Optuna study for both model selection and hyperparameter tuning with cross-validation to choose a model to predict if an unseen applicant will repay their loan.\n\n\n\n\n\n\nMar 19, 2024\n\n\nDaniel Smith\n\n\n\n\n\n\n  \n\n\n\n\nInvestigating the Geographic Distribution of Charity Donors with Interactive Maps made using Folium\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nFolium\n\n\nEmmanuel House\n\n\n\n\nI illustrate my method for investigating the geographic distribution of individual donors to Emmanuel House Support Centre in Nottingham by constructing interactive leaflet maps in folium with a synthetic dataset.\n\n\n\n\n\n\nMar 17, 2024\n\n\nDaniel Smith\n\n\n\n\n\n\n  \n\n\n\n\nGeocoding Postcodes in Python: pgeocode v ONS\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nFolium\n\n\nEmmanuel House\n\n\n\n\nI explain a problem encountered in voluntary work undertaken for Emmanuel House Support Centre in Nottingham related to encoding postcodes as coordinates, and the solution found.\n\n\n\n\n\n\nMar 16, 2024\n\n\nDaniel Smith\n\n\n\n\n\n\n  \n\n\n\n\nThe Boltzmann Equation - 2. Probabilistic Preliminaries\n\n\n\n\n\n\n\nMathematics\n\n\nProbability Theory\n\n\nBoltzmann Equation\n\n\n\n\nBefore we can discuss Information Theory and it’s consequences for the Boltzmann equation we first need to make some definitions from probability theory.\n\n\n\n\n\n\nFeb 13, 2024\n\n\nDaniel Smith\n\n\n\n\n\n\n  \n\n\n\n\nThe Boltzmann Equation - 1. Introduction\n\n\n\n\n\n\n\nMathematics\n\n\nPDEs\n\n\nBoltzmann Equation\n\n\n\n\nI introduce the rigorous theory of the Boltzmann Transport Equation, following my undergraduate research project at Warwick Mathematics Institue: ‘The interplay between Information theory and the long-time behaviour of a dilute gas’.\n\n\n\n\n\n\nJan 29, 2024\n\n\nDaniel Smith\n\n\n\n\n\n\n  \n\n\n\n\nSentiment Analysis with NLTK and Hugging Face Transformers\n\n\n\n\n\n\n\nPython\n\n\nNatural Language Processing\n\n\nNLTK\n\n\nHugging Face\n\n\nSentiment Analysis\n\n\n\n\nSentiment analysis is performed on a dataset of Amazon reviews using NLTK’s VADER and a RoBERTa-base model from Hugging Face.\n\n\n\n\n\n\nJan 23, 2024\n\n\nDaniel Smith\n\n\n\n\n\n\n  \n\n\n\n\nLogistic Regression with a Neural Network Mindset\n\n\n\n\n\n\n\nPython\n\n\nMachine Learning\n\n\nNumPy\n\n\n\n\nLogistic regression is implemented in NumPy and interpreted as a perceptron with sigmoid activation. The resulting model is used to detect cats in an image classification problem. Overfitting to the training data is counteracted by including a regularization term in the cost function. The regularization parameter is tuned to improve accuracy on the validation data.\n\n\n\n\n\n\nJan 16, 2024\n\n\nDaniel Smith\n\n\n\n\n\n\n  \n\n\n\n\nForecasting Energy Consumption with XGBoost\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nXGBoost\n\n\nTime Series Forecasting\n\n\n\n\nInformed by YouTube videos of Rob Mulla we use XGBoost to forecast energy consumption in the eastern US.\n\n\n\n\n\n\nDec 22, 2023\n\n\nDaniel Smith\n\n\n\n\n\n\n  \n\n\n\n\nThe Spaceship Titanic with LightGBM\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nLightGBM\n\n\n\n\nA LightGBM classifier is trained with hyperparameters tuned using a random search to achieve &gt;80% classification accuracy on the Spaceship Titanic dataset.\n\n\n\n\n\n\nNov 23, 2023\n\n\nDaniel Smith\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/SELECTION_TUNING_OPTUNA/index.html",
    "href": "posts/SELECTION_TUNING_OPTUNA/index.html",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nimport xgboost as xgb\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\nimport optuna\nprint(\"Python version:\")\n!python --version\n\nPython version:\nPython 3.11.4\nprint(\"Optuna version:\")\nprint(optuna.__version__)\n\nOptuna version:\n3.5.0\nprint(\"TensorFlow version:\")\nprint(tf.__version__)\n\nTensorFlow version:\n2.14.0"
  },
  {
    "objectID": "posts/SELECTION_TUNING_OPTUNA/index.html#section",
    "href": "posts/SELECTION_TUNING_OPTUNA/index.html#section",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "—-",
    "text": "—-\nWe use a modified LendingClub dataset from Kaggle, found in a Udemy course by Jose Portilla.\nBefore preprocessing, the dataset has the following features:\n\n\n\n\n\n\n\nLoanStatNew\n\n\nDescription\n\n\n\n\n\n\n0\n\n\nloan_amnt\n\n\nThe listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\n\n\n\n\n1\n\n\nterm\n\n\nThe number of payments on the loan. Values are in months and can be either 36 or 60.\n\n\n\n\n2\n\n\nint_rate\n\n\nInterest Rate on the loan\n\n\n\n\n3\n\n\ninstallment\n\n\nThe monthly payment owed by the borrower if the loan originates.\n\n\n\n\n4\n\n\ngrade\n\n\nLC assigned loan grade\n\n\n\n\n5\n\n\nsub_grade\n\n\nLC assigned loan subgrade\n\n\n\n\n6\n\n\nemp_title\n\n\nThe job title supplied by the Borrower when applying for the loan.*\n\n\n\n\n7\n\n\nemp_length\n\n\nEmployment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n\n\n\n\n8\n\n\nhome_ownership\n\n\nThe home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER\n\n\n\n\n9\n\n\nannual_inc\n\n\nThe self-reported annual income provided by the borrower during registration.\n\n\n\n\n10\n\n\nverification_status\n\n\nIndicates if income was verified by LC, not verified, or if the income source was verified\n\n\n\n\n11\n\n\nissue_d\n\n\nThe month which the loan was funded\n\n\n\n\n12\n\n\nloan_status\n\n\nCurrent status of the loan\n\n\n\n\n13\n\n\npurpose\n\n\nA category provided by the borrower for the loan request.\n\n\n\n\n14\n\n\ntitle\n\n\nThe loan title provided by the borrower\n\n\n\n\n15\n\n\nzip_code\n\n\nThe first 3 numbers of the zip code provided by the borrower in the loan application.\n\n\n\n\n16\n\n\naddr_state\n\n\nThe state provided by the borrower in the loan application\n\n\n\n\n17\n\n\ndti\n\n\nA ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n\n\n\n\n18\n\n\nearliest_cr_line\n\n\nThe month the borrower’s earliest reported credit line was opened\n\n\n\n\n19\n\n\nopen_acc\n\n\nThe number of open credit lines in the borrower’s credit file.\n\n\n\n\n20\n\n\npub_rec\n\n\nNumber of derogatory public records\n\n\n\n\n21\n\n\nrevol_bal\n\n\nTotal credit revolving balance\n\n\n\n\n22\n\n\nrevol_util\n\n\nRevolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n\n\n\n\n23\n\n\ntotal_acc\n\n\nThe total number of credit lines currently in the borrower’s credit file\n\n\n\n\n24\n\n\ninitial_list_status\n\n\nThe initial listing status of the loan. Possible values are – W, F\n\n\n\n\n25\n\n\napplication_type\n\n\nIndicates whether the loan is an individual application or a joint application with two co-borrowers\n\n\n\n\n26\n\n\nmort_acc\n\n\nNumber of mortgage accounts.\n\n\n\n\n27\n\n\npub_rec_bankruptcies\n\n\nNumber of public record bankruptcies"
  },
  {
    "objectID": "posts/SELECTION_TUNING_OPTUNA/index.html#section-1",
    "href": "posts/SELECTION_TUNING_OPTUNA/index.html#section-1",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "—",
    "text": "—\n\ndf = pd.read_csv('lending_club_loan.csv')\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 396030 entries, 0 to 396029\nData columns (total 27 columns):\n #   Column                Non-Null Count   Dtype  \n---  ------                --------------   -----  \n 0   loan_amnt             396030 non-null  float64\n 1   term                  396030 non-null  object \n 2   int_rate              396030 non-null  float64\n 3   installment           396030 non-null  float64\n 4   grade                 396030 non-null  object \n 5   sub_grade             396030 non-null  object \n 6   emp_title             373103 non-null  object \n 7   emp_length            377729 non-null  object \n 8   home_ownership        396030 non-null  object \n 9   annual_inc            396030 non-null  float64\n 10  verification_status   396030 non-null  object \n 11  issue_d               396030 non-null  object \n 12  loan_status           396030 non-null  object \n 13  purpose               396030 non-null  object \n 14  title                 394274 non-null  object \n 15  dti                   396030 non-null  float64\n 16  earliest_cr_line      396030 non-null  object \n 17  open_acc              396030 non-null  float64\n 18  pub_rec               396030 non-null  float64\n 19  revol_bal             396030 non-null  float64\n 20  revol_util            395754 non-null  float64\n 21  total_acc             396030 non-null  float64\n 22  initial_list_status   396030 non-null  object \n 23  application_type      396030 non-null  object \n 24  mort_acc              358235 non-null  float64\n 25  pub_rec_bankruptcies  395495 non-null  float64\n 26  address               396030 non-null  object \ndtypes: float64(12), object(15)\nmemory usage: 81.6+ MB\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nloan_amnt\nterm\nint_rate\ninstallment\ngrade\nsub_grade\nemp_title\nemp_length\nhome_ownership\nannual_inc\n...\nopen_acc\npub_rec\nrevol_bal\nrevol_util\ntotal_acc\ninitial_list_status\napplication_type\nmort_acc\npub_rec_bankruptcies\naddress\n\n\n\n\n0\n10000.0\n36 months\n11.44\n329.48\nB\nB4\nMarketing\n10+ years\nRENT\n117000.0\n...\n16.0\n0.0\n36369.0\n41.8\n25.0\nw\nINDIVIDUAL\n0.0\n0.0\n0174 Michelle Gateway\\r\\nMendozaberg, OK 22690\n\n\n1\n8000.0\n36 months\n11.99\n265.68\nB\nB5\nCredit analyst\n4 years\nMORTGAGE\n65000.0\n...\n17.0\n0.0\n20131.0\n53.3\n27.0\nf\nINDIVIDUAL\n3.0\n0.0\n1076 Carney Fort Apt. 347\\r\\nLoganmouth, SD 05113\n\n\n2\n15600.0\n36 months\n10.49\n506.97\nB\nB3\nStatistician\n&lt; 1 year\nRENT\n43057.0\n...\n13.0\n0.0\n11987.0\n92.2\n26.0\nf\nINDIVIDUAL\n0.0\n0.0\n87025 Mark Dale Apt. 269\\r\\nNew Sabrina, WV 05113\n\n\n3\n7200.0\n36 months\n6.49\n220.65\nA\nA2\nClient Advocate\n6 years\nRENT\n54000.0\n...\n6.0\n0.0\n5472.0\n21.5\n13.0\nf\nINDIVIDUAL\n0.0\n0.0\n823 Reid Ford\\r\\nDelacruzside, MA 00813\n\n\n4\n24375.0\n60 months\n17.27\n609.33\nC\nC5\nDestiny Management Inc.\n9 years\nMORTGAGE\n55000.0\n...\n13.0\n0.0\n24584.0\n69.8\n43.0\nf\nINDIVIDUAL\n1.0\n0.0\n679 Luna Roads\\r\\nGreggshire, VA 11650\n\n\n\n\n5 rows × 27 columns"
  },
  {
    "objectID": "posts/SELECTION_TUNING_OPTUNA/index.html#emp_title-and-emp_length",
    "href": "posts/SELECTION_TUNING_OPTUNA/index.html#emp_title-and-emp_length",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "emp_title and emp_length",
    "text": "emp_title and emp_length\n\ndf['emp_title']\n\n0                        Marketing\n1                  Credit analyst \n2                     Statistician\n3                  Client Advocate\n4          Destiny Management Inc.\n                    ...           \n396025            licensed bankere\n396026                       Agent\n396027                City Carrier\n396028        Gracon Services, Inc\n396029    Internal Revenue Service\nName: emp_title, Length: 396030, dtype: object\n\n\n\ndf['emp_title'].nunique()\n\n173105\n\n\nemp_title has too many unique values to encode\n\ndf = df.drop('emp_title', axis=1)\n\n\ndf['emp_length']\n\n0         10+ years\n1           4 years\n2          &lt; 1 year\n3           6 years\n4           9 years\n            ...    \n396025      2 years\n396026      5 years\n396027    10+ years\n396028    10+ years\n396029    10+ years\nName: emp_length, Length: 396030, dtype: object\n\n\n\nsorted(df['emp_length'].dropna().unique())\n\n['1 year',\n '10+ years',\n '2 years',\n '3 years',\n '4 years',\n '5 years',\n '6 years',\n '7 years',\n '8 years',\n '9 years',\n '&lt; 1 year']\n\n\n\nordered_emp_lengths = ['&lt; 1 year',\n '1 year',\n '2 years',\n '3 years',\n '4 years',\n '5 years',\n '6 years',\n '7 years',\n '8 years',\n '9 years',\n '10+ years']\n\n\nplt.figure(figsize=(12,5))\nsns.countplot(data=df,\n              x='emp_length',\n              order=ordered_emp_lengths,\n              palette='viridis')\n\n&lt;Axes: xlabel='emp_length', ylabel='count'&gt;\n\n\n\n\n\n\nplt.figure(figsize=(12,5))\nsns.countplot(data=df,\n              x='emp_length',\n              order=ordered_emp_lengths,\n              hue='loan_repaid')\n\n&lt;Axes: xlabel='emp_length', ylabel='count'&gt;\n\n\n\n\n\n\nemp_co = df[df['loan_repaid'] == 0].groupby('emp_length').count()['loan_repaid']\nemp_fp = df[df['loan_repaid'] == 1].groupby('emp_length').count()['loan_repaid']\n\nplt.figure(figsize=(6,2))\n((emp_co/(emp_fp+emp_co))*100).plot(kind='bar')\n\n&lt;Axes: xlabel='emp_length'&gt;\n\n\n\n\n\nemp_length doesn’t look to tell us anything about loan_repaid. Very similar fractions of all employment lengths did not manage to repay their loans, so we drop this column too.\n\ndf = df.drop('emp_length',axis=1)"
  },
  {
    "objectID": "posts/SELECTION_TUNING_OPTUNA/index.html#filling-mort_acc-using-total_acc",
    "href": "posts/SELECTION_TUNING_OPTUNA/index.html#filling-mort_acc-using-total_acc",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "Filling mort_acc using total_acc",
    "text": "Filling mort_acc using total_acc\n\npercentage_null(df)\n\n\nPercentages of values missing:\n\nloan_amnt               0.00\nterm                    0.00\nint_rate                0.00\ninstallment             0.00\ngrade                   0.00\nsub_grade               0.00\nhome_ownership          0.00\nannual_inc              0.00\nverification_status     0.00\nissue_d                 0.00\nloan_status             0.00\npurpose                 0.00\ndti                     0.00\nearliest_cr_line        0.00\nopen_acc                0.00\npub_rec                 0.00\nrevol_bal               0.00\nrevol_util              0.07\ntotal_acc               0.00\ninitial_list_status     0.00\napplication_type        0.00\nmort_acc                9.54\npub_rec_bankruptcies    0.14\naddress                 0.00\nloan_repaid             0.00\ndtype: float64\n\n\n\ndf['mort_acc'].value_counts()\n\nmort_acc\n0.0     139777\n1.0      60416\n2.0      49948\n3.0      38049\n4.0      27887\n5.0      18194\n6.0      11069\n7.0       6052\n8.0       3121\n9.0       1656\n10.0       865\n11.0       479\n12.0       264\n13.0       146\n14.0       107\n15.0        61\n16.0        37\n17.0        22\n18.0        18\n19.0        15\n20.0        13\n24.0        10\n22.0         7\n21.0         4\n25.0         4\n27.0         3\n32.0         2\n31.0         2\n23.0         2\n26.0         2\n28.0         1\n30.0         1\n34.0         1\nName: count, dtype: int64\n\n\n\ndf.corr(numeric_only=True)['mort_acc'].sort_values()\n\nint_rate               -0.082583\ndti                    -0.025439\nrevol_util              0.007514\npub_rec                 0.011552\npub_rec_bankruptcies    0.027239\nloan_repaid             0.073111\nopen_acc                0.109205\ninstallment             0.193694\nrevol_bal               0.194925\nloan_amnt               0.222315\nannual_inc              0.236320\ntotal_acc               0.381072\nmort_acc                1.000000\nName: mort_acc, dtype: float64\n\n\nmort_acc correlates most strongly with total_acc, so we can leverage the fact that total_acc has no missing values to fill the missing values in mort_acc\n\ntotal_acc_avgs = df.groupby('total_acc').mean(numeric_only=True)['mort_acc']\n\n\ndef fill_mort_acc(total_acc,mort_acc):\n    \n    if np.isnan(mort_acc):\n        return total_acc_avgs[total_acc]\n    else:\n        return mort_acc\n\n\ndf['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'],x['mort_acc']),axis=1)\n\n\ndf.isnull().sum()\n\nloan_amnt                 0\nterm                      0\nint_rate                  0\ninstallment               0\ngrade                     0\nsub_grade                 0\nhome_ownership            0\nannual_inc                0\nverification_status       0\nissue_d                   0\nloan_status               0\npurpose                   0\ndti                       0\nearliest_cr_line          0\nopen_acc                  0\npub_rec                   0\nrevol_bal                 0\nrevol_util              276\ntotal_acc                 0\ninitial_list_status       0\napplication_type          0\nmort_acc                  0\npub_rec_bankruptcies    535\naddress                   0\nloan_repaid               0\ndtype: int64\n\n\n\npercentage_null(df)\n\n\nPercentages of values missing:\n\nloan_amnt               0.00\nterm                    0.00\nint_rate                0.00\ninstallment             0.00\ngrade                   0.00\nsub_grade               0.00\nhome_ownership          0.00\nannual_inc              0.00\nverification_status     0.00\nissue_d                 0.00\nloan_status             0.00\npurpose                 0.00\ndti                     0.00\nearliest_cr_line        0.00\nopen_acc                0.00\npub_rec                 0.00\nrevol_bal               0.00\nrevol_util              0.07\ntotal_acc               0.00\ninitial_list_status     0.00\napplication_type        0.00\nmort_acc                0.00\npub_rec_bankruptcies    0.14\naddress                 0.00\nloan_repaid             0.00\ndtype: float64\n\n\nOnly a neglible amount of rows still have missing data points, so we now feel comfortable dropping these rows\n\ndf = df.dropna()\n\n\npercentage_null(df)\n\n\nPercentages of values missing:\n\nloan_amnt               0.0\nterm                    0.0\nint_rate                0.0\ninstallment             0.0\ngrade                   0.0\nsub_grade               0.0\nhome_ownership          0.0\nannual_inc              0.0\nverification_status     0.0\nissue_d                 0.0\nloan_status             0.0\npurpose                 0.0\ndti                     0.0\nearliest_cr_line        0.0\nopen_acc                0.0\npub_rec                 0.0\nrevol_bal               0.0\nrevol_util              0.0\ntotal_acc               0.0\ninitial_list_status     0.0\napplication_type        0.0\nmort_acc                0.0\npub_rec_bankruptcies    0.0\naddress                 0.0\nloan_repaid             0.0\ndtype: float64"
  },
  {
    "objectID": "posts/SELECTION_TUNING_OPTUNA/index.html#categorical-features",
    "href": "posts/SELECTION_TUNING_OPTUNA/index.html#categorical-features",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "Categorical Features",
    "text": "Categorical Features\nHere we treat the categorical features such as home_ownership and purpose by either dropping them or one-hot encoding them as to end with a dataframe consisting only of numeric features.\n\ndf.select_dtypes(['object']).columns\n\nIndex(['term', 'grade', 'sub_grade', 'home_ownership', 'verification_status',\n       'issue_d', 'loan_status', 'purpose', 'earliest_cr_line',\n       'initial_list_status', 'application_type', 'address'],\n      dtype='object')\n\n\n\ndf['term'].value_counts()\n\nterm\n 36 months    301247\n 60 months     93972\nName: count, dtype: int64\n\n\n\ndf['term'] = df['term'].apply(lambda x: int(x.split()[0]))\n\n\ndf['term']\n\n0         36\n1         36\n2         36\n3         36\n4         60\n          ..\n396025    60\n396026    36\n396027    36\n396028    60\n396029    36\nName: term, Length: 395219, dtype: int64\n\n\n\ndf['zip_code'] = df['address'].apply(lambda address: address[-5:])\n\n\ndf['zip_code'].value_counts()\n\nzip_code\n70466    56880\n22690    56413\n30723    56402\n48052    55811\n00813    45725\n29597    45393\n05113    45300\n11650    11210\n93700    11126\n86630    10959\nName: count, dtype: int64\n\n\n\ndf['home_ownership'].value_counts()\n\nhome_ownership\nMORTGAGE    198022\nRENT        159395\nOWN          37660\nOTHER          110\nNONE            29\nANY              3\nName: count, dtype: int64\n\n\n\ndf['home_ownership'] = df['home_ownership'].replace(['NONE','ANY'],'OTHER')\n\n\ndf['home_ownership'].value_counts()\n\nhome_ownership\nMORTGAGE    198022\nRENT        159395\nOWN          37660\nOTHER          142\nName: count, dtype: int64\n\n\none-hot encoding with pandas.get_dummies:\n\ncolumns_to_1hot = ['home_ownership',\n                   'verification_status',\n                   'application_type',\n                   'initial_list_status',\n                   'purpose',\n                   'zip_code',\n                   'sub_grade']\n\n\ndf = pd.get_dummies(data=df,\n                    columns=columns_to_1hot,\n                    drop_first=True,\n                    dtype=int)\n\n\ndf.columns\n\nIndex(['loan_amnt', 'term', 'int_rate', 'installment', 'grade', 'annual_inc',\n       'issue_d', 'loan_status', 'dti', 'earliest_cr_line', 'open_acc',\n       'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'mort_acc',\n       'pub_rec_bankruptcies', 'address', 'loan_repaid',\n       'home_ownership_OTHER', 'home_ownership_OWN', 'home_ownership_RENT',\n       'verification_status_Source Verified', 'verification_status_Verified',\n       'application_type_INDIVIDUAL', 'application_type_JOINT',\n       'initial_list_status_w', 'purpose_credit_card',\n       'purpose_debt_consolidation', 'purpose_educational',\n       'purpose_home_improvement', 'purpose_house', 'purpose_major_purchase',\n       'purpose_medical', 'purpose_moving', 'purpose_other',\n       'purpose_renewable_energy', 'purpose_small_business',\n       'purpose_vacation', 'purpose_wedding', 'zip_code_05113',\n       'zip_code_11650', 'zip_code_22690', 'zip_code_29597', 'zip_code_30723',\n       'zip_code_48052', 'zip_code_70466', 'zip_code_86630', 'zip_code_93700',\n       'sub_grade_A2', 'sub_grade_A3', 'sub_grade_A4', 'sub_grade_A5',\n       'sub_grade_B1', 'sub_grade_B2', 'sub_grade_B3', 'sub_grade_B4',\n       'sub_grade_B5', 'sub_grade_C1', 'sub_grade_C2', 'sub_grade_C3',\n       'sub_grade_C4', 'sub_grade_C5', 'sub_grade_D1', 'sub_grade_D2',\n       'sub_grade_D3', 'sub_grade_D4', 'sub_grade_D5', 'sub_grade_E1',\n       'sub_grade_E2', 'sub_grade_E3', 'sub_grade_E4', 'sub_grade_E5',\n       'sub_grade_F1', 'sub_grade_F2', 'sub_grade_F3', 'sub_grade_F4',\n       'sub_grade_F5', 'sub_grade_G1', 'sub_grade_G2', 'sub_grade_G3',\n       'sub_grade_G4', 'sub_grade_G5'],\n      dtype='object')\n\n\n\ndf['earliest_cr_line']\n\n0         Jun-1990\n1         Jul-2004\n2         Aug-2007\n3         Sep-2006\n4         Mar-1999\n            ...   \n396025    Nov-2004\n396026    Feb-2006\n396027    Mar-1997\n396028    Nov-1990\n396029    Sep-1998\nName: earliest_cr_line, Length: 395219, dtype: object\n\n\n\ndf['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda x: int(x.split('-')[-1]))\n\n\ndf['earliest_cr_year']\n\n0         1990\n1         2004\n2         2007\n3         2006\n4         1999\n          ... \n396025    2004\n396026    2006\n396027    1997\n396028    1990\n396029    1998\nName: earliest_cr_year, Length: 395219, dtype: int64\n\n\nDropping categorical features not worth encoding:\n\ncolumns_to_drop = ['earliest_cr_line',\n                   'grade',\n                   'issue_d',\n                   'loan_status',\n                   'address']\n\n\ndf = df.drop(columns_to_drop,axis=1)"
  },
  {
    "objectID": "posts/SELECTION_TUNING_OPTUNA/index.html#traintest-split",
    "href": "posts/SELECTION_TUNING_OPTUNA/index.html#traintest-split",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "TRAIN/TEST SPLIT",
    "text": "TRAIN/TEST SPLIT\nSplitting the dataset into a training set and a test set:\n(Note that cross-validation is performed inside the Optuna study, so we don’t need to separate out a validation set here)\n\nX = df.drop('loan_repaid', axis=1).values\ny = df['loan_repaid'].values\n\nfeatures = df.columns.drop('loan_repaid')\ntarget = 'loan_repaid'\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=594)\n\n# Convert to DataFrame and add column names\nX_test_df = pd.DataFrame(X_test, columns=features)\ny_test_df = pd.DataFrame(y_test, columns=[target])\n\ndf_test = pd.concat([X_test_df, y_test_df], axis=1)\n\n\nprint(\"Training set shape: \", X_train.shape, y_train.shape)\nprint(\"Test set shape: \", X_test.shape, y_test.shape)\n\nTraining set shape:  (375458, 78) (375458,)\nTest set shape:  (19761, 78) (19761,)"
  },
  {
    "objectID": "posts/SELECTION_TUNING_OPTUNA/index.html#normalising-the-data",
    "href": "posts/SELECTION_TUNING_OPTUNA/index.html#normalising-the-data",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "Normalising the Data",
    "text": "Normalising the Data\nSome machine learning algorithms require all columns of the dataframe to consist of data on comparable scales. We use StandardScalar from scikit-learn to force this to be true.\nThere is rarely a drawback to normalising data like this. If in doubt, normalise.\n\nscaler = StandardScaler()\n\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nX_train\n\narray([[ 0.82301704, -0.55823002, -1.28362299, ..., -0.03076372,\n        -0.02818382, -2.75955691],\n       [-1.45078579, -0.55823002, -0.46740923, ..., -0.03076372,\n        -0.02818382,  0.01934793],\n       [-1.25930765, -0.55823002,  1.14265627, ..., -0.03076372,\n        -0.02818382,  0.5751289 ],\n       ...,\n       [ 0.70334321, -0.55823002, -0.36901634, ..., -0.03076372,\n        -0.02818382,  0.29723841],\n       [ 1.90008154, -0.55823002, -0.33994571, ..., -0.03076372,\n        -0.02818382, -1.78694022],\n       [-0.52929727, -0.55823002, -1.17404909, ..., -0.03076372,\n        -0.02818382, -0.25854255]])"
  },
  {
    "objectID": "posts/SELECTION_TUNING_OPTUNA/index.html#producing-some-visualisations-investigating-the-distribution-of-misclassified-points",
    "href": "posts/SELECTION_TUNING_OPTUNA/index.html#producing-some-visualisations-investigating-the-distribution-of-misclassified-points",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "Producing some visualisations investigating the distribution of misclassified points",
    "text": "Producing some visualisations investigating the distribution of misclassified points\n\nmisclassified = df.iloc[np.concatenate((fp_indices[0], fn_indices[0]))].copy()\nmisclassified.loc[:, 'type'] = ['FP' if i in fp_indices[0] else 'FN' for i in misclassified.index]\n\n\n# Boxplot of loan_amnt\nplt.figure(figsize=(10,6))\nsns.boxplot(x='type', y='loan_amnt', data=misclassified)\nplt.title('Loan Amount Spread for Misclassified Points')\nplt.show()\n\n\n\n\n\n# Boxplot of int_rate\nplt.figure(figsize=(10,6))\nsns.boxplot(x='type', y='int_rate', data=misclassified)\nplt.title('Interest Rate Spread for Misclassified Points')\nplt.show()\n\n\n\n\n\n# Boxplot of annual_inc\nplt.figure(figsize=(10,6))\nsns.boxplot(x='type', y='annual_inc', data=misclassified)\nplt.title('Annual Income Spread for Misclassified Points')\nplt.ylim(0,200000)\nplt.show()\n\n\n\n\n\n# Boxplot of revol_bal\nplt.figure(figsize=(10,6))\nsns.boxplot(x='type', y='revol_bal', data=misclassified)\nplt.title('Revolving Balance Spread for Misclassified Points')\nplt.ylim(0,60000)\nplt.show()"
  },
  {
    "objectID": "posts/BTE3/index.html#relative-entropy-mutual-information",
    "href": "posts/BTE3/index.html#relative-entropy-mutual-information",
    "title": "The Boltzmann Equation - 3. Information Theory",
    "section": "3.2 - Relative Entropy & Mutual Information",
    "text": "3.2 - Relative Entropy & Mutual Information\n\nDefinition 3.2.1 (KL divergence)\nThe relative entropy (also known as the Kullback-Liebler divergence) \\(D(f\\,||\\,g)\\) between two densities \\(f\\) and \\(g\\) is defined as\n\\[D(f\\,||\\,g) = \\int f(x)\\log\\frac{f(x)}{g(x)}\\,\\text{d}x.\\]\n\nNote: \\[D(f\\,||\\,g) &lt; \\infty \\Longleftrightarrow \\text{supp}\\,f\\subseteq\\text{supp}\\,g.\\]\n\n\nDefinition 3.2.2 (Mutual information)\nGiven two random variables \\(X\\) and \\(Y\\) with joint density \\(f_{X,Y}\\) define the mutual information \\(I(X;Y)\\) between \\(X\\) and \\(Y\\) by\n\\[I(X;Y) = \\int f_{X,Y}(x,y)\\log\\left[\\frac{f_{X,Y}(x,y)}{f_X(x)f_Y(y)}\\right]\\,\\text{d}x\\,\\text{d}y.\\]\n\nFrom the definition it is clear that we have the formulas\n\\[\\begin{aligned}\n    I(X;Y) &= h(X) - h(X|Y)\\\\\n    &= h(Y) - h(Y|X)\\\\\n    &= h(X) + h(Y) - h(X,Y).\n\\end{aligned}\\]\nAlong with\n\\[I(X;Y) = D(f_{X,Y}\\,||\\,f_X\\otimes f_Y).\\]\nNote the special cases\n\\[\\begin{aligned}\nI(X;Y) &= I(Y;X),\\\\\nI(X;X) &= h(X).\n\\end{aligned}\\]\n\n\nTheorem 3.2.3 (Information inequality)\nFor any pair of densities \\(f,\\,g\\):\n\\[D(f\\,||\\,g) \\geq 0,\\]\nwith equality if and only if \\(f = g\\) a.e.\nProof.\nWithout loss of generality assume \\(f/g\\geq1\\). Use the fact that \\(\\int f = \\int g = 1\\) to rewrite\n\\[\\begin{aligned}\n    D(f\\,||\\,g) &= \\int f\\log\\frac{f}{g}\\\\\n    &= \\int f\\left(\\frac{g}{f}-1-\\log\\frac{g}{f}\\right).\n\\end{aligned}\\]\nNow note that for \\(t\\geq1\\) we have the inequality:\n\\[t - 1 -\\log t \\geq 0,\\]\nin which equality holds iff \\(t=1.\\) This can be easily established graphically or by means of elementary calculus. Applying this inequality to \\(f/g\\) and integrating yields\n\\[\\int \\frac{g}{f}-1-\\log\\frac{g}{f}\\;\\geq\\; 0\\]\nwith equality iff \\(f = g\\) a.e. ◻\n\n\n\nCorollary 3.2.4\nFor any pair of random variables \\(X,\\,Y\\): \\[I(X;Y)\\geq 0,\\] with equality if and only if \\(X\\) and \\(Y\\) are independent.\nProof.\n\\[I(X;Y) = D(f_{X,Y}\\,||\\,f_X\\otimes f_Y) \\geq 0\\]\nwith equality iff \\(f_{X,Y} = f_X\\otimes f_Y\\) a.e. i.e. iff \\(X\\) and \\(Y\\) are independent. ◻\n\n\n\nCorollary 3.2.5\nFor any pair of random variables \\(X,\\,Y\\):\n\\[h(X|Y) \\leq h(X),\\]\nwith equality if and only if \\(X\\) and \\(Y\\) are independent.\nProof.\n\\[h(X) - h(X|Y) = I(X;Y)\\,\\geq\\,0,\\]\nwith equality iff \\(X\\) and \\(Y\\) are independent by Corollary 3.2.4. ◻"
  }
]