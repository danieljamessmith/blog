[
  {
    "objectID": "posts/yt2/index.html",
    "href": "posts/yt2/index.html",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\ncolor_pal = sns.color_palette(\"mako\")\n\nfrom wordcloud import WordCloud\nfrom tqdm.notebook import tqdm  #Progress Bar\n\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom scipy.special import softmax\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch._utils\")\nprint(\"Python version:\")\n!python --version\n\nPython version:\nPython 3.11.4\nTable of contents\n1. Data\n1.1. EDA\n1.2. Cleaning Text\n2. Sentiment Analysis with VADER\n2.1. Basic NLTK\n2.2. SentimentIntensityAnalyzer\n2.3. Plotting VADER Results\n3. Sentiment Analysis with a Hugging Face Transformer (RoBERTa)\n4. Comparing Models\n5. Reviewing Examples\nSentiment analysis is a standard application of natural language processing (NLP) in which a machine learning algorithm is trained to classify text as having either positive, negative or neutral emotional tone.\nAs a subfield of NLP, sentiment analysis is closely related to computational linguistics. Sentiment analysis has also been found to be useful in the implementation of recommender systems, in which an automated understanding of the emotional content of reviews proves to be crucial for accurate and personalized content recommendation.\nIn this post sentiment analysis is performed on a dataset of Amazon reviews using both NLTK’s VADER sentiment analysis tool and the Hugging Face transformer-based model RoBERTa."
  },
  {
    "objectID": "posts/yt2/index.html#eda",
    "href": "posts/yt2/index.html#eda",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "1.1. EDA",
    "text": "1.1. EDA\n\nax = df['Score'].value_counts().sort_index() \\\n    .plot(kind='bar',\n          title='Count of Reviews by Stars',\n          figsize=(10,5),\n          color = color_pal[3])\nax.set_xlabel('Review Stars')\nax.set_ylabel('Number of Reviews')\n\nText(0, 0.5, 'Number of Reviews')\n\n\n\n\n\n\n\n\n\n\n# Generating a worldcloud of all the text in the 'Text' column of the dataframe\n\ntext = ' '.join(df['Text'])\nwordcloud = WordCloud(width=800, height=400, background_color='white',colormap='mako').generate(text)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\ndf['ReviewLength'] = df['Text'].apply(len)\n\nplt.figure(figsize=(10, 5))\nplt.hist(df['ReviewLength'], bins=50, color=color_pal[2], edgecolor='black')\nplt.title('Distribution of Review Lengths')\nplt.xlabel('Review Length (Number of Characters)')\nplt.ylabel('Number of Reviews')\nplt.show()\n\n\n\n\n\n\n\n\n\nvectorizer = CountVectorizer(stop_words='english', max_features=20)\nword_matrix = vectorizer.fit_transform(df['Text'])\nword_frequency = word_matrix.sum(axis=0)\n\nwords = vectorizer.get_feature_names_out()\ncounts = word_frequency.A1\n\nsorted_indices = counts.argsort()[::-1]\nwords = [words[i] for i in sorted_indices]\ncounts = counts[sorted_indices]\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=words, y=counts, palette='mako')\nplt.title('Top 20 Most Common Words in Reviews')\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45, ha='right');"
  },
  {
    "objectID": "posts/yt2/index.html#cleaning-text",
    "href": "posts/yt2/index.html#cleaning-text",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "1.2. Cleaning Text",
    "text": "1.2. Cleaning Text\n“br” is such a common word beacuse it is present in many of the reviews as a html tag: &lt;br&gt;&lt;/br&gt;\nWe can fix this using the BeautifulSoup library.\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\ndef clean_text(text):\n    # Remove HTML tags\n    soup = BeautifulSoup(str(text), 'html.parser')\n    cleaned_text = soup.get_text()\n\n    return cleaned_text\n\n\ndf['Text'] = df['Text'].apply(clean_text)\n\n\n# Generating a worldcloud of all the text in the 'Text' column of the dataframe\n\ntext = ' '.join(df['Text'])\nwordcloud = WordCloud(width=800, height=400, background_color='white',colormap='mako').generate(text)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nvectorizer = CountVectorizer(stop_words='english', max_features=20)\nword_matrix = vectorizer.fit_transform(df['Text'])\nword_frequency = word_matrix.sum(axis=0)\n\nwords = vectorizer.get_feature_names_out()\ncounts = word_frequency.A1\n\nsorted_indices = counts.argsort()[::-1]\nwords = [words[i] for i in sorted_indices]\ncounts = counts[sorted_indices]\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=words, y=counts, palette='mako')\nplt.title('Top 20 Most Common Words in Reviews')\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45, ha='right');"
  },
  {
    "objectID": "posts/yt2/index.html#basic-nltk",
    "href": "posts/yt2/index.html#basic-nltk",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "2.1. Basic NLTK",
    "text": "2.1. Basic NLTK\nNLTK stands for Natural Language Toolkit. It is a comprehensive library in Python that provides tools and resources for working text data. NLTK includes various modules and packages for tasks such as tokenization, stemming, tagging, parsing, and sentiment analysis, making it a valuable resource for NLP tasks.\n\nexample = df['Text'][50]\nprint(example)\n\nThis oatmeal is not good. Its mushy, soft, I don't like it. Quaker Oats is the way to go.\n\n\n\ntokens = nltk.word_tokenize(example)\ntokens[:10]\n\n['This', 'oatmeal', 'is', 'not', 'good', '.', 'Its', 'mushy', ',', 'soft']\n\n\n\ntagged = nltk.pos_tag(tokens)\ntagged[:10]\n\n[('This', 'DT'),\n ('oatmeal', 'NN'),\n ('is', 'VBZ'),\n ('not', 'RB'),\n ('good', 'JJ'),\n ('.', '.'),\n ('Its', 'PRP$'),\n ('mushy', 'NN'),\n (',', ','),\n ('soft', 'JJ')]\n\n\n\nentities = nltk.chunk.ne_chunk(tagged)\nentities.pprint()\n\n(S\n  This/DT\n  oatmeal/NN\n  is/VBZ\n  not/RB\n  good/JJ\n  ./.\n  Its/PRP$\n  mushy/NN\n  ,/,\n  soft/JJ\n  ,/,\n  I/PRP\n  do/VBP\n  n't/RB\n  like/VB\n  it/PRP\n  ./.\n  (ORGANIZATION Quaker/NNP Oats/NNPS)\n  is/VBZ\n  the/DT\n  way/NN\n  to/TO\n  go/VB\n  ./.)"
  },
  {
    "objectID": "posts/yt2/index.html#sentimentintensityanalyzer",
    "href": "posts/yt2/index.html#sentimentintensityanalyzer",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "2.2. SentimentIntensityAnalyzer",
    "text": "2.2. SentimentIntensityAnalyzer\nSentimentIntensityAnalyzer is a class in the NLTK library’s VADER module. It is designed for sentiment analysis and provides a pre-trained machine learning model to assess the sentiment of a piece of text by analyzing the intensity of positive, negative, and neutral sentiments.\nThis uses a “bag of words” approach in which: 1. Stop words are removed 2. Each word is scored, and combined to give a total score\n\nsia = SentimentIntensityAnalyzer()\n\n\nsia.polarity_scores('This is a positive sentence.')\n\n{'neg': 0.0, 'neu': 0.29, 'pos': 0.71, 'compound': 0.5994}\n\n\n\nsia.polarity_scores('This is a negative sentence.')\n\n{'neg': 0.529, 'neu': 0.286, 'pos': 0.186, 'compound': -0.5267}\n\n\n\nsia.polarity_scores('I am so happy!')\n\n{'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.6468}\n\n\n\nsia.polarity_scores('I hate this product')\n\n{'neg': 0.649, 'neu': 0.351, 'pos': 0.0, 'compound': -0.5719}\n\n\n\nexample\n\n\"This oatmeal is not good. Its mushy, soft, I don't like it. Quaker Oats is the way to go.\"\n\n\n\nsia.polarity_scores(example)\n\n{'neg': 0.22, 'neu': 0.78, 'pos': 0.0, 'compound': -0.5448}\n\n\n\n# Run the polarity score on the entire dataset\nres = {}\nfor i, row in tqdm(df.iterrows(), total=len(df)):\n    text = row['Text']\n    myid = row['Id']\n    res[myid] = sia.polarity_scores(text)\n\n\n\n\n\n# Convert res to a dataframe\nvaders = pd.DataFrame(res).T\nvaders = vaders.reset_index().rename(columns={'index':'Id'})\nvaders = vaders.merge(df,how='left')\n\n\n# Now we have sentiment score and metadata\nvaders\n\n\n\n\n\n\n\n\nId\nneg\nneu\npos\ncompound\nProductId\nUserId\nProfileName\nHelpfulnessNumerator\nHelpfulnessDenominator\nScore\nTime\nSummary\nText\nReviewLength\n\n\n\n\n0\n1\n0.000\n0.695\n0.305\n0.9441\nB001E4KFG0\nA3SGXH7AUHU8GW\ndelmartian\n1\n1\n5\n1303862400\nGood Quality Dog Food\nI have bought several of the Vitality canned d...\n263\n\n\n1\n2\n0.138\n0.862\n0.000\n-0.5664\nB00813GRG4\nA1D87F6ZCVE5NK\ndll pa\n0\n0\n1\n1346976000\nNot as Advertised\nProduct arrived labeled as Jumbo Salted Peanut...\n190\n\n\n2\n3\n0.091\n0.754\n0.155\n0.8265\nB000LQOCH0\nABXLMWJIXXAIN\nNatalia Corres \"Natalia Corres\"\n1\n1\n4\n1219017600\n\"Delight\" says it all\nThis is a confection that has been around a fe...\n509\n\n\n3\n4\n0.000\n1.000\n0.000\n0.0000\nB000UA0QIQ\nA395BORC6FGVXV\nKarl\n3\n3\n2\n1307923200\nCough Medicine\nIf you are looking for the secret ingredient i...\n219\n\n\n4\n5\n0.000\n0.552\n0.448\n0.9468\nB006K2ZZ7K\nA1UQRSCLF8GW1T\nMichael D. Bigham \"M. Wassir\"\n0\n0\n5\n1350777600\nGreat taffy\nGreat taffy at a great price. There was a wid...\n140\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n996\n0.026\n0.716\n0.257\n0.9788\nB006F2NYI2\nA1D3F6UI1RTXO0\nSwopes\n1\n1\n5\n1331856000\nHot & Flavorful\nBLACK MARKET HOT SAUCE IS WONDERFUL.... My hus...\n477\n\n\n996\n997\n0.000\n0.786\n0.214\n0.9309\nB006F2NYI2\nAF50D40Y85TV3\nMike A.\n1\n1\n5\n1328140800\nGreat Hot Sauce and people who run it!\nMan what can i say, this salsa is the bomb!! i...\n305\n\n\n997\n998\n0.000\n0.673\n0.327\n0.9634\nB006F2NYI2\nA3G313KLWDG3PW\nkefka82\n1\n1\n5\n1324252800\nthis sauce is the shiznit\nthis sauce is so good with just about anything...\n265\n\n\n998\n999\n0.063\n0.874\n0.062\n-0.0129\nB006F2NYI2\nA3NIDDT7E7JIFW\nV. B. Brookshaw\n1\n2\n1\n1336089600\nNot Hot\nNot hot at all. Like the other low star review...\n280\n\n\n999\n1000\n0.032\n0.928\n0.041\n-0.1027\nB006F2NYI2\nA132DJVI37RB4X\nScottdrum\n2\n5\n2\n1332374400\nNot hot, not habanero\nI have to admit, I was a sucker for the large ...\n563\n\n\n\n\n1000 rows × 15 columns"
  },
  {
    "objectID": "posts/yt2/index.html#plotting-vader-results",
    "href": "posts/yt2/index.html#plotting-vader-results",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "2.3. Plotting VADER Results",
    "text": "2.3. Plotting VADER Results\n\nax = sns.barplot(data=vaders,\n                 x = 'Score',\n                 y = 'compound',\n                 palette = color_pal)\nax.set_title('Compound Score by Amazon Star Review')\n\nText(0.5, 1.0, 'Compound Score by Amazon Star Review')\n\n\n\n\n\n\n\n\n\nUnsuprisingly there is a positive correlation between review score (stars out of five) and the compound score from VADER.\n\nfig, axs = plt.subplots(1, 3, figsize=(15,5))\nsns.barplot(data=vaders, x = 'Score', y = 'pos', ax=axs[0], palette = color_pal)\naxs[0].set_title('Positive Score')\nsns.barplot(data=vaders, x = 'Score', y = 'neu', ax=axs[1], palette = color_pal)\naxs[1].set_title('Neutral Score')\nsns.barplot(data=vaders, x = 'Score', y = 'neg', ax=axs[2], palette = color_pal)\naxs[2].set_title('Negative Score')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/ng1/index.html",
    "href": "posts/ng1/index.html",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nfrom PIL import Image\n\n%matplotlib inline\nprint(\"Python version:\")\n!python --version\n\nPython version:\nPython 3.11.4\nTable of contents\n1. DATA\n1.1.1. IMPORTANT REMARK\n2. THE SIGMOID\n3. LOGISTIC REGRESSION AND GRADIENT DESCENT\n3.1. Mathematical expression of the algorithm\n3.2. Vectorization\n3.3. Gradient Descent\n4. THE UNREGULARIZED MODEL\n4.1. Helper Functions\n4.2. Combining into logistic regression\n4.3. Errors and learning curves\n4.4. Animal Testing\n5. THE L2-REGULARIZED MODEL\n5.1. Regularizing the helper functions\n5.2. Combining into regularized logistic regression\n5.3. Tuning the regularization parameter\n5.4. More animal testing\nThe data and basic architecture of this post is taken from an exercise in the first course of Andrew Ng’s Deep Learning Specialization:\nhttps://www.coursera.org/specializations/deep-learning"
  },
  {
    "objectID": "posts/ng1/index.html#mathematical-expression-of-the-algorithm",
    "href": "posts/ng1/index.html#mathematical-expression-of-the-algorithm",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "3.1. Mathematical expression of the algorithm",
    "text": "3.1. Mathematical expression of the algorithm\nFor one training example \\(\\left(x^{(i)},y^{(i)}\\right)\\in \\mathbb{R}^n \\times \\{0,1\\}\\) in the training set \\(\\left\\{ \\left(x^{(i)} , y^{(i)} \\right) \\right\\} _{i=1}^{m}\\) and a choice of parameters \\(w\\in\\mathbb{R}^{n}\\,\\), \\(b\\in\\mathbb{R}\\), forward propagation consists of computing the prediction \\(\\hat y^{(i)}\\in(0,1)\\) of the model as \\(x \\mapsto \\left(w\\cdot x + b\\right) \\mapsto \\sigma\\left( w\\cdot x + b \\right)\\). That is,\n\\[z^{(i)} = w \\cdot x^{(i)} + b\\]\n\\[\\hat{y}^{(i)} = a^{(i)} = \\sigma\\left(z^{(i)}\\right)\\]\nwhere \\(\\sigma: \\mathbb{R} \\to \\left(0,1\\right)\\) is the sigmoid defined above, \\(n\\) is the number of features (length of any of the vectors \\(x\\)) and \\(m = m_{\\text{train}}\\) is the number of training examples. The scalar \\(a^{(i)}\\) is the activation of the perceptron, which is numerically equal to the output \\(\\hat y^{(i)}\\) for a neural network with no hidden layers such as logistic regression.\nThe prediction \\(\\hat{y}^{(i)}\\in \\left(0,1\\right)\\) is interpreted as the probability that \\(x^{(i)}\\) is in class 1 (i.e. is an image of a cat)\n\\[\\hat{y}^{(i)} = \\mathbb{P}\\left( y^{(i)}=1 \\,|\\, x^{(i)} \\,; \\,w, b \\right)\\]\nWe can extract a binary prediction in \\(\\{0,1\\} \\equiv \\{\\text{non-cat},\\text{cat}\\}\\) from the prediction \\(\\hat{y}^{(i)}\\) by applying a threshold \\[y^{(i)}_{\\text{pred}} = \\mathbb{1} {\\left\\{a^{(i)} &gt; 0.5\\right\\}} = \\begin{cases}\n      1 & \\text{if}\\ a^{(i)} &gt; 0.5 \\\\\n      0 & \\text{otherwise}\n    \\end{cases}\n\\]\nSuch a threshold can be implemented in code using, for example, numpy.round.\nTraining the model consists of using the training data to find parameters \\(w,\\, b\\) solving the optimization problem \\[\n\\min_{w\\in\\mathbb{R}^n,\\,b\\in\\mathbb{R}} J(w, b)\n\\] where \\(J = J(w,b): \\mathbb{R}^n \\times \\mathbb{R} \\rightarrow [0,\\infty)\\) is the cost function defined in terms of a loss function \\(\\mathcal{L}\\). The loss function \\(\\mathcal{L}\\) measures the error between the model’s prediction \\(\\hat y^{(i)} = a^{(i)}\\in(0,1)\\) for one of the training examples \\(x^{(i)}\\in\\mathbb{R}^n\\) and the true label \\(y^{(i)}\\in\\{0,1\\}.\\)\nWe use the binary cross entropy (negative log-loss) loss function \\(\\mathcal{L}\\), defined as\n\\[\\begin{align*}\n\\mathcal{L}\\left(a, y\\right) &=  - y  \\log a - \\left(1-y \\right)  \\log\\left(1-a\\right)\\\\[0.2cm]\n&= \\begin{cases}\n- \\log\\left(a\\right) & \\text{if } y = 1 \\\\\n- \\log\\left(1 - a\\right) & \\text{if } y = 0\n\\end{cases}\n\\end{align*}\\]\nThe (unregularized) cost \\(J\\) is then computed by summing over all training examples:\n\\[\\begin{align*}\nJ(w,b) &= \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}\\left(a^{(i)}, y^{(i)}\\right)\\\\[0.2cm]\n  &= -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)}  \\log\\left(a^{(i)}\\right) + \\left(1-y^{(i)} \\right)  \\log\\left(1-a^{(i)}\\right) \\right]\n\\end{align*}\\]\nTo counteract overfitting of the model to the training data one can include a regularization term in the cost function to penalise large weights \\(w\\). For example, with an \\(L_2\\)-regularization term the cost \\(J\\) becomes\n\\[J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}\\left(a^{(i)}, y^{(i)}\\right) + \\frac{\\lambda}{2m}||w||_2^2\\]\nwhere \\(\\lambda \\geq 0\\) is the regularization parameter and \\(||w||_2^2 = w_1^2 + w_2^2 + \\dots + w_n^2 = w \\cdot w\\) denotes the squared Euclidean norm of \\(w\\).\nBackpropagation consists of computing the derivatives\n\\[\\frac{\\partial J (w,b)}{\\partial w_j},\\, \\frac{\\partial J(w,b)}{\\partial b}\\]\nfor use when optimizing \\((w,b)\\) using gradient descent. It is easy to compute that in the unregularized case:\n\\[\\begin{align*}\n\\frac{\\partial J (w,b)}{\\partial w_j} &= \\frac{1}{m}\\sum_{i=1}^m \\left(a^{(i)} - y^{(i)}\\right)x_j^{(i)}\\\\\n\\frac{\\partial J(w,b)}{\\partial b} &= \\frac{1}{m}\\sum_{i=1}^m \\left(a^{(i)} - y^{(i)}\\right)\n\\end{align*}\\]\nWhile in the \\(L_2\\)-regularized case:\n\\[\\begin{align*}\n\\frac{\\partial J (w,b)}{\\partial w_j} &= \\frac{1}{m}\\sum_{i=1}^m \\left(a^{(i)} - y^{(i)}\\right)x_j^{(i)} +\\frac{\\lambda}{m}w_j  \\\\\n\\frac{\\partial J(w,b)}{\\partial b} &= \\frac{1}{m}\\sum_{i=1}^m \\left(a^{(i)} - y^{(i)}\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/ng1/index.html#vectorization",
    "href": "posts/ng1/index.html#vectorization",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "3.2. Vectorization",
    "text": "3.2. Vectorization\nLooping over all the \\(m\\) training examples \\(\\left (x^{(i)},y^{(i)} \\right)\\) in turn to calculate \\(\\hat{y}^{(i)} = a^{(i)} = \\sigma\\left(z^{(i)}\\right) = \\sigma\\left( w \\cdot x^{(i)} + b\\right)\\) and \\(\\mathcal{L}\\left(a^{(i)}, y^{(i)}\\right)\\) is computationally inefficient if \\(m\\) is large \\(\\left(\\text{e.g.}\\,\\, m\\sim10^6\\right)\\) as is common in modern industry applications.\nBy turning to a so called vectorized implementation we can take advantage of NumPy’s powerful numerical linear algebra capabilities to implement forward propagation more efficiently.\nDefine vectors \\(Z = \\left( z^{(1)}, z^{(2)}, \\dots, z^{(m)} \\right) \\in \\mathbb{R}^m\\) and \\(A = \\left( a^{(1)}, a^{(2)}, \\dots, a^{(m)} \\right) \\in \\mathbb{R}^m\\). Define the \\(n\\,\\times\\,m\\) matrix \\(X\\) with \\(i^{\\text{th}}\\) column \\(x^{(i)}\\). That is,\n\\[\\begin{equation}\nX = \\begin{bmatrix}\n    | & | & \\cdots & | \\\\\n    x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n    | & | & \\cdots & |\n\\end{bmatrix}\\in \\mathcal{M}_{n,m} \\left(\\mathbb{R}\\right)\n\\end{equation}\\]\nThen\n\\[\\begin{align*}\nw^T X + \\left(b,b,\\dots,b\\right) &= \\left( w^T x^{(1)}+b, \\,w^T x^{(2)}+b, \\dots ,\\,w^T x^{(n)}+b \\right)\\\\[0.2cm]\n                                 &= \\left( z^{(1)}, z^{(2)}, \\dots, z^{(m)} \\right)\\\\[0.2cm]\n                                 &= Z\n\\end{align*}\\]\nSo if \\(\\mathbf{b} = \\left(b,b,\\dots,b\\right)\\) then \\(Z = w^T X + \\mathbf{b}\\).\nWe can implement this in code as Z = np.dot(w.T,X) + b where we have taken advantage of python broadcasting to add the scalar b to the array np.dot(w.T,X). NumPy then interprets this addition as element-wise. We then have \\[A = \\left( a^{(1)}, a^{(2)}, \\dots, a^{(m)} \\right) = \\sigma (Z)\\] since the sigmoid \\(\\sigma\\) acts on arrays element-wise.\nA = sigmoid(np.dot(w.T,X) + b) is a computationally efficient implementation of forward propagation across the entire training set at once. In particular, this is more efficient than using a for loop to iterate over each training example in turn."
  },
  {
    "objectID": "posts/ng1/index.html#gradient-descent",
    "href": "posts/ng1/index.html#gradient-descent",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "3.3. Gradient Descent",
    "text": "3.3. Gradient Descent\nThe optimization problem \\[\n\\min_{w\\in\\mathbb{R}^n,\\,b\\in\\mathbb{R}} J(w, b)\n\\]\nis numerically solved by gradient descent. For our purposes, gradient descent comprises of iteratively and simultaneously updating \\(b\\) and \\(w\\) according to\n\\[\\begin{align*}\nw_j &\\mapsto w_j - \\alpha \\frac{\\partial J(w,b)}{\\partial w_j}\\\\[0.1cm]\nb &\\mapsto b - \\alpha \\frac{\\partial J(w,b)}{\\partial b}\n\\end{align*}\\]\nwhere \\(\\alpha &lt;&lt; 1\\) is a fixed hyperparameter called the learning rate. Another hyperparameter introduced with gradient descent is the number of iterations num_interations to repeat this updating process."
  },
  {
    "objectID": "posts/ng1/index.html#helper-functions",
    "href": "posts/ng1/index.html#helper-functions",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "4.1. Helper Functions",
    "text": "4.1. Helper Functions\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    Create a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (int)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias) of type float\n    \"\"\"\n    w = np.zeros((dim,1))\n    b = float(0)\n    \n    return w, b\n\n\ndef forward_propagate(w, b, X):\n    \"\"\"\n    Implements forward propogation across the training set X, computing the activation matrix A \n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n\n    Returns:\n    A -- activation of the neuron, numpy array of size (num_px * num_px * 3, number of examples)\n    \"\"\"\n    A = sigmoid(np.dot(w.T,X)+b)\n    return A\n\n\ndef compute_cost(w, b, X, Y):\n    '''\n    Computes the negative log-likelihood cost J(w,b) across the training set    \n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    \n    Returns:\n    cost -- negative log-likelihood cost for logistic regression\n    '''\n    m = X.shape[1]\n    A = forward_propagate(w, b, X)\n    cost = (-1/m) * (np.dot(Y, np.log(A).T) + np.dot((1 - Y), np.log(1 - A).T))\n    cost = np.squeeze(np.array(cost))  \n    return cost\n\n\ndef backward_propagate(w, b, X, Y):\n    '''\n    Calculates the gradient of the cost function J with respect to the parameters w, b\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    \n    Returns:\n    grads -- dictionary containing the gradients of J w.r.t. the weights and bias\n            (dw -- gradient of the loss with respect to w, thus same shape as w)\n            (db -- gradient of the loss with respect to b, thus same shape as b)\n    '''\n    m = X.shape[1]\n    \n    A = forward_propagate(w, b, X)\n    \n    dw = (1/m)*np.dot(X,(A-Y).T)\n    db = (1/m)*np.sum(A-Y)\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads\n\n\ndef optimize(w, b, X, Y, num_iterations=3000, learning_rate=0.005, print_cost=False):\n    \"\"\"\n    Optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        cost = compute_cost(w, b, X, Y)\n        grads = backward_propagate(w, b, X, Y)\n        \n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n \n        if i % 100 == 0:\n            costs.append(cost)\n        \n            # Print the cost every 100 training iterations\n            if print_cost:\n                print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs\n\n\ndef predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) outputted from `optimize`\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1, m))\n    \n    w = w.reshape(X.shape[0], 1)\n\n    A = sigmoid(np.dot(w.T,X)+b) \n\n    for i in range(A.shape[1]):        \n        Y_prediction[0,i] = np.round(A[0,i]) # Applies a threshold of 0.5\n    \n    return Y_prediction"
  },
  {
    "objectID": "posts/ng1/index.html#combining-into-logistic-regression",
    "href": "posts/ng1/index.html#combining-into-logistic-regression",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "4.2. Combining into logistic regression",
    "text": "4.2. Combining into logistic regression\nCombining the previously defined functions intialize_with_zeros, propagate, optimize and predict into the logistic regression model\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations=3000, learning_rate=0.005, print_cost=False):\n    \"\"\"\n    Combines the helper functions to construct the unregularized logistic regression model\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to True to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    w, b = initialize_with_zeros(X_train.shape[0])\n    \n    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost=False)\n    \n    w = params['w']\n    b = params['b']\n    \n    Y_prediction_test = predict(w,b,X_test)\n    Y_prediction_train = predict(w,b,X_train)\n\n    # Print train/test Errors\n    if print_cost:\n        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d\n\n\nlogistic_regression_model = model(train_set_x, \n                                  train_set_y, \n                                  test_set_x,\n                                  test_set_y,\n                                  num_iterations=3000,\n                                  learning_rate=0.005,\n                                  print_cost=True)\n\ntrain accuracy: 99.52153110047847 %\ntest accuracy: 68.0 %\n\n\nThe model accuractely classified &gt;99% of the images in the training set and 70% of the images in the test set, suggesting that we are experiences overfitting to our training data."
  },
  {
    "objectID": "posts/ng1/index.html#errors-and-learning-curves",
    "href": "posts/ng1/index.html#errors-and-learning-curves",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "4.3. Errors and learning curves",
    "text": "4.3. Errors and learning curves\n\n# Example of a 'cat' that was inaccuractely classified as 'not-cat' (False Negative)\nindex = 10\nplt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", predicted as \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 1, predicted as \"non-cat\" picture.\n\n\n\n\n\n\n\n\n\n\n# Example of a 'not-cat' that was inaccuractely classified as 'cat' (False Positive)\nindex = 34\nplt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", predicted as \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 0, predicted as \"cat\" picture.\n\n\n\n\n\n\n\n\n\n\n# Plot learning curve (with costs)\ncosts = np.squeeze(logistic_regression_model['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate = \" + str(logistic_regression_model[\"learning_rate\"]))\nplt.show()"
  },
  {
    "objectID": "posts/ng1/index.html#animal-testing",
    "href": "posts/ng1/index.html#animal-testing",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "4.4. Animal Testing",
    "text": "4.4. Animal Testing\n\ndef is_cat(image_str):\n    '''\n    Applies the trained logistic regression model to predict if an inputted image is a cat (y=1) or a non-cat (y=0)\n    \n    Arguments:\n    image_str - a string encoding the file name of the .jpg file, \n                e.g. 'cat.jpg' if cat.jpg is the file name of an image saved in the same directory as this notebook.\n    Returns:\n    None\n    '''\n    # Read the original image\n    original_image = np.array(Image.open(image_str))\n    \n    # Show the original image\n    plt.subplot(1, 2, 1)\n    plt.imshow(original_image)\n    plt.title(\"Original Image\")\n    \n    # Resize the image to 64x64\n    resized_image = np.array(Image.open(image_str).resize((num_px, num_px)))\n    \n    # Show the resized image\n    plt.subplot(1, 2, 2)\n    plt.imshow(resized_image)\n    plt.title(\"Resized Image (64x64)\")\n    \n    # Standardize and flatten the resized image \n    image = resized_image / 255.\n    image = image.reshape((1, num_px * num_px * 3)).T\n    \n    # Predict label using training logistic regression model\n    my_predicted_image = predict(logistic_regression_model[\"w\"], logistic_regression_model[\"b\"], image)\n    \n    # Print the prediction for the resized image\n    print(\"y = \" + str(int(np.squeeze(my_predicted_image))) + \", the model predicts this is a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n\nLet’s test the function is_cat on an image of my own cat, William.\nHe’s middle-aged and overweight.\n\nis_cat('william.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\n\n\n\n\nUnsuprising. He’s always been a disappointment.\nObserving on some other images in my camera roll:\n\nis_cat('flora.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat('ginger_greek_cat.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat('william_yawning.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat('cambridge_cat.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat('lexi.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat('toby.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat('mr_president.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat('bojack.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat('american_football.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\n\n\n\n\nOh dear. Time to do some tuning."
  },
  {
    "objectID": "posts/ng1/index.html#regularizing-the-helper-functions",
    "href": "posts/ng1/index.html#regularizing-the-helper-functions",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "5.1. Regularizing the helper functions",
    "text": "5.1. Regularizing the helper functions\n\ndef compute_cost_regularized(w, b, X, Y, lambda_):\n    '''\n    Computes the L2-regularized negative log-likelihood cost J(w,b) across the training set\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    lambda_ -- regularization hyperparameter\n    \n    Returns:\n    cost -- L2-regularized negative log-likelihood cost for logistic regression\n    '''\n    m = X.shape[1]\n    reg_cost = compute_cost(w, b, X, Y) + (lambda_/m)*(np.linalg.norm(w)**2)\n    \n    return reg_cost\n\n\ndef backward_propagate_regularized(w, b, X, Y, lambda_):\n    '''\n    Calculates the gradient of the L2-regularized cost function J with respect to the parameters w, b\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    lambda_ -- regularization hyperparameter\n    \n    Returns:\n    grads -- dictionary containing the gradients of J w.r.t. the weights and bias\n            (dw -- gradient of the loss with respect to w, thus same shape as w)\n            (db -- gradient of the loss with respect to b, thus same shape as b)\n    '''\n    m = X.shape[1]\n    grads = backward_propagate(w, b, X, Y)\n    grads['dw'] = grads['dw'] + (lambda_/m)*w\n    \n    return grads\n\n\ndef optimize_regularized(w, b, X, Y, num_iterations=3000, learning_rate=0.005, lambda_=0, print_cost=False):\n    \"\"\"\n    Optimizes w and b by running a gradient descent algorithm on the regularized cost function\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    lambda_ -- regularization hyperparameter\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        cost = compute_cost_regularized(w, b, X, Y, lambda_)\n        grads = backward_propagate_regularized(w, b, X, Y, lambda_)\n        \n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n \n        if i % 100 == 0:\n            costs.append(cost)\n        \n            # Print the cost every 100 training iterations\n            if print_cost:\n                print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs"
  },
  {
    "objectID": "posts/ng1/index.html#combining-into-regularized-logistic-regression",
    "href": "posts/ng1/index.html#combining-into-regularized-logistic-regression",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "5.2. Combining into regularized logistic regression",
    "text": "5.2. Combining into regularized logistic regression\n\ndef regularized_model(X_train, Y_train, X_test, Y_test, num_iterations=3000, learning_rate=0.5, lambda_=0, print_cost=False):\n    \"\"\"\n    Combines the helper functions to construct the regularized model\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    lambda_ -- regularization hyperparameter\n    print_cost -- Set to True to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    w, b = initialize_with_zeros(X_train.shape[0])\n    \n    params, grads, costs = optimize_regularized(w, b, X_train, Y_train, num_iterations, learning_rate, lambda_, print_cost=False)\n    \n    w = params['w']\n    b = params['b']\n    \n    Y_prediction_test = predict(w,b,X_test)\n    Y_prediction_train = predict(w,b,X_train)\n\n    # Print train/test Errors\n    if print_cost:\n        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d\n\n\nlogistic_regression_model_regularized = regularized_model(train_set_x,\n                                                          train_set_y,\n                                                          test_set_x,\n                                                          test_set_y,\n                                                          num_iterations=3000,\n                                                          learning_rate=0.005,\n                                                          lambda_=100,\n                                                          print_cost=True)\n\ntrain accuracy: 89.47368421052632 %\ntest accuracy: 74.0 %"
  },
  {
    "objectID": "posts/ng1/index.html#tuning-the-regularization-parameter",
    "href": "posts/ng1/index.html#tuning-the-regularization-parameter",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "5.3. Tuning the regularization parameter",
    "text": "5.3. Tuning the regularization parameter\n\ndef tune_lambda(lambdas):\n    '''\n    Trains the regularized model with a choice of different regularization hyperparameters lambda_\n    \n    Arguments:\n    lambdas - a list of regularization hyperparameters lambda_\n    \n    Returns:\n    None\n    '''\n    for lambda_ in lambdas:\n        print(f\"Training a model with regularization parameter lambda = {lambda_}\")\n        regularized_model(train_set_x,\n                          train_set_y,\n                          test_set_x,\n                          test_set_y,\n                          num_iterations=3000,\n                          learning_rate=0.005,\n                          lambda_=lambda_,\n                          print_cost=True)\n        print(\"\\n\") \n\n\nlambdas = [0, 50, 100, 150, 200, 250]\ntune_lambda(lambdas)\n\nTraining a model with regularization parameter lambda = 0\ntrain accuracy: 99.52153110047847 %\ntest accuracy: 68.0 %\n\n\nTraining a model with regularization parameter lambda = 50\ntrain accuracy: 95.69377990430623 %\ntest accuracy: 74.0 %\n\n\nTraining a model with regularization parameter lambda = 100\ntrain accuracy: 89.47368421052632 %\ntest accuracy: 74.0 %\n\n\nTraining a model with regularization parameter lambda = 150\ntrain accuracy: 84.21052631578948 %\ntest accuracy: 80.0 %\n\n\nTraining a model with regularization parameter lambda = 200\ntrain accuracy: 75.59808612440192 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 250\ntrain accuracy: 72.72727272727273 %\ntest accuracy: 82.0 %\n\n\n\n\n\nlambdas = [130, 140, 150, 160, 170]\ntune_lambda(lambdas)\n\nTraining a model with regularization parameter lambda = 130\ntrain accuracy: 87.08133971291866 %\ntest accuracy: 80.0 %\n\n\nTraining a model with regularization parameter lambda = 140\ntrain accuracy: 87.08133971291866 %\ntest accuracy: 80.0 %\n\n\nTraining a model with regularization parameter lambda = 150\ntrain accuracy: 84.21052631578948 %\ntest accuracy: 80.0 %\n\n\nTraining a model with regularization parameter lambda = 160\ntrain accuracy: 81.33971291866028 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 170\ntrain accuracy: 80.38277511961722 %\ntest accuracy: 82.0 %\n\n\n\n\n\nlambdas = [150, 152, 154, 156, 158, 160]\ntune_lambda(lambdas)\n\nTraining a model with regularization parameter lambda = 150\ntrain accuracy: 84.21052631578948 %\ntest accuracy: 80.0 %\n\n\nTraining a model with regularization parameter lambda = 152\ntrain accuracy: 83.73205741626793 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 154\ntrain accuracy: 83.25358851674642 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 156\ntrain accuracy: 83.25358851674642 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 158\ntrain accuracy: 82.29665071770334 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 160\ntrain accuracy: 81.33971291866028 %\ntest accuracy: 82.0 %"
  },
  {
    "objectID": "posts/ng1/index.html#more-animal-testing",
    "href": "posts/ng1/index.html#more-animal-testing",
    "title": "Logistic Regression with Gradient Descent and L2-Regularization",
    "section": "5.4. More animal testing",
    "text": "5.4. More animal testing\n\ntuned_regularized_model = regularized_model(train_set_x,\n                                            train_set_y,\n                                            test_set_x,\n                                            test_set_y,\n                                            num_iterations=3000,\n                                            learning_rate=0.005,\n                                            lambda_=152,\n                                            print_cost=True)\n\ntrain accuracy: 83.73205741626793 %\ntest accuracy: 82.0 %\n\n\n\ndef is_cat_tuned_regularized(image_str):\n    '''\n    Applies the trained regularized & tuned logistic regression model to predict if an inputted image is a cat (y=1) or a non-cat (y=0)\n    \n    Arguments:\n    image_str - a string encoding the file name of the .jpg file, \n                e.g. 'cat.jpg' if cat.jpg is the file name of an image saved in the same directory as this notebook.\n    Returns:\n    None\n    '''\n    # Read the original image\n    original_image = np.array(Image.open(image_str))\n    \n    # Show the original image\n    plt.subplot(1, 2, 1)\n    plt.imshow(original_image)\n    plt.title(\"Original Image\")\n    \n    # Resize the image to 64x64\n    resized_image = np.array(Image.open(image_str).resize((num_px, num_px)))\n    \n    # Show the resized image\n    plt.subplot(1, 2, 2)\n    plt.imshow(resized_image)\n    plt.title(\"Resized Image (64x64)\")\n    \n    # Standardize and flatten the resized image \n    image = resized_image / 255.\n    image = image.reshape((1, num_px * num_px * 3)).T\n    \n    # Predict label using training logistic regression model\n    my_predicted_image = predict(tuned_regularized_model[\"w\"], tuned_regularized_model[\"b\"], image)\n    \n    # Print the prediction for the resized image\n    print(\"y = \" + str(int(np.squeeze(my_predicted_image))) + \", the model predicts this is a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n\n\nis_cat_tuned_regularized('william.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat_tuned_regularized('flora.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat_tuned_regularized('ginger_greek_cat.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat_tuned_regularized('william_yawning.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat_tuned_regularized('cambridge_cat.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat_tuned_regularized('lexi.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat_tuned_regularized('toby.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat_tuned_regularized('mr_president.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat_tuned_regularized('bojack.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\n\nis_cat_tuned_regularized('american_football.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\n\n\n\nAn improvement to be sure, but the model is still far from perfect. It seems that we have improved the validation accuracy at the expense of increasing the prevelence of false positives.\nThis might be because the validation set has a lot of cat images and we are overfitting to it, or because the images I am testing on are drawn from a different probability distribution than the images in the validation set.\nWe could investigate further by performing a more rigorous cross-validation process.\nAdding hidden layers to the neural network would result in a more expressive model capable of telling apart cats and anthropomorphic cartoon horses or midwest emo album covers. We will explore these possiblities in later blog posts."
  },
  {
    "objectID": "posts/k1/index.html",
    "href": "posts/k1/index.html",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import randint, uniform\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nfrom lightgbm import LGBMClassifier\nimport sys\nprint(\"Python version:\")\nprint(sys.version)\n\nPython version:\n3.11.4 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 13:38:37) [MSC v.1916 64 bit (AMD64)]\nTable of contents\n1. Data imports and exploration\n2. Missing values and feature engineering\n2.1. Filling HomePlanet, Destination and VIP\n2.2. Filling Age and the expenditure features\n2.3. New features - AgeGroup, CabinSide and GroupSize\n2.4. Finishing preprocessing - dropping features and splitting into train and test sets\n3. Tuning a LGBMClassifier with RandomizedSearchCV\n4. Fitting the best LGBMClassifier to the train set\n5. Using the trained model to predict on the test set\n6. Further Directions"
  },
  {
    "objectID": "posts/k1/index.html#filling-homeplanet-destination-and-vip",
    "href": "posts/k1/index.html#filling-homeplanet-destination-and-vip",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "2.1. Filling HomePlanet, Destination and VIP",
    "text": "2.1. Filling HomePlanet, Destination and VIP\n\ndf_train['HomePlanet'].value_counts()\n\nHomePlanet\nEarth     4602\nEuropa    2131\nMars      1759\nName: count, dtype: int64\n\n\n\ndf_test['HomePlanet'].value_counts()\n\nHomePlanet\nEarth     2263\nEuropa    1002\nMars       925\nName: count, dtype: int64\n\n\nThe mode for HomePlanet for both the train and test sets is “Earth”, so we use this to fill the null values\n\ndata['HomePlanet'] = data['HomePlanet'].fillna('Earth')\n\n\ndf_train['Destination'].value_counts()\n\nDestination\nTRAPPIST-1e      5915\n55 Cancri e      1800\nPSO J318.5-22     796\nName: count, dtype: int64\n\n\n\ndf_test['Destination'].value_counts()\n\nDestination\nTRAPPIST-1e      2956\n55 Cancri e       841\nPSO J318.5-22     388\nName: count, dtype: int64\n\n\nThe mode for Destination for both the train and test sets is “TRAPPIST-1e”, so we use this to fill the null values\n\ndata['Destination'] = data['Destination'].fillna('TRAPPIST-1e')\n\n\ndf_train['VIP'].value_counts()\n\nVIP\nFalse    8291\nTrue      199\nName: count, dtype: int64\n\n\n\ndf_test['VIP'].value_counts()\n\nVIP\nFalse    4110\nTrue       74\nName: count, dtype: int64\n\n\n\ndata['VIP'] = data['VIP'].fillna(False)\n\n\ndata.isna().sum()\n\nPassengerId            0\nHomePlanet             0\nCryoSleep              0\nCabin                299\nDestination            0\nAge                  270\nVIP                    0\nRoomService          170\nFoodCourt            180\nShoppingMall         175\nSpa                  177\nVRDeck               177\nName                 294\nTransported         4277\nTotalExpenditure       0\ndtype: int64"
  },
  {
    "objectID": "posts/k1/index.html#filling-age-and-the-expenditure-features",
    "href": "posts/k1/index.html#filling-age-and-the-expenditure-features",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "2.2. Filling Age and the expenditure features",
    "text": "2.2. Filling Age and the expenditure features\nTo fill the remaining null values in Age and Expenses_features we will use the median, to reduce the influence of outliers. This requires seperating data back into constituent train and test sets, to avoid data leakage.\n\ntrain = data[:len(df_train)]\ntest = data[len(df_train):].drop('Transported', axis=1)\n\n\nprint(len(train) == len(df_train))\n\nTrue\n\n\n\ntrain.loc[:, 'Age'] = train['Age'].fillna(train['Age'].median())\ntest.loc[:, 'Age'] = test['Age'].fillna(test['Age'].median())\n\n\ntrain.loc[:,Expenses_features] = train[Expenses_features].fillna(train[Expenses_features].median())\ntest.loc[:,Expenses_features] = test[Expenses_features].fillna(test[Expenses_features].median())\n\n\nprint('Remaining null values in train:\\n')\nprint(train.isna().sum())\nprint('\\nRemaining null values in test:\\n')\nprint(test.isna().sum())\n\nRemaining null values in train:\n\nPassengerId           0\nHomePlanet            0\nCryoSleep             0\nCabin               199\nDestination           0\nAge                   0\nVIP                   0\nRoomService           0\nFoodCourt             0\nShoppingMall          0\nSpa                   0\nVRDeck                0\nName                200\nTransported           0\nTotalExpenditure      0\ndtype: int64\n\nRemaining null values in test:\n\nPassengerId           0\nHomePlanet            0\nCryoSleep             0\nCabin               100\nDestination           0\nAge                   0\nVIP                   0\nRoomService           0\nFoodCourt             0\nShoppingMall          0\nSpa                   0\nVRDeck                0\nName                 94\nTotalExpenditure      0\ndtype: int64\n\n\nRedefine data as the concatenation of train and test\n\ndata = pd.concat([train,test], axis=0)"
  },
  {
    "objectID": "posts/k1/index.html#new-features---agegroup-cabinside-and-groupsize",
    "href": "posts/k1/index.html#new-features---agegroup-cabinside-and-groupsize",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "2.3. New features - AgeGroup, CabinSide and GroupSize",
    "text": "2.3. New features - AgeGroup, CabinSide and GroupSize\nCreate a new feature AgeGroup by binning the Age feature into 8 different categories.\n\ndata['Age'].max()\n\n79.0\n\n\n\ndata['AgeGroup'] = 0\nfor i in range(8):\n    data.loc[(data.Age &gt;= 10*i) & (data.Age &lt; 10*(i + 1)), 'AgeGroup'] = i\n\n\ndata['AgeGroup'].value_counts()\n\nAgeGroup\n2    4460\n3    2538\n1    2235\n4    1570\n0     980\n5     809\n6     312\n7      66\nName: count, dtype: int64\n\n\nCreate a dummy feature Group by extracting the first character from the PassengerId column. Use Group to define a new feature GroupSize indicating how many people are in the passengers group. Drop the feature Group as it has too many values to be useful.\n\ndata['Group'] = data['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\ndata['GroupSize'] = data['Group'].map(lambda x: data['Group'].value_counts()[x])\ndata = data.drop('Group', axis=1)\n\nCreate a new boolean feature Solo, indicating if a passenger is in a group just by themselves\n\ndata['Solo'] = (data['GroupSize'] == 1).astype(int)\n\nWe won’t use Cabin directly, but we engineer a new feature CabinSide by taking the last character of Cabin. “P” for port and “S” for starboard. To implement this we fill Cabin with a placeholder value.\n\ndata['Cabin'] = data['Cabin'].fillna('T/0/P')\n\n\ndata['CabinSide'] = data['Cabin'].apply(lambda x: x.split('/')[-1])"
  },
  {
    "objectID": "posts/k1/index.html#finishing-preprocessing---dropping-features-and-splitting-into-train-and-test-sets",
    "href": "posts/k1/index.html#finishing-preprocessing---dropping-features-and-splitting-into-train-and-test-sets",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "2.4. Finishing preprocessing - dropping features and splitting into train and test sets",
    "text": "2.4. Finishing preprocessing - dropping features and splitting into train and test sets\n\ndata = data.drop(['PassengerId','Cabin','Name'], axis=1)\n\n\ndata.isna().sum()\n\nHomePlanet             0\nCryoSleep              0\nDestination            0\nAge                    0\nVIP                    0\nRoomService            0\nFoodCourt              0\nShoppingMall           0\nSpa                    0\nVRDeck                 0\nTransported         4277\nTotalExpenditure       0\nAgeGroup               0\nGroupSize              0\nSolo                   0\nCabinSide              0\ndtype: int64\n\n\n\ndata\n\n\n\n\n\n\n\n\nHomePlanet\nCryoSleep\nDestination\nAge\nVIP\nRoomService\nFoodCourt\nShoppingMall\nSpa\nVRDeck\nTransported\nTotalExpenditure\nAgeGroup\nGroupSize\nSolo\nCabinSide\n\n\n\n\n0\nEuropa\nFalse\nTRAPPIST-1e\n39.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nFalse\n0.0\n3\n1\n1\nP\n\n\n1\nEarth\nFalse\nTRAPPIST-1e\n24.0\nFalse\n109.0\n9.0\n25.0\n549.0\n44.0\nTrue\n736.0\n2\n1\n1\nS\n\n\n2\nEuropa\nFalse\nTRAPPIST-1e\n58.0\nTrue\n43.0\n3576.0\n0.0\n6715.0\n49.0\nFalse\n10383.0\n5\n2\n0\nS\n\n\n3\nEuropa\nFalse\nTRAPPIST-1e\n33.0\nFalse\n0.0\n1283.0\n371.0\n3329.0\n193.0\nFalse\n5176.0\n3\n2\n0\nS\n\n\n4\nEarth\nFalse\nTRAPPIST-1e\n16.0\nFalse\n303.0\n70.0\n151.0\n565.0\n2.0\nTrue\n1091.0\n1\n1\n1\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4272\nEarth\nTrue\nTRAPPIST-1e\n34.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n0.0\n3\n2\n0\nS\n\n\n4273\nEarth\nFalse\nTRAPPIST-1e\n42.0\nFalse\n0.0\n847.0\n17.0\n10.0\n144.0\nNaN\n1018.0\n4\n1\n1\nP\n\n\n4274\nMars\nTrue\n55 Cancri e\n26.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n0.0\n2\n1\n1\nP\n\n\n4275\nEuropa\nFalse\nTRAPPIST-1e\n26.0\nFalse\n0.0\n2680.0\n0.0\n0.0\n523.0\nNaN\n3203.0\n2\n1\n1\nP\n\n\n4276\nEarth\nTrue\nPSO J318.5-22\n43.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n0.0\n4\n1\n1\nS\n\n\n\n\n12970 rows × 16 columns\n\n\n\n\ntrain = data[:len(df_train)]\ntest = data[len(df_train):].drop('Transported', axis=1)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nHomePlanet\nCryoSleep\nDestination\nAge\nVIP\nRoomService\nFoodCourt\nShoppingMall\nSpa\nVRDeck\nTransported\nTotalExpenditure\nAgeGroup\nGroupSize\nSolo\nCabinSide\n\n\n\n\n0\nEuropa\nFalse\nTRAPPIST-1e\n39.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nFalse\n0.0\n3\n1\n1\nP\n\n\n1\nEarth\nFalse\nTRAPPIST-1e\n24.0\nFalse\n109.0\n9.0\n25.0\n549.0\n44.0\nTrue\n736.0\n2\n1\n1\nS\n\n\n2\nEuropa\nFalse\nTRAPPIST-1e\n58.0\nTrue\n43.0\n3576.0\n0.0\n6715.0\n49.0\nFalse\n10383.0\n5\n2\n0\nS\n\n\n3\nEuropa\nFalse\nTRAPPIST-1e\n33.0\nFalse\n0.0\n1283.0\n371.0\n3329.0\n193.0\nFalse\n5176.0\n3\n2\n0\nS\n\n\n4\nEarth\nFalse\nTRAPPIST-1e\n16.0\nFalse\n303.0\n70.0\n151.0\n565.0\n2.0\nTrue\n1091.0\n1\n1\n1\nS\n\n\n\n\n\n\n\n\ntest.head()\n\n\n\n\n\n\n\n\nHomePlanet\nCryoSleep\nDestination\nAge\nVIP\nRoomService\nFoodCourt\nShoppingMall\nSpa\nVRDeck\nTotalExpenditure\nAgeGroup\nGroupSize\nSolo\nCabinSide\n\n\n\n\n0\nEarth\nTrue\nTRAPPIST-1e\n27.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n1\n1\nS\n\n\n1\nEarth\nFalse\nTRAPPIST-1e\n19.0\nFalse\n0.0\n9.0\n0.0\n2823.0\n0.0\n2832.0\n1\n1\n1\nS\n\n\n2\nEuropa\nTrue\n55 Cancri e\n31.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3\n1\n1\nS\n\n\n3\nEuropa\nFalse\nTRAPPIST-1e\n38.0\nFalse\n0.0\n6652.0\n0.0\n181.0\n585.0\n7418.0\n3\n1\n1\nS\n\n\n4\nEarth\nFalse\nTRAPPIST-1e\n20.0\nFalse\n10.0\n0.0\n635.0\n0.0\n0.0\n645.0\n2\n1\n1\nS\n\n\n\n\n\n\n\nThese are our final dataframes for the train and test set. We have engineered new features TotalExpenditure, AgeGroup, GroupSize, Solo and CabinSide. We have filled all null values, and are now nearly ready to train a model"
  },
  {
    "objectID": "posts/eh3/index.html",
    "href": "posts/eh3/index.html",
    "title": "Geographic Analysis of Charity Donors - Latest Leaflet Maps",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport folium\nfrom folium.plugins import MarkerCluster\nfrom branca.colormap import LinearColormap\n\nfrom geopy.distance import geodesic\n\npd.set_option('display.max_columns', None)\n\nTable of contents\n1. The Data\n2. ENGINEERING DISTANCE FROM EHSC\n3. VISUALISING DISTRIBUTION OF DONORS WITH FOLIUM\n4. REINTERPRETING AS A CLUSTER MAP\n5. REMARKS\n\n\n\ndf = pd.read_csv('data.csv').drop('DistanceFromEHSC', axis=1)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nLatitude\nLongitude\nNewsletter\nTransactions_LifetimeGiftsAmount\nTransactions_LifetimeGiftsNumber\nTransactions_AverageGiftAmount\nDonationFrequency\nDonationFrequencyActive\nTransactions_Months1To12GiftsAmount\nTransactions_Months1To12GiftsNumber\nmonthlyDonorMonths1to12\nTransactions_Months13To24GiftsAmount\nTransactions_Months13To24GiftsNumber\nmonthlyDonorMonths13to24\nTransactions_Months25To36GiftsAmount\nTransactions_Months25To36GiftsNumber\nmonthlyDonorMonths25to36\nTransactions_DateOfFirstGift\nTransactions_FirstGiftAmount\nTransactions_DateOfLastGift\nTransactions_LastGiftAmount\nmonthsSinceFirstDonation\nmonthsSinceLastDonation\nactiveMonths\nTransactions_DateOfHighestGift\nTransactions_HighestGiftAmount\nTransactions_DateOfLowestGift\nTransactions_LowestGiftAmount\n\n\n\n\n0\n52.961066\n-1.205200\n1\n419.66\n4\n104.915000\n0.133333\n0.148148\n152.89\n1\n0\n154.77\n1\n0\n97.19\n1\n0\n2021-10-23\n96.09\n2023-12-31\n148.05\n29\n3\n27\n2024-01-01\n148.44\n2021-10-20\n99.05\n\n\n1\n52.924105\n-1.216433\n1\n111.05\n3\n37.016667\n0.103448\n0.230769\n0.00\n0\n0\n50.43\n1\n0\n52.93\n1\n0\n2021-11-26\n48.27\n2022-11-17\n50.97\n28\n16\n13\n2022-11-21\n48.67\n2022-11-19\n49.69\n\n\n2\n52.936510\n-1.127547\n1\n982.82\n11\n89.347273\n0.297297\n1.000000\n0.00\n0\n0\n0.00\n0\n0\n901.00\n10\n0\n2021-03-30\n97.38\n2022-01-13\n103.32\n36\n26\n11\n2022-01-12\n97.97\n2022-01-13\n102.72\n\n\n3\n52.997952\n-1.189854\n1\n20.45\n2\n10.225000\n0.117647\n2.000000\n0.00\n0\n0\n10.45\n1\n0\n0.00\n0\n0\n2022-12-06\n10.03\n2022-12-06\n9.92\n16\n16\n1\n2022-12-06\n9.79\n2022-12-03\n9.94\n\n\n4\n52.971756\n-1.203969\n1\n20.75\n2\n10.375000\n0.074074\n2.000000\n0.00\n0\n0\n0.00\n0\n0\n10.75\n1\n0\n2022-01-20\n10.52\n2022-01-20\n9.68\n26\n26\n1\n2022-01-23\n9.64\n2022-01-20\n10.02\n\n\n\n\n\n\n\n\n1. The Data\nThe fictional donors in data.csv have the following features:\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nLatitude\nThe approximate latitude of the donor.\n\n\nLongitude\nThe approximate longitude of the donor.\n\n\nNewsletter\nA binary indicator (1 or 0) representing whether the donor is subscribed to the Charity’s email newsletter.\n\n\n\n\n\n\nTransactions_LifetimeGiftsAmount\nThe total amount of donations made by the donor over their lifetime.\n\n\nTransactions_LifetimeGiftsNumber\nThe total number of donations made by the donor over their lifetime.\n\n\nTransactions_AverageGiftAmount\nThe average donation made by the donor.\n\n\nDonationFrequency\nThe frequency of donations made by the donor (donations/month).\n\n\nDonationFrequencyActive\nThe frequency of donations made by the donor while they were active (donations/month).\n\n\n\n\n\n\nTransactions_Months1To12GiftsAmount\nThe total amount of donations made by the donor in the latest 12 months.\n\n\nmonthlyDonorMonths1to12\nA binary indicator showing if the donor made monthly donations in the latest 12 months.\n\n\nTransactions_Months13To24GiftsAmount\nThe total amount of donations made by the donor in the latest 13-24 months.\n\n\nTransactions_Months13To24GiftsNumber\nThe total number of donations made by the donor in the latest 13-24 months.\n\n\nmonthlyDonorMonths13to24\nA binary indicator showing if the donor made monthly donations in the latest 13-24 months.\n\n\nTransactions_Months25To36GiftsAmount\nThe total amount of donations made by the donor in the latest 25-36 months.\n\n\nTransactions_Months25To36GiftsNumber\nThe total number of donations made by the donor in the latest 25-36 months.\n\n\nmonthlyDonorMonths25to36\nA binary indicator showing if the donor made monthly donations in the latest 25-36 months.\n\n\n\n\n\n\nTransactions_DateOfFirstGift\nThe date of the first donation made by the donor.\n\n\nTransactions_FirstGiftAmount\nThe amount of the first donation made by the donor.\n\n\nTransactions_DateOfLastGift\nThe date of the last donation made by the donor.\n\n\nTransactions_LastGiftAmount\nThe amount of the last donation made by the donor.\n\n\n\n\n\n\nmonthsSinceFirstDonation\nThe number of months since the donor’s first donation.\n\n\nmonthsSinceLastDonation\nThe number of months since the donor’s last donation.\n\n\nactiveMonths\nThe number of months the donor has been an active supported of the Charity.\n\n\n\n\n\n\nTransactions_DateOfHighestGift\nThe date of the highest donation made by the donor.\n\n\nTransactions_HighestGiftAmount\nThe amount of the highest donation made by the donor.\n\n\nTransactions_DateOfLowestGift\nThe date of the lowest donation made by the donor.\n\n\nTransactions_LowestGiftAmount\nThe amount of the lowest donation made by the donor.\n\n\n\nNote that these data points have been randomized, permuted and anonymized. This is not the true data of real donors to the charity.\n\ndf.sample(10)\n\n\n\n\n\n\n\n\nLatitude\nLongitude\nNewsletter\nTransactions_LifetimeGiftsAmount\nTransactions_LifetimeGiftsNumber\nTransactions_AverageGiftAmount\nDonationFrequency\nDonationFrequencyActive\nTransactions_Months1To12GiftsAmount\nTransactions_Months1To12GiftsNumber\nmonthlyDonorMonths1to12\nTransactions_Months13To24GiftsAmount\nTransactions_Months13To24GiftsNumber\nmonthlyDonorMonths13to24\nTransactions_Months25To36GiftsAmount\nTransactions_Months25To36GiftsNumber\nmonthlyDonorMonths25to36\nTransactions_DateOfFirstGift\nTransactions_FirstGiftAmount\nTransactions_DateOfLastGift\nTransactions_LastGiftAmount\nmonthsSinceFirstDonation\nmonthsSinceLastDonation\nactiveMonths\nTransactions_DateOfHighestGift\nTransactions_HighestGiftAmount\nTransactions_DateOfLowestGift\nTransactions_LowestGiftAmount\n\n\n\n\n425\n52.976130\n-1.142525\n0\n148.69\n2\n74.345000\n0.054054\n2.000000\n0.00\n0\n0\n0.00\n0\n0\n73.69\n1\n0\n2021-03-30\n76.11\n2021-04-01\n74.40\n36\n36\n1\n2021-04-03\n75.72\n2021-04-03\n75.02\n\n\n39\n52.974401\n-1.105213\n1\n19.90\n2\n9.950000\n0.071429\n2.000000\n0.00\n0\n0\n0.00\n0\n0\n9.90\n1\n0\n2021-12-15\n9.73\n2021-12-15\n10.00\n27\n27\n1\n2021-12-16\n10.15\n2021-12-19\n9.91\n\n\n13\n52.911530\n-1.107064\n0\n198.84\n3\n66.280000\n0.250000\n3.000000\n98.84\n2\n0\n0.00\n0\n0\n0.00\n0\n0\n2023-04-23\n49.53\n2023-04-23\n50.55\n11\n11\n1\n2023-04-27\n48.94\n2023-04-23\n50.41\n\n\n227\n52.925344\n-1.259633\n1\n50.17\n2\n25.085000\n0.153846\n2.000000\n25.17\n1\n0\n0.00\n0\n0\n0.00\n0\n0\n2023-03-29\n24.72\n2023-03-30\n24.49\n12\n12\n1\n2023-03-26\n24.75\n2023-03-28\n24.65\n\n\n310\n52.952328\n-1.159730\n1\n337.34\n35\n9.638286\n0.972222\n1.000000\n105.45\n11\n0\n125.94\n13\n1\n96.48\n10\n0\n2021-05-01\n9.94\n2024-02-28\n9.87\n35\n1\n35\n2024-02-29\n9.62\n2024-02-27\n9.32\n\n\n563\n52.970545\n-1.136419\n1\n323.56\n16\n20.222500\n0.444444\n1.066667\n0.00\n0\n0\n83.93\n4\n0\n219.63\n11\n0\n2021-04-30\n20.04\n2022-06-30\n19.66\n35\n21\n15\n2022-06-26\n20.73\n2022-06-29\n21.06\n\n\n302\n52.986120\n-1.147062\n0\n1794.31\n24\n74.762917\n0.648649\n1.043478\n0.00\n0\n0\n912.18\n12\n1\n807.13\n11\n0\n2021-04-02\n71.64\n2023-01-30\n77.85\n36\n14\n23\n2023-01-30\n73.85\n2023-02-01\n74.46\n\n\n37\n52.990988\n-1.137386\n1\n231.41\n4\n57.852500\n1.333333\n2.000000\n156.41\n3\n0\n0.00\n0\n0\n0.00\n0\n0\n2024-01-03\n51.10\n2024-02-27\n48.58\n2\n1\n2\n2024-03-02\n50.08\n2024-02-28\n50.89\n\n\n398\n52.940981\n-1.214600\n1\n93.46\n18\n5.192222\n0.900000\n1.000000\n52.85\n10\n0\n35.89\n7\n0\n0.00\n0\n0\n2022-08-27\n5.05\n2024-01-02\n5.05\n19\n2\n18\n2024-01-04\n5.17\n2023-12-31\n4.99\n\n\n95\n52.964628\n-1.149133\n1\n86.08\n16\n5.380000\n0.444444\n0.444444\n63.50\n12\n1\n10.05\n2\n0\n10.17\n1\n0\n2021-04-30\n10.22\n2024-03-08\n4.84\n35\n0\n36\n2021-05-01\n10.12\n2024-03-05\n4.95\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 750 entries, 0 to 749\nData columns (total 28 columns):\n #   Column                                Non-Null Count  Dtype  \n---  ------                                --------------  -----  \n 0   Latitude                              750 non-null    float64\n 1   Longitude                             750 non-null    float64\n 2   Newsletter                            750 non-null    int64  \n 3   Transactions_LifetimeGiftsAmount      750 non-null    float64\n 4   Transactions_LifetimeGiftsNumber      750 non-null    int64  \n 5   Transactions_AverageGiftAmount        750 non-null    float64\n 6   DonationFrequency                     750 non-null    float64\n 7   DonationFrequencyActive               750 non-null    float64\n 8   Transactions_Months1To12GiftsAmount   750 non-null    float64\n 9   Transactions_Months1To12GiftsNumber   750 non-null    int64  \n 10  monthlyDonorMonths1to12               750 non-null    int64  \n 11  Transactions_Months13To24GiftsAmount  750 non-null    float64\n 12  Transactions_Months13To24GiftsNumber  750 non-null    int64  \n 13  monthlyDonorMonths13to24              750 non-null    int64  \n 14  Transactions_Months25To36GiftsAmount  750 non-null    float64\n 15  Transactions_Months25To36GiftsNumber  750 non-null    int64  \n 16  monthlyDonorMonths25to36              750 non-null    int64  \n 17  Transactions_DateOfFirstGift          750 non-null    object \n 18  Transactions_FirstGiftAmount          750 non-null    float64\n 19  Transactions_DateOfLastGift           750 non-null    object \n 20  Transactions_LastGiftAmount           750 non-null    float64\n 21  monthsSinceFirstDonation              750 non-null    int64  \n 22  monthsSinceLastDonation               750 non-null    int64  \n 23  activeMonths                          750 non-null    int64  \n 24  Transactions_DateOfHighestGift        750 non-null    object \n 25  Transactions_HighestGiftAmount        750 non-null    float64\n 26  Transactions_DateOfLowestGift         750 non-null    object \n 27  Transactions_LowestGiftAmount         750 non-null    float64\ndtypes: float64(13), int64(11), object(4)\nmemory usage: 164.2+ KB\n\n\nConverting the columns containing dates to datetime type\n\ndate_cols = ['Transactions_DateOfFirstGift', 'Transactions_DateOfLowestGift', 'Transactions_DateOfHighestGift', 'Transactions_DateOfLastGift']\n\nfor col in date_cols:\n    df[col] = pd.to_datetime(df[col])\n\n\n\n2. ENGINEERING DISTANCE FROM EHSC\n\ndef calculate_distance(row, base_coords):\n    return geodesic((row['Latitude'], row['Longitude']), base_coords).km\n\n\nehsc_coords = (52.95383, -1.14168)\n\ndf['DistanceFromEHSC'] = df.apply(calculate_distance, axis=1, base_coords=ehsc_coords)\n\n\ndf['DistanceFromEHSC']\n\n0      4.344046\n1      6.016685\n2      2.148884\n3      5.880628\n4      4.636596\n         ...   \n745    2.801735\n746    3.900813\n747    6.047917\n748    3.728712\n749    3.338494\nName: DistanceFromEHSC, Length: 750, dtype: float64\n\n\n\ndf['DistanceFromEHSC'].plot()\n\n\n\n\n\n\n\n\n\ndf['DistanceFromEHSC'].describe()\n\ncount    750.000000\nmean       4.603878\nstd        2.307654\nmin        0.096575\n25%        2.821600\n50%        4.273026\n75%        6.219849\nmax        9.978092\nName: DistanceFromEHSC, dtype: float64\n\n\n\ndf.to_csv('data.csv', index=False)\n\n\n\n3. VISUALISING DISTRIBUTION OF DONORS WITH FOLIUM\n\nm = folium.Map(location=[52.9548, -1.1581], zoom_start=12)\n\ncolors = ['green', 'yellow', 'orange', 'red', 'purple']\nlinear_colormap = LinearColormap(colors=colors,\n                                 index=[0, 100, 250, 500, 1000],\n                                 vmin=df['Transactions_LifetimeGiftsAmount'].min(),\n                                 vmax=df['Transactions_LifetimeGiftsAmount'].quantile(0.94))\n\n# Create FeatureGroups\nfgroups = [folium.map.FeatureGroup(name=f\"Total Donated:  £{lower}{('-£' + str(upper)) if upper != float('inf') else '+'}\") for lower, upper in zip([0, 100, 250, 500, 750, 1000], [100, 250, 500, 750, 1000, float('inf')])]\n\n\nfor index, row in df.iterrows():    \n    fname = 'Example'\n    lname = 'Donor'\n    email = 'exampledonor@email.com'\n    \n    total_don = row['Transactions_LifetimeGiftsAmount']\n    num_don = row['Transactions_LifetimeGiftsNumber']\n    avg_don = row['Transactions_AverageGiftAmount']\n    \n    news = bool(row['Newsletter'])\n    monthly = bool(row['monthlyDonorMonths1to12'])\n    \n    lat = row['Latitude']\n    long = row['Longitude']\n    \n    dateoffirst = row['Transactions_DateOfFirstGift'].strftime('%d/%m/%Y')\n    dateoflast = row['Transactions_DateOfLastGift'].strftime('%d/%m/%Y')\n\n    active = row['activeMonths']\n    freq = row['DonationFrequency']\n    freq_active = row['DonationFrequencyActive']\n\n    dist = row['DistanceFromEHSC']\n    \n    popup_text = f'''\n                    &lt;div style=\"width: 200px; font-family: Arial; line-height: 1.2;\"&gt;\n                        &lt;h4 style=\"margin-bottom: 5px;\"&gt;{fname} {lname}&lt;/h4&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Total Donated:&lt;/b&gt; £{total_don:.2f}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Number of Donations:&lt;/b&gt; {num_don}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Average Donation:&lt;/b&gt; £{avg_don:.2f}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;First Recorded Donation:&lt;/b&gt; {dateoffirst}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Last Recorded Donation:&lt;/b&gt; {dateoflast}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;ActiveMonths:&lt;/b&gt; {active}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;DonationFrequency&lt;/b&gt; {freq:.2f}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;DonationFrequencyActive&lt;/b&gt; {freq_active:.2f}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Subscribed to Newsletter:&lt;/b&gt; {\"Yes\" if news else \"No\"}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Current Monthly Donor:&lt;/b&gt; {\"Yes\" if monthly else \"No\"}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Distance from EHSC:&lt;/b&gt; {dist:.2f}km&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Email:&lt;/b&gt;&lt;br&gt; {email}&lt;/p&gt;\n                    &lt;/div&gt;\n                    '''\n\n    \n    color = linear_colormap(total_don)\n    \n    marker = folium.CircleMarker(\n        location=[lat, long],\n        radius=5,\n        color=color, \n        fill=True,\n        fill_color=color,\n        fill_opacity=0.7,\n        popup=popup_text\n    )\n    \n    # Add the marker to the appropriate FeatureGroup\n    for fgroup, (lower, upper) in zip(fgroups, zip([0, 100, 250, 500, 750, 1000], [100, 250, 500, 750, 1000, float('inf')])):\n        if lower &lt;= total_don &lt; upper:\n            fgroup.add_child(marker)\n            break\n\n# Add the FeatureGroups to the map\nfor fgroup in fgroups:\n    m.add_child(fgroup)\n\nlinear_colormap.add_to(m)\nlinear_colormap.caption = 'Total Donated (£)'\nm.add_child(folium.LayerControl())\n\n# Create a marker at EHSC\npopup_html = '''&lt;h4 style=\"margin-bottom: 5px;\"&gt;Emmanuel House Support Centre&lt;/h4&gt;\n&lt;a href=\"https://www.emmanuelhouse.org.uk/\" target=\"_blank\"&gt;https://www.emmanuelhouse.org.uk/&lt;/a&gt;\n&lt;p&gt;Emmanuel House is an independent charity that supports people who are homeless, rough sleeping, in crisis, or at risk of homelessness in Nottingham.&lt;/p&gt;\n'''\nmarker = folium.Marker(location=ehsc_coords, popup=folium.Popup(popup_html))\nm.add_child(marker)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nNote the popups that appear when clicking on each datapoint in the above map!\nThere is a layer control menu hidden in the top right corner until mouseover. It lets you show and hide the points with donation totals in specific ranges.\n\n\n\n4. REINTERPRETING AS A CLUSTER MAP\n\nm = folium.Map(location=[52.9548, -1.1581], zoom_start=12)\n\ncolors = ['green', 'yellow', 'orange', 'red', 'purple']\nlinear_colormap = LinearColormap(colors=colors,\n                                 index=[0, 100, 250, 500, 1000],\n                                 vmin=df['Transactions_LifetimeGiftsAmount'].min(),\n                                 vmax=df['Transactions_LifetimeGiftsAmount'].quantile(0.94))\n\n# Create a MarkerCluster\nmarker_cluster = MarkerCluster().add_to(m)\n\nfor index, row in df.iterrows():    \n    fname = 'Example'\n    lname = 'Donor'\n    email = 'exampledonor@email.com'\n    \n    total_don = row['Transactions_LifetimeGiftsAmount']\n    num_don = row['Transactions_LifetimeGiftsNumber']\n    avg_don = row['Transactions_AverageGiftAmount']\n    \n    news = bool(row['Newsletter'])\n    monthly = bool(row['monthlyDonorMonths1to12'])\n    \n    lat = row['Latitude']\n    long = row['Longitude']\n    \n    dateoffirst = row['Transactions_DateOfFirstGift'].strftime('%d/%m/%Y')\n    dateoflast = row['Transactions_DateOfLastGift'].strftime('%d/%m/%Y')\n\n    active = row['activeMonths']\n    freq = row['DonationFrequency']\n    freq_active = row['DonationFrequencyActive']\n\n    dist = row['DistanceFromEHSC']\n    \n    popup_text = f'''\n                    &lt;div style=\"width: 200px; font-family: Arial; line-height: 1.2;\"&gt;\n                        &lt;h4 style=\"margin-bottom: 5px;\"&gt;{fname} {lname}&lt;/h4&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Total Donated:&lt;/b&gt; £{total_don:.2f}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Number of Donations:&lt;/b&gt; {num_don}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Average Donation:&lt;/b&gt; £{avg_don:.2f}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;First Recorded Donation:&lt;/b&gt; {dateoffirst}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Last Recorded Donation:&lt;/b&gt; {dateoflast}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;ActiveMonths:&lt;/b&gt; {active}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;DonationFrequency&lt;/b&gt; {freq:.2f}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;DonationFrequencyActive&lt;/b&gt; {freq_active:.2f}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Subscribed to Newsletter:&lt;/b&gt; {\"Yes\" if news else \"No\"}&lt;/p&gt;\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Current Monthly Donor:&lt;/b&gt; {\"Yes\" if monthly else \"No\"}&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Distance from EHSC:&lt;/b&gt; {dist:.2f}km&lt;/p&gt;\n                        &lt;br&gt;\\\n                        &lt;p style=\"margin: 0;\"&gt;&lt;b&gt;Email:&lt;/b&gt;&lt;br&gt; {email}&lt;/p&gt;\n                    &lt;/div&gt;\n                    '''\n\n    \n    color = linear_colormap(total_don)\n    \n    marker = folium.CircleMarker(\n        location=[lat, long],\n        radius=5,\n        color=color, \n        fill=True,\n        fill_color=color,\n        fill_opacity=0.7,\n        popup=popup_text\n    )\n    \n    # Add the marker to the MarkerCluster\n    marker.add_to(marker_cluster)\n\nlinear_colormap.add_to(m)\nlinear_colormap.caption = 'Total Donated (£)'\nm.add_child(folium.LayerControl())\n\n# Create a marker at EHSC\npopup_html = '''&lt;h4 style=\"margin-bottom: 5px;\"&gt;Emmanuel House Support Centre&lt;/h4&gt;\n&lt;a href=\"https://www.emmanuelhouse.org.uk/\" target=\"_blank\"&gt;https://www.emmanuelhouse.org.uk/&lt;/a&gt;\n&lt;p&gt;Emmanuel House is an independent charity that supports people who are homeless, rough sleeping, in crisis, or at risk of homelessness in Nottingham.&lt;/p&gt;\n'''\nmarker = folium.Marker(location=ehsc_coords, popup=folium.Popup(popup_html))\nm.add_child(marker)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n5. REMARKS\n\nThe distribution of the fictional donors contained in data.csv more closely resembles the distribution of the real donors than the synthetic data constructed in my previous blog post:\nInvestigating The Geographic Distribution Of Charity Donors With Interactive Maps Made Using Folium\nThe Latitudes and Longitudes have been constructed to be the features following a distribution that is the least representative of the real data, for obvious privacy concerns. This is the reason for the donors on main roads, in Wollaton park etc."
  },
  {
    "objectID": "posts/eh1/index.html",
    "href": "posts/eh1/index.html",
    "title": "Geocoding Postcodes in Python: pgeocode v ONS",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport folium\nimport pgeocode\n\nTable of contents\n1. Geocoding with pgeocode\n2. Geocoding with ONS data\n\n\nAs part of my voluntary work for Emmanuel House, a Nottingham charity providing support and shelter for the homeless and vulnerably housed, I undertook a project studying the geographic distribution of individual donors to the charity across the UK.\nIn this post I illustrate an issue I encountered while attempting to encode the postcodes of individual donors as latitude and longitude pairs, and the solution.\nThe goal of encoding the postcodes was to produce visualisations showing how donors are distributed across Nottingham. This allowed us to gain insight into which areas/demographics should be targeted by marketing campaigns, and which social groups are underrepresented in Emmanuel House’s donor database.\n\ndf = pd.read_csv('nottm_postcodes.csv')\n\nnottm_postcodes.csv is a file I constructed consisting of 100 random postcodes within Nottingham city.\nImportantly, these are not postcodes of real donors to the charity. Sharing such information online would violate GDPR. These postcodes are randomly selected from publicly available datasets\n\ndf.head(10)\n\n\n\n\n\n\n\n\nPostcode\n\n\n\n\n0\nNG9 3WF\n\n\n1\nNG9 4WP\n\n\n2\nNG9 3EL\n\n\n3\nNG1 9FH\n\n\n4\nNG5 6QZ\n\n\n5\nNG7 5QL\n\n\n6\nNG7 2FT\n\n\n7\nNG4 1PY\n\n\n8\nNG8 3SL\n\n\n9\nNG9 4AX\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   Postcode  100 non-null    object\ndtypes: object(1)\nmemory usage: 928.0+ bytes\n\n\n\ndf.nunique()\n\nPostcode    100\ndtype: int64\n\n\n\npostcodes = df['Postcode'].to_list()\n\n\n1. Geocoding with pgeocode\nI first attempted to geocode the postcodes using pgeocode, a lightweight Python library for geocoding postcodes and distance calculations.\n\nnomi = pgeocode.Nominatim('gb')\n\n\nprint('Querying postcode:', df.iloc[0]['Postcode'])\nnomi.query_postal_code(df.iloc[0]['Postcode'])\n\nQuerying postcode: NG9 3WF\n\n\npostal_code                                                     NG9\ncountry_code                                                     GB\nplace_name        Bramcote, Beeston, Toton, Stapleford, Attenbor...\nstate_name                                                  England\nstate_code                                                      ENG\ncounty_name                                         Nottinghamshire\ncounty_code                                                11609044\ncommunity_name                                                  NaN\ncommunity_code                                                  NaN\nlatitude                                                   52.91914\nlongitude                                                  -1.24548\naccuracy                                                        4.0\nName: 0, dtype: object\n\n\n\nnomi.query_postal_code(postcodes)[['latitude','longitude']].head()\n\n\n\n\n\n\n\n\nlatitude\nlongitude\n\n\n\n\n0\n52.91914\n-1.24548\n\n\n1\n52.91914\n-1.24548\n\n\n2\n52.91914\n-1.24548\n\n\n3\n52.95360\n-1.15050\n\n\n4\n53.00060\n-1.13150\n\n\n\n\n\n\n\n\ndf[['Latitude','Longitude']] = nomi.query_postal_code(postcodes)[['latitude','longitude']]\n\n\ndf.head()\n\n\n\n\n\n\n\n\nPostcode\nLatitude\nLongitude\n\n\n\n\n0\nNG9 3WF\n52.91914\n-1.24548\n\n\n1\nNG9 4WP\n52.91914\n-1.24548\n\n\n2\nNG9 3EL\n52.91914\n-1.24548\n\n\n3\nNG1 9FH\n52.95360\n-1.15050\n\n\n4\nNG5 6QZ\n53.00060\n-1.13150\n\n\n\n\n\n\n\nAt a first glance it seems that pgeocode has provided an elegent solution to the problem in only a few lines of fast-running code. However upon closer inspection:\n\ndf[['Latitude','Longitude']].value_counts()\n\nLatitude   Longitude\n52.953600  -1.150500    42\n52.929150  -1.114850    13\n52.964800  -1.213200    12\n52.919140  -1.245480    11\n53.000600  -1.131500    10\n52.998900  -1.197100     5\n52.970960  -1.081000     3\n52.878620  -1.195240     2\n52.903579  -1.044274     1\n52.952566  -0.894169     1\nName: count, dtype: int64\n\n\n\ndf[['Latitude','Longitude']].nunique()\n\nLatitude     10\nLongitude    10\ndtype: int64\n\n\nDespite df containing 100 unique postcodes, pgeocode has only encoded these into 10 different latitude, longitude pairs.\nWhats worse is that 42(!!) distinct postcodes were each encoded into (52.953600, -1.150500).\nWe can judge this approach by contructing a visualisation in the form as an interactive .html map using Folium:\n\npgeocode_map = folium.Map(location=[52.9548, -1.1581], zoom_start=13)\n\nfor index, row in df.iterrows():\n    folium.Marker(\n        location=[row['Latitude'], row['Longitude']],\n    ).add_to(pgeocode_map)\n\npgeocode_map.save('pgeocode_map.html')\npgeocode_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nTaking a screenshot of this map and displaying using matplotlib:\n\nimg = mpimg.imread('pgeocode_map.png')\nimgplot = plt.imshow(img)\nplt.show()\n\n\n\n\n\n\n\n\nIt indeed seems that pgeocode has encoded the postcodes with a less than desirable accuracy, resulting in large numbers of distinct postcodes being sent to the same point on the map.\n\ndf = df.drop(['Latitude', 'Longitude'], axis=1)\n\n\n\n2. Geocoding with ONS data\nUpon suggestion from a friend I proceeded to directly encode the postcodes by downloading data from the Office for National Statistics (ONS) instead of relying on a Python library.\nThis had the advantage of resulting in a far more accurate geocoding, but had the disadvantage of being more computationally expensive.\n\npost = pd.read_csv(r'C:\\Users\\Daniel\\Downloads\\open_postcode_geo.csv\\open_postcode_geo.csv', header=None)\npost = post[[0, 7, 8]]\npost = post.rename({0: 'Postcode', 7: 'Latitude', 8: 'Longitude'}, axis=1)\npost.sample(5)\n\n\n\n\n\n\n\n\nPostcode\nLatitude\nLongitude\n\n\n\n\n717388\nEC4A 2LE\n51.513622\n-0.112106\n\n\n1402379\nMK40 2LE\n52.144992\n-0.462139\n\n\n1094844\nKT1 3QG\n51.407402\n-0.280400\n\n\n1612623\nNR29 4RQ\n52.702233\n1.623660\n\n\n2294608\nTA20 4DN\n50.868756\n-2.933350\n\n\n\n\n\n\n\nopen_postcode_geo.csv can be downloaded from https://www.data.gov.uk/dataset/091feb1c-aea6-45c9-82bf-768a15c65307/open-postcode-geo and is contructed from data made publicly available by the ONS.\nopen_postcode_geo.csv consists of over 2.6 million UK postcodes, along with their latitudes and longitudes. Other information about the postcodes is also included which is irrelevant for the purposes of this post.\nBecause of the length of the dataset the code cells in this section take some time to run.\n\nlen(post)\n\n2631536\n\n\nWe can now geocode the postcodes in df by joining the corresponding Latitude and Longitude from post:\n\ndf = df.merge(post, on='Postcode', how='left')\ndf.head()\n\n\n\n\n\n\n\n\nPostcode\nLatitude\nLongitude\n\n\n\n\n0\nNG9 3WF\n52.930121\n-1.198353\n\n\n1\nNG9 4WP\n52.921587\n-1.247504\n\n\n2\nNG9 3EL\n52.938985\n-1.239510\n\n\n3\nNG1 9FH\n52.955008\n-1.141045\n\n\n4\nNG5 6QZ\n52.996670\n-1.106307\n\n\n\n\n\n\n\n\ndf[['Latitude', 'Longitude']].value_counts()\n\nLatitude   Longitude\n52.945107  -1.135586    8\n52.955008  -1.141045    4\n52.930121  -1.198353    3\n52.955053  -1.141030    2\n52.959917  -1.222127    2\n                       ..\n52.953058  -1.144924    1\n52.952972  -1.228161    1\n52.952828  -1.145360    1\n52.952059  -1.170039    1\n52.998288  -1.135252    1\nName: count, Length: 85, dtype: int64\n\n\n\ndf[['Latitude','Longitude']].nunique()\n\nLatitude     85\nLongitude    85\ndtype: int64\n\n\nThis solution has resulted in 85 unique latitude, longitude pairs. A significant improvement over the results from pgeocode.\nProducing another visualisation using Folium:\n\nons_map = folium.Map(location=[52.9548, -1.1581], zoom_start=13)\n\nfor index, row in df.iterrows():\n    folium.Marker(\n        location=[row['Latitude'], row['Longitude']],\n    ).add_to(ons_map)\n\nons_map.save('ons_map.html')\nons_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nTaking a screenshot of the map using ShareX and displaying using matplotlib:\n\nimg = mpimg.imread('ons_map.png')\nimgplot = plt.imshow(img)\nplt.show()\n\n\n\n\n\n\n\n\nThis looks significantly more realistic than the map produced using pgeocode to geocode."
  },
  {
    "objectID": "posts/bte2/index.html",
    "href": "posts/bte2/index.html",
    "title": "The Boltzmann Equation - 2. Probabilistic Preliminaries",
    "section": "",
    "text": "Table of contents\n1. Probability Theory\n1.1. Random Variables and Random Vectors\n1.1.1. Definition 1.1.1 - Random Variables\n1.1.2. Definition 1.1.2 - Continuous RVs\n1.1.3. Definition 1.1.3 - Random Vectors\n1.2. Expectation, Moments and Independence\n1.2.1. Definition 1.2.1 - Expectation\n1.2.2. Definition 1.2.2 - Conditional Probability\n1.2.3. Definition 1.2.3 - Independence of RVs\n1.2.4. Remark 1.2.4\n2. References"
  },
  {
    "objectID": "posts/bte2/index.html#random-variables-and-random-vectors",
    "href": "posts/bte2/index.html#random-variables-and-random-vectors",
    "title": "The Boltzmann Equation - 2. Probabilistic Preliminaries",
    "section": "1.1. Random Variables and Random Vectors",
    "text": "1.1. Random Variables and Random Vectors\nThroughout we fix a probability space \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) and consider \\(\\mathbb{R}\\) equipped with Lebesgue measure \\(\\lambda\\) on the Borel \\(\\sigma\\)-algebra \\(\\mathcal{B}(\\mathbb{R}).\\) The reader is assumed to have taken a first course on measure theory.\nNote that a probability space \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) is a measure space with unit total mass\n\\[\\mathbb{P}(\\Omega)=1.\\]\n\n\n1.1.1. Definition 1.1.1 - Random Variables\nA random variable \\(X\\) is a measurable function \\(X:\\Omega\\longrightarrow\\mathbb{R}\\).\nThe cumulative distribution function (CDF) \\(F_X\\) of a random variable \\(X\\) is defined by \\(F_X(x) = \\mathbb{P}(X\\leq x).\\)\nA stochastic process is an indexed family of random variables \\({\\{X_t\\}_{t\\in T}}\\), where the indexing set \\(T\\) is not necessarily countable, and the index \\(t\\) is often interpreted as time.\n\n\n\n1.1.2. Definition 1.1.2 - Continuous RVs\n\\(X\\) is said to be a continuous random variable if its law \\(\\,\\mathbb{P}_X = \\mathbb{P}\\circ X^{-1}\\) is absolutely continuous with respect to the Lebesgue measure \\(\\lambda\\) as a measure on \\(\\mathbb{R}\\). That is, if\n\\[\\forall \\, N \\in \\mathcal{B}(\\mathbb{R}):\\] \\[\\lambda(N) = 0 \\Rightarrow \\mathbb{P}(X \\in N) = 0.\\]\nBy the Radon-Nikodym theorem, a random variable \\(X\\) is continuous if (and only if) there exists a measurable function \\(f_X : \\mathbb{R} \\longrightarrow [0,\\infty)\\) such that for all \\(\\, B \\in \\mathcal{B}(\\mathbb{R})\\),\n\\[\\mathbb{P}(X\\in B) = \\int_B f_X\\,\\text{d}\\lambda.\\]\nThe function \\(f\\) is called the probability density function (PDF) of \\(X\\) and is unique up to equality almost everywhere.\n\nUnless otherwise stated we now assume that a random variable \\(X\\) is continuous and has density \\(f\\).\n\n\n1.1.3. Definition 1.1.3 - Random Vectors\nA random vector \\(\\mathbf{X}\\) is an n-tuple of random variables\n\\[\\mathbf{X}=(X_1,\\ldots,X_n) : \\Omega \\longrightarrow \\mathbb{R}^n.\\]\nThe joint CDF \\(F_{X,Y}\\) of a pair of random variables \\(X,\\,Y\\) is defined as\n\\[F_{X,Y}(x,y) = \\mathbb{P}(X\\leq x, Y\\leq y).\\]\nFrom the joint CDF \\(F_{X,Y}\\) we can recover the marginal CDFs \\(F_X\\), \\(F_Y\\) by sending the other variable to infinity:\n\\[\\begin{aligned}\nF_X(x) &= \\lim_{y\\to\\infty}F_{X,Y}(x,y),\\\\\n\\\\\nF_Y(y) &= \\lim_{x\\to\\infty}F_{X,Y}(x,y).\n\\end{aligned}\\]\nX and Y are jointly continuous if their joint law is absolutely continuous with respect to the two-dimensional Lebesgue measure \\(\\lambda_2\\) on \\((\\mathbb{R}^2,\\mathcal{B}(\\mathbb{R}^2)).\\) By the Radon-Nikodym theorem, X and Y are jointly continuous if (and only if) there exists a measurable function \\(f_{X,Y} : \\mathbb{R}^2 \\longrightarrow [0,\\infty)\\) such that for all \\(\\, B \\in \\mathcal{B}(\\mathbb{R}^2)\\),\n\\[\\mathbb{P}((X,Y)\\in B) = \\int_B f_{X,Y}\\,\\text{d}\\lambda_2,\\]\nwhere \\(f_{X,Y}\\) is called the joint PDF of X and Y. The marginal PDFs \\(f_X\\), \\(f_Y\\) can be obtained from \\(f_{X,Y}\\) by integrating out the other variable:\n\\[\\begin{aligned}\nf_X(x) &= \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)\\,\\text{d} y\\\\\n\\\\\nf_Y(y) &= \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)\\,\\text{d} x.\n\\end{aligned}\\]\nIn particular, jointly continuous random variables are automatically marginally continuous, although the converse is not true in general.\nAnalagously, \\(X_1,\\dots,X_n\\) are jointly continuous if their joint law is absolutely continuous with respect to n-dimensional Lebesgue measure \\(\\lambda_n\\), or equivalently if there exists a joint PDF for the random vector \\((X_1,\\dots,X_n)\\)."
  },
  {
    "objectID": "posts/bte2/index.html#expectation-moments-and-independence",
    "href": "posts/bte2/index.html#expectation-moments-and-independence",
    "title": "The Boltzmann Equation - 2. Probabilistic Preliminaries",
    "section": "1.2. Expectation, Moments and Independence",
    "text": "1.2. Expectation, Moments and Independence\n\n1.2.1. Definition 1.2.1 - Expectation\nThe expectation \\(\\mathbb{E}[X]\\) of a random variable \\(X\\) is simply its Lebesgue integral with respect to \\(\\mathbb{P}\\)\n\\[\\begin{aligned}\n    \\mathbb{E}[X] &= \\int_\\Omega X \\, \\text{d}\\mathbb{P}\\\\\n    &= \\int_\\mathbb{R} xf(x) \\,\\text{d} x.\n\\end{aligned}\\]\nThe \\(k^{th}\\) moment of \\(f\\) around the non-random value \\(c\\in\\mathbb{R}\\) is\n\\[\\mathbb{E}\\left[(X-c)^k\\right] = \\int_{-\\infty}^{\\infty} (x-c)^kf(x)\\,\\text{d} x.\\]\nThe \\(k^{th}\\) raw moment is the \\(k^{th}\\) moment around the origin, \\(c=0\\), \\[\\mathbb{E}[X^k] = \\int_{-\\infty}^{\\infty} x^kf(x)\\,\\text{d} x.\\]\nFor example, the \\(1^{st}\\) raw moment is the distribution’s mean, \\(\\mu = \\mathbb{E}[X]\\).\nThe \\(k^{th}\\) central moment is the \\(k^{th}\\) moment around the mean, \\(c = \\mu = \\mathbb{E}[X]\\),\n\\[\\mathbb{E}[(X-\\mu)^k] = \\int_{-\\infty}^{\\infty} (x-\\mu)^kf(x)\\,\\text{d} x.\\]\nFor example, the \\(2^{nd}\\) central moment is the distribution’s variance, \\(\\sigma^2 = \\mathbb{E}\\left[(X-\\mu)^2\\right]\\).\nBy abuse of language, it is common to refer to integrals of the form \\(\\int r(x)f(x)\\,\\text{d} x\\) as ‘moments’ even when the function \\(r\\) is not a polynomial in \\(x\\).\n\n\n\n1.2.2. Definition 1.2.2 - Conditional Probability\nGiven continuous random variables \\(X\\) and \\(Y\\) with \\(f_Y(y)&gt;0\\) we define:\n\nThe conditional CDF \\(F_{X|Y}\\) of \\(X\\) given Y by\n\\[F_{X|Y}(x|y) = \\frac{\\int_{-\\infty}^{x}f_{X,Y}(u,y)\\,\\text{d} u}{f_Y(y)}.\\]\nThe conditional PDF \\(f_{X|Y}\\) of \\(X\\) given \\(Y\\) by\n\\[f_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}.\\]\nThe conditional probability of the event \\(\\{ X\\in A\\}\\), \\(A\\in\\mathcal{B}(\\mathbb{R)}\\), given that \\(Y = y\\) by\n\\[\\mathbb{P}(X\\in A|Y = y) = \\int_A f_{X|Y}(x|y)\\,\\text{d} x.\\]\n\n\n\n\n1.2.3. Definition 1.2.3 - Independence of RVs\nRandom variables \\(X\\) and \\(Y\\) are said to be independent if their joint CDF \\(F_{X,Y}\\) factorises into the tensor product of the marginals:\n\\[F_{X,Y}(x,y) = F_X(x)F_Y(y).\\]\nEquivalently, if X and Y are jointly continuous then we say they are independent if their joint PDF \\(f_{X,Y}\\) factorises into the tensor product of the marginals:\n\\[f_{X,Y}(x,y) = f_X(x)f_Y(y)\\quad\\text{a.e.}\\]\nWhere for \\(f,g:E\\longrightarrow \\mathbb{R}\\) the tensor product \\(f\\otimes g : E^2\\longrightarrow \\mathbb{R}\\) is defined by \\(f\\otimes g(x,y) = f(x)g(y).\\)\n\n\n\n1.2.4. Remark 1.2.4\nThis is not the most compact and elegant way of defining independence of random variables through the independence of the \\(\\sigma\\)-algebras they generate. However this definition is equivalent, more intuitive and will suffice for our purposes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning\n\n\n\n\n\n\nPython\n\n\nML\n\n\nPandas\n\n\nscikit-learn\n\n\nfolium\n\n\nKMeans\n\n\nGMM\n\n\nBIRCH\n\n\nPCA\n\n\nClustering\n\n\nDonor Segmentation\n\n\nEmmanuel House\n\n\n\nRead a Jupyter notebook from my machine learning project for Emmanuel House Support Centre: ‘Donor segmentation using an ensemble of clustering algorithms’.\n\n\n\n\n\nApr 2, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n\n\n\n\n\n\nGeographic Analysis of Charity Donors - Latest Leaflet Maps\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nfolium\n\n\nGeoPy\n\n\nHTML\n\n\nEmmanuel House\n\n\n\nExplore my latest interactive map displaying the geographic distribution of a artificial dataset of donors to a Nottingham charity. The geodesic distance from the donor to the charity is engineered using GeoPy.\n\n\n\n\n\nMar 31, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n\n\n\n\n\n\nThe Boltzmann Equation - 3. Information Theory\n\n\n\n\n\n\nMathematics\n\n\nProbability Theory\n\n\nInformation Theory\n\n\nBoltzmann Equation\n\n\n\nWe introduce key notions from information theory such as differential entropy and the Kullback-Liebler (KL) divergence and prove the information inequality.\n\n\n\n\n\nMar 23, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n\n\n\n\n\n\nModel Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction\n\n\n\n\n\n\nPython\n\n\nML\n\n\nPandas\n\n\nSeaborn\n\n\nscikit-learn\n\n\nTensorFlow\n\n\nXGBoost\n\n\nOptuna\n\n\n\nAfter cleaning and preprocessing a modified LendingClub dataset of loan applicants I implement an Optuna study for both model selection and hyperparameter tuning with cross-validation to choose a model to predict if an unseen applicant will repay their loan.\n\n\n\n\n\nMar 19, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising the Geographic Distribution of Charity Donors with Interactive Leaflet Maps\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nfolium\n\n\nEmmanuel House\n\n\n\nI illustrate my method for investigating the geographic distribution of individual donors to Emmanuel House Support Centre in Nottingham by constructing interactive leaflet maps in folium with a synthetic dataset.\n\n\n\n\n\nMar 17, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n\n\n\n\n\n\nGeocoding Postcodes in Python: pgeocode v ONS\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nfolium\n\n\nEmmanuel House\n\n\n\nI explain a problem encountered in voluntary work undertaken for Emmanuel House Support Centre in Nottingham related to encoding postcodes as coordinates, and the solution found.\n\n\n\n\n\nMar 16, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n\n\n\n\n\n\nThe Boltzmann Equation - 2. Probabilistic Preliminaries\n\n\n\n\n\n\nMathematics\n\n\nProbability Theory\n\n\nBoltzmann Equation\n\n\n\nBefore we can discuss Information Theory and its consequences for the Boltzmann equation, we first need to make some definitions from probability theory.\n\n\n\n\n\nFeb 13, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n\n\n\n\n\n\nThe Boltzmann Equation - 1. Introduction\n\n\n\n\n\n\nMathematics\n\n\nPDEs\n\n\nBoltzmann Equation\n\n\n\nI introduce the rigorous theory of the Boltzmann Transport Equation, following my undergraduate research project at Warwick Mathematics Institue: ‘The interplay between Information theory and the long-time behaviour of a dilute gas’.\n\n\n\n\n\nJan 29, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Analysis with NLTK and Hugging Face Transformers\n\n\n\n\n\n\nPython\n\n\nML\n\n\nNatural Language Processing\n\n\nNLTK\n\n\nHugging Face\n\n\nSentiment Analysis\n\n\n\nSentiment analysis is performed on a dataset of Amazon reviews using NLTK’s VADER and a RoBERTa-base model from Hugging Face.\n\n\n\n\n\nJan 23, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression with Gradient Descent and L2-Regularization\n\n\n\n\n\n\nPython\n\n\nML\n\n\nNumPy\n\n\n\nLogistic regression is implemented in NumPy and interpreted as a perceptron with sigmoid activation. The resulting model is used to detect cats in an image classification problem. Overfitting to the training data is counteracted by including a regularization term in the cost function. The regularization parameter is tuned to improve accuracy on the validation data.\n\n\n\n\n\nJan 16, 2024\n\n\nDaniel J Smith\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Energy Consumption with XGBoost\n\n\n\n\n\n\nPython\n\n\nML\n\n\nPandas\n\n\nSeaborn\n\n\nXGBoost\n\n\nTime Series Forecasting\n\n\n\nInformed by YouTube videos of Rob Mulla we use XGBoost to forecast energy consumption in the eastern US.\n\n\n\n\n\nDec 22, 2023\n\n\nDaniel J Smith\n\n\n\n\n\n\n\n\n\n\n\n\nThe Spaceship Titanic with LightGBM\n\n\n\n\n\n\nPython\n\n\nML\n\n\nPandas\n\n\nSeaborn\n\n\nLightGBM\n\n\n\nA LightGBM classifier is trained with hyperparameters tuned using a random search to achieve &gt;80% classification accuracy on the Spaceship Titanic dataset.\n\n\n\n\n\nNov 23, 2023\n\n\nDaniel J Smith\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is made using Quarto, an open source technical publishing system that provides the functionality to render .ipynb files as blog posts, and hosted with GitHub pages.\nThe .ipynb files are written in Python3 using Jupyter Lab.\nThe mathematics posts are typically written in LaTeX and converted to markdown using Pandoc."
  },
  {
    "objectID": "posts/bte1/index.html",
    "href": "posts/bte1/index.html",
    "title": "The Boltzmann Equation - 1. Introduction",
    "section": "",
    "text": "Table of contents\n1. The Boltzmann Equation\n1.1. The Evolution Equations\n1.2. Boundary conditions\n1.3. Collision kernels\n1.4. The H Theorem\n1.4.1. Theorem 1.4.1 (Boltzmann’s H Theorem)\n1.5. Collision Invariants and conservation laws\n1.5.1. Lemma 1.5.1\n2. References"
  },
  {
    "objectID": "posts/bte1/index.html#the-evolution-equations",
    "href": "posts/bte1/index.html#the-evolution-equations",
    "title": "The Boltzmann Equation - 1. Introduction",
    "section": "1.1. The Evolution Equations",
    "text": "1.1. The Evolution Equations\nThe Boltzmann equation (also known as the Boltzmann transport equation) models the behaviour of a gas comprised of a single particle species, sufficiently dilute so that quantum effects are negligible and when all inter-particle interactions are assumed to be elastic binary collisions:\n\\[\\begin{aligned}\n\\frac{\\partial f}{\\partial t} + v\\cdot \\nabla_{x} f = Q(f,f),\\quad x\\in X,\\: v\\in \\mathbb{R}^d,\\: t\\geq0.\n\\end{aligned}\\]\nWhere \\(f=f(t,x,v)=f_t(x,v)\\) is the particle distribution function in phase space and \\(X \\subset \\mathbb{R}^d\\) \\((d\\geq2)\\) is the spatial domain of the gas. If we instead adopt index notation the Boltzmann equation reads\n\\[\\begin{aligned}\n    \\frac{\\partial f}{\\partial t} + v_j\\frac{\\partial f}{\\partial x_j} = Q(f,f).\n\\end{aligned}\\]\nwhere summation over the term \\(v_j\\frac{\\partial f}{\\partial x_j}\\) is left implicit following Einstein’s summation convention. The operator \\(Q\\) appearing in the above is the Boltzmann collision operator, defined by the integral\n\\[\\begin{aligned}\n    Q(f,f) = \\int_{\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(f'f'_*-ff_*)\\,\\mathrm{d}\\sigma\\,\\mathrm{d}v_*,\n\\end{aligned}\\]\nwhere the function \\(B\\) is the so-called collision kernel, whose precise form depends on the nature of the inter-particle interactions being considered, and we have used the standard abbreviations \\(f'=f(t,x,v')\\), \\(f_*=f(t,x,v_*)\\) and \\(f'_*=f(t,x,v'_*)\\).\nHere we denote by \\((v',v'_*)\\) the precollisional velocities of a pair of particles who have velocities \\((v,v_*)\\) post-collision. By applying the conservation of momentum and energy we can relate the pairs \\((v',v'_*)\\) and \\((v,v_*)\\) via\n\\[\\begin{aligned}\n    \\begin{cases}\n    v' +v'_* = v + v_*\\\\\n    |v'|^2+|v'_*|^2 = |v|^2 +|v_*|^2\n    \\end{cases}\n\\end{aligned}\\]\nWe parameterise the solution space of this system via the so-called \\(\\mathbf{\\sigma}\\)-representation:\n\\[\\begin{aligned}\n\\begin{cases}\nv' = \\frac{v+v_*}{2} + \\frac{|v-v_*|}{2}\\sigma\\\\\n  v_*' = \\frac{v+v_*}{2} - \\frac{|v-v_*|}{2}\\sigma\n\\end{cases}   \n\\end{aligned}\\]\nwhere \\(\\sigma \\in \\mathbb{S}^{d-1}\\) is precisely the variable \\(\\sigma\\) integrated over in the above integral expression for \\(Q(f,f)\\). The Boltzmann operator \\(Q\\) admits a clear decomposition into a gain term \\(Q^+\\) and a loss term \\(Q^-\\):\n\\[\\begin{aligned}\nQ(f,f) = Q^+(f,f)-Q^-(f,f),\n\\end{aligned}\\]\nwhere \\(Q^+(f,f)\\) ‘counts’ all the collisions which result in a new particle moving with velocity \\(v\\) and \\(Q^-(f,f)\\) ‘counts’ all the collisions in which a particle of velocity \\(v\\) collides with another particle resulting in less particles at velocity \\(v\\). It is frequently convenient to view the Boltzmann operator as the quadratic form induced by the (non-symmetric) bilinear form\n\\[\\begin{aligned}\n    Q(g,f) = \\int_{\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(g'f'_*-gf_*)\\,\\mathrm{d}\\sigma\\,\\mathrm{d}v_*.\n\\end{aligned}\\]\nWhen the distribution function \\(f\\) is independent of the spatial variable \\(x\\) the full Boltzmann equation reduces to the spatially homogeneous Boltzmann equation\n\\[\\begin{aligned}\n    \\frac{\\partial f}{\\partial t} = Q(f,f).\n\\end{aligned}\\]\nIf an external force \\(F = F(x)\\) is applied to the system, we instead consider the more general Boltzmann equation with force term\n\\[\\begin{aligned}\n    \\frac{\\partial f}{\\partial t} + v\\cdot \\nabla_{x} f + F\\cdot\\nabla_{v} f = Q(f,f),\n\\end{aligned}\\]\nwhich in index notation reads as\n\\[\\begin{aligned}\n    \\frac{\\partial f}{\\partial t} + v_j\\frac{\\partial f}{\\partial x_j} + F_j\\frac{\\partial f}{\\partial v_j} =  Q(f,f).\n\\end{aligned}\\]\nGiven a solution \\(f\\) of the Boltzmann equation (without external force) we can define (in adimensional form) the local density \\(\\rho\\), the local macroscopic velocity \\(u\\) and the local temperature \\(T\\) by\n\\[\\begin{aligned}\n\\begin{split}\n    \\rho &= \\int_{\\mathbb{R}^d}f(t,x,v)\\,\\text{d}v,\\\\\n    u &= \\frac{1}{\\rho}\\int_{\\mathbb{R}^d}f(t,x,v)v\\,\\text{d}v,\\\\\n    T &= \\frac{1}{\\rho d}\\int_{\\mathbb{R}^d}f(t,x,v)|v-u|^2\\,\\text{d}v.\n\\end{split}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/bte1/index.html#boundary-conditions",
    "href": "posts/bte1/index.html#boundary-conditions",
    "title": "The Boltzmann Equation - 1. Introduction",
    "section": "1.2. Boundary conditions",
    "text": "1.2. Boundary conditions\nFor there to be any hope of our problem being well-posed we need to supplement the Boltzmann equation with boundary conditions, modelling the interactions between the particles and the boundary \\(\\partial X\\) of the spatial domain \\(X\\).\nOf course, the only thing restricting the choice of such boundary conditions is our imagination. We briefly discuss some of the most popular and physically relevant choices.\n\nSpecular reflection: \\[\\begin{aligned}\n        f(x,R_xv)& = f(x,v),\\quad x\\in\\partial X,\\\\\n    \\end{aligned}\\] where\n\\[R_xv = v - 2(v\\cdot n(x))n(x)\\]\nand \\(n(x)\\) denotes the unit normal at \\(x\\in\\partial X\\). Physically, specular reflection corresponds to the particles elastically colliding with a static, hard wall. Although not particularly accurate, specular reflection is a natural first guess at a boundary condition as it avoids the complex question of modelling the interactions of the particles with the fine microscopic structure of the wall.\nBounce-back: \\[\\begin{aligned}\n        f(x,v) = f(x,-v),\\quad x\\in\\partial X.\n        \\end{aligned}\\] Physically, this condition simply states that particles reflect from the boundary with a velocity opposite to their incident velocity. Although clearly not particularly realistic, it occasionally leads to more physical results than specular reflection as it allows for the transfer of some tangential momentum to the wall, which is not allowed by specular reflection.\nMaxwellian diffusion: \\[\\begin{aligned}\n    f(x,v) &= \\rho_-(x)M_w(v), \\quad v\\cdot n(x)&gt;0,\\\\\n     \\rho_-(x) &= \\int_{v\\cdot n\\lt0}f(x,v)|v\\cdot n| \\,\\mathrm{d}v,\\\\\n    M_w(v) &= \\frac{1}{(2\\pi)^{\\frac{d-1}{2}}T_w^{\\frac{d+1}{2}}}\\,\\mathrm{exp}\\left(-\\frac{|v|^2}{2T_w}\\right).\n    \\end{aligned}\\]"
  },
  {
    "objectID": "posts/bte1/index.html#collision-kernels",
    "href": "posts/bte1/index.html#collision-kernels",
    "title": "The Boltzmann Equation - 1. Introduction",
    "section": "1.3. Collision kernels",
    "text": "1.3. Collision kernels\nThe collision kernel \\(B\\) is related to the physical cross section \\(\\Sigma(v-v_*,\\sigma)\\) by the identity\n\\[\\begin{aligned}\n    B = |v-v_*|\\Sigma\n\\end{aligned}\\]\nOn physical grounds (Galilean invariance) it is assumed that the collision kernel \\(B\\) depends only on the magnitude of the relative velocity \\(|v-v_*|\\) and the cosine of the deviation angle \\(\\mathrm{cos}\\:\\theta = \\left\\langle \\frac{v-v_*}{|v-v_*|},\\sigma\\right\\rangle\\). For this reason it is common to abuse notation by writing\n\\[\\begin{aligned}\n    B(v-v_*,\\sigma) = B(|v-v_*|,\\mathrm{cos}\\:\\theta)\n\\end{aligned}\\]\nto emphasise the specific forms that collision kernels can take. Maxwell showed that for a given impact parameter \\(p\\geq0\\) and relative velocity \\(z\\in\\mathbb{R}^3\\) the collision kernel is implicitly given by\n\\[\\begin{aligned}\n    B(|z|,\\mathrm{cos}\\:\\theta) = \\frac{p}{\\mathrm{sin}\\theta}\\frac{\\text{d}p}{\\text{d}\\theta}|z|.\n\\end{aligned}\\]\nThis can be made explicit in the crucially important example of a gas of hard spheres, where the gas particles are treated as spheres of fixed radius that interact via elastic collisions (‘billiard balls’). For hard spheres in \\(d = 3\\) the cross section is constant, and thus the collision kernel is given by\n\\[\\begin{aligned}\n    B = |v-v_*|.\n\\end{aligned}\\]\nIn the case of particles interacting by an inverse \\(s\\)-power law force the collision kernel factorises as\n\\[\\begin{aligned}\n\\begin{split}\n    B(v-v_*,\\cos\\theta) &= \\Psi(|v-v_*|)\\,b(\\cos\\theta)\\\\\n    &= |v-v_*|^{\\gamma}\\,b(\\cos\\theta),\n\\end{split}\n\\end{aligned}\\]\nwhere \\(\\Psi(|z|) = |z|^\\gamma\\) is conventionally called the kinetic collision kernel,\n\\(\\gamma = (s-5)/(s-1)\\) in dimension \\(d = 3\\) and \\(b(\\cos\\theta)\\) is conventionally called the angular collision kernel. The the function \\(b\\) is typically complicated, smooth away from \\(0\\) and is only known implicitly.\nSuch collision kernels are often further classified by the values of \\(\\gamma\\) as follows:\n\n\\(\\gamma &gt; 0\\) : hard potentials\n\\(\\gamma &lt; 0\\) : soft potentials\n\\(\\gamma = 0\\) : Maxwellian potentials\n\nThe edge case of Maxwellian potentials is interesting as in such cases the collision kernel has no kinetic part and only depends on the cosine of the deviation angle. This occurs for particles interacting via an inverse \\((2d-1)\\)-power law force in \\(\\mathbb{R}^d\\) (e.g. a force like \\(r^{-5}\\) in \\(\\mathbb{R}^3\\)). Such particles are called Maxwellian molecules and should only be considered a theoretical construction.\nA crucial assumption frequently made in analytical treatments of the Boltzmann equation is Grad’s angular cut-off, which simply assumes that the angular collision kernel \\(b\\) is integrable:\n\\[\\begin{aligned}\n\\int_{\\mathbb{S}^{d-1}}b(k\\cdot\\sigma)\\,\\text{d}\\sigma = \\left|\\mathbb{S}^{d-2}\\right|\\int_0^{\\pi}b(\\cos\\theta)\\sin^{d-2}\\theta\\,\\text{d}\\theta&lt;\\infty.\n\\end{aligned}\\]\nFor a spatially inhomogeneous, equilibrium solution of the Boltzmann equation (i.e. a density \\(f = f(x,v)\\) solving \\(v\\cdot \\nabla_{x} f = Q(f,f)\\)) we use these quantities to define the local Maxwellian distribution \\(M_{\\text{loc}}^f\\) associated to \\(f\\) by\n\\[\\begin{aligned}\n    M_{\\mathrm{loc}}^f(x,v) = \\frac{\\rho(x)}{(2\\pi T(x))^{d/2}}\\,\\mathrm{exp}\\left[-\\frac{1}{2T(x)}|v-u(x)|^2\\right].\n\\end{aligned}\\]\nFor a spatially homogeneous, equilibrium solution of the Boltzmann equation (i.e. a density \\(f = f(v)\\) solving \\(Q(f,f)=0\\)) \\(\\,\\rho,\\,u\\) and \\(T\\) are constant and correspond to the macroscopic density, velocity and temperature respectively.\nUsing these quantities we define the (global) Maxwellian distribution \\(M^f\\) associated to \\(f\\), which physically describes the state of thermodynamic equilibrium in which the gas is maximally diffused, by\n\\[\\begin{aligned}\n    M^f(v) = \\frac{\\rho}{(2\\pi T)^{d/2}}\\,\\mathrm{exp}\\left[-\\frac{1}{2T}|v-u|^2\\right].\n\\end{aligned}\\]\nIn the theory of the Boltzmann equation the following question is of central importance:\nUnder what conditions, and in what sense, do we have \\(f\\longrightarrow M^f\\) as \\(t \\longrightarrow \\infty\\)?"
  },
  {
    "objectID": "posts/bte1/index.html#the-h-theorem",
    "href": "posts/bte1/index.html#the-h-theorem",
    "title": "The Boltzmann Equation - 1. Introduction",
    "section": "1.4. The H Theorem",
    "text": "1.4. The H Theorem\nFor a probability density \\(f\\) on \\(X \\times \\mathbb{R}^d\\) define Boltzmann’s H functional by \\[\\begin{aligned}\n    H(f)\n= \\int_{X\\times\\mathbb{R}^d}f\\log f\\,\\text{d} x\\,\\text{d} v.\n\\end{aligned}\\]\n\\(H(f)\\) is well-defined in \\(\\mathbb{R}\\cup\\{\\infty\\}\\) provided that \\[\\int f(x,v)|v|^2\\,\\text{d} x\\,\\text{d} v &lt; \\infty,\\] i.e. if \\(f\\) has finite energy. We note that up to a change of sign the \\(H\\) functional is just the differential entropy of \\(f\\), to be defined in a future post.\nBoltzmann’s H theorem loosely states that \\(H(f)\\) is a quantity monotonically decreasing (non-increasing) in time, and is stationary if and only if \\(f\\) is a Maxwellian. Making this precise is a task more technical that it might first appear.\n\n\n1.4.1. Theorem 1.4.1 (Boltzmann’s H Theorem)\nLet \\((f_t)_{t\\geq0}\\) be a well-behaved (smooth) solution of the Boltzmann equation (in particular with finite entropy), with either periodic, bounce-back or specular boundary conditions. In the latter case assume further that \\(d = 2 \\text{ or } 3\\) and the spatial domain \\(X\\) has no axis of symmetry. Then:\n\n\\[\\frac{\\text{d}}{\\text{d} t} H(f_t) \\leq 0.\\] Moreover, one can define another functional \\(D\\) on \\(L^1\\left(\\mathbb{R}^d\\right)\\) called the such that \\[\\frac{\\text{d}}{\\text{d} t} H(f_t) = -\\int_X D(f_t(x,\\cdot\\,)\\,\\text{d} x.\\]\nAssume that \\(B(v-v_*,\\sigma)&gt;0\\) for a.e. \\((v,v_*,\\sigma)\\in\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}\\) (this is always true in cases of physical interest).\nLet \\(f(x,v)\\) be a probability density on \\(X\\times\\mathbb{R}^d\\) with finite energy, \\(\\int f(x,v)|v|^2\\,\\text{d} x\\,\\text{d} v &lt;\\infty.\\) Then:\n\\[\\int_X D(f(x,\\cdot\\,)\\text{d} x = 0\\,\\, \\Longleftrightarrow\\,\\, f\\; \\text{is a local Maxwellian}, \\text{ i.e. } f = M_{\\mathrm{loc}}^f.\\]\n\\[\\begin{aligned}\n        (f_t)_{t\\geq0} \\text{ is stationary }\\,\\,&\\Longleftrightarrow\\,\\, D(f_t(x,\\cdot\\,) = 0\\quad\\forall\\, t\\geq 0\\\\\n        &\\Longleftrightarrow\\,\\, f_t \\text{ is a global Maxwellian, i.e. } f_t(x,v) = M^f(v) \\quad\\forall\\, t\\geq 0.\n\\end{aligned}\\]\n\nProof.\nSee Theorem 1, section 1.1.2 in Rezakhanlou, Villani & Golse [3.]."
  },
  {
    "objectID": "posts/bte1/index.html#collision-invariants-and-conservation-laws",
    "href": "posts/bte1/index.html#collision-invariants-and-conservation-laws",
    "title": "The Boltzmann Equation - 1. Introduction",
    "section": "1.5. Collision Invariants and conservation laws",
    "text": "1.5. Collision Invariants and conservation laws\nThe entropy dissipation functional \\(D\\) introduced in Theorem 1 can be expressed as\n\\[\\begin{aligned}\n   D(f) = \\frac{1}{4}\\int_{\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(f'f'_*-ff_*)\\mathrm{log}\\,\\frac{f'f'_*}{ff_*}\\,\\text{d}\\sigma\\,\\text{d} v_*\\,\\text{d} v.\n\\end{aligned}\\]\nAnd formally satisfies\n\\[\\frac{\\text{d}}{\\text{d} t} H(f_t) = - \\int D(f_t(x,\\cdot\\,)\\,\\text{d} x\\]\nin the spatially inhomogeneous case and\n\\[\\frac{\\text{d}}{\\text{d} t} H(f_t) = D(f_t),\\]\nin the spatially homogeneous case.\nThe derivation of the above formula for \\(D\\) relies on the following lemma used to symmetrize the Boltzmann operator \\(Q\\):\n\n\n1.5.1. Lemma 1.5.1\nThe change of variables interchanging the primed and unprimed velocities\n\\[\\begin{aligned}\n    (v,v_*,\\sigma) \\longmapsto (v',v'_*,k)\\\\\n\\end{aligned}\\]\nwhere\n\\[\\begin{aligned}\nk &= \\frac{v-v_*}{|v-v_*|}\\\\\n\\\\\n\\sigma &= \\frac{v'-v'_*}{|v'-v'_*|}\n\\end{aligned}\\]\nis involutive (self-inverse) and has unit Jacobian determinant.\nSimilarly, the change of variables interchanging the starred and unstarred velocities \\((v,v_*,v',v'_*)\\longmapsto(v_*,v,v'_*,v')\\) is also involutive with unit Jacobian.\n\nMorally, this lemma simply states than under an integral sign one can interchange primed and unprimed velocities \\((v,v_*) \\longmapsto (v',v'_*)\\) and starred and unstarred velocities \\((v,v_*) \\longmapsto (v_*,v)\\) at will. Physically, this property relies on the time- and space-reversal symmetry of the microscopic dynamics.\nRepeated application of Lemma 1 to an integral of the form \\(\\int Q(f,f)\\phi\\) for an arbitrary continuous function \\(\\phi\\) of velocity gives\n\\[\\begin{aligned}\n\\int_{\\mathbb{R}^d} Q(f,f)\\phi\\,dv &= \\int_{\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(f'f'_*-ff_*)\\phi\\;\\text{d}\\sigma\\,\\text{d}v_*\\text{d}v\\\\\n&=\\int_{\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(ff_*)(\\phi'-\\phi)\\;\\text{d}\\sigma\\,\\text{d}v_*\\text{d}v\\\\\n&=\\frac{1}{2}\\int_{\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(ff_*)(\\phi'+\\phi'_*-\\phi-\\phi_*)\\;\\text{d}\\sigma\\,\\text{d}v_*\\text{d}v\\\\\n&=-\\frac{1}{4}\\int_{\\mathbb{R}^d\\times\\mathbb{R}^d\\times\\mathbb{S}^{d-1}}B(v-v_{*},\\sigma)(f'f'_*-ff_*)(\\phi'+\\phi'_*-\\phi-\\phi_*)\\;\\text{d}\\sigma\\,\\text{d}v_*\\text{d}v\\\\\n\\end{aligned}\\]\nTaking \\(\\phi = \\mathrm{log}\\,f\\) yields the above integral formula for \\(D\\). Moreover, we see from the last line of the above that for \\(\\phi\\) satisfying the functional equation\n\\[\\begin{aligned}\n    \\phi + \\phi_* = \\phi' + \\phi'_*\n\\end{aligned}\\]\nwe have, at least formally, the conservation law:\n\\[\\begin{aligned}\n        \\frac{\\text{d}}{\\text{d} t}\\int f(t,x,v)\\phi(v)\\,\\text{d} x\\,\\text{d} v = 0.\n\\end{aligned}\\]\nSuch functions \\(\\phi\\) are called collision invariants as they correspond to quantities conserved by the microscopic dynamics.\nThe functional equation \\(\\phi + \\phi_* = \\phi' + \\phi'_*\\) has been solved under progressively weaker and weaker assumptions on \\(\\phi\\), each time concluding that the most general solution is\n\\[\\phi(v) = A + B\\cdot v + C|v|^2\\]\nfor some \\(A,\\,C\\in\\mathbb{R}\\), \\(B\\in\\mathbb{R}^d.\\)\nThus, all collision invariants can be written as a linear combination of the so-called elementary collision invariants\n\\[\\begin{aligned}\n    \\phi(v) = 1, v_1,\\dots,v_d,\\frac{v^2}{2},\n\\end{aligned}\\]\nwhich correspond to the conservation of mass, the conservation of each of the \\(d\\) components of momentum and the conservation of energy respectively."
  },
  {
    "objectID": "posts/bte3/index.html#relative-entropy-mutual-information",
    "href": "posts/bte3/index.html#relative-entropy-mutual-information",
    "title": "The Boltzmann Equation - 3. Information Theory",
    "section": "2.2. - Relative Entropy & Mutual Information",
    "text": "2.2. - Relative Entropy & Mutual Information\n\n2.2.1. Definition 2.2.1 - KL Divergence\nThe relative entropy (also known as the Kullback-Liebler divergence) \\(D(f\\,||\\,g)\\) between two densities \\(f\\) and \\(g\\) is defined as\n\\[D(f\\,||\\,g) = \\int f(x)\\log\\frac{f(x)}{g(x)}\\,\\text{d}x.\\]\n\nNote: \\[D(f\\,||\\,g) &lt; \\infty \\Longleftrightarrow \\text{supp}\\,f\\subseteq\\text{supp}\\,g.\\]\n\n\n2.2.2. Definition 2.2.2 - Mutual Information\nGiven two random variables \\(X\\) and \\(Y\\) with joint density \\(f_{X,Y}\\) define the mutual information \\(I(X;Y)\\) between \\(X\\) and \\(Y\\) by\n\\[I(X;Y) = \\int f_{X,Y}(x,y)\\log\\left[\\frac{f_{X,Y}(x,y)}{f_X(x)f_Y(y)}\\right]\\,\\text{d}x\\,\\text{d}y.\\]\n\nFrom the definition it is clear that we have the formulas\n\\[\\begin{aligned}\n    I(X;Y) &= h(X) - h(X|Y)\\\\\n    &= h(Y) - h(Y|X)\\\\\n    &= h(X) + h(Y) - h(X,Y).\n\\end{aligned}\\]\nAlong with\n\\[I(X;Y) = D(f_{X,Y}\\,||\\,f_X\\otimes f_Y).\\]\nNote the special cases\n\\[\\begin{aligned}\nI(X;Y) &= I(Y;X),\\\\\nI(X;X) &= h(X).\n\\end{aligned}\\]\n\n\n2.2.3. Theorem 2.2.3 - Information inequality\nFor any pair of densities \\(f,\\,g\\):\n\\[D(f\\,||\\,g) \\geq 0,\\]\nwith equality if and only if \\(f = g\\) a.e.\nProof.\nWithout loss of generality assume \\(f/g\\geq1\\). Use the fact that \\(\\int f = \\int g = 1\\) to rewrite\n\\[\\begin{aligned}\n    D(f\\,||\\,g) &= \\int f\\log\\frac{f}{g}\\\\\n    &= \\int f\\left(\\frac{g}{f}-1-\\log\\frac{g}{f}\\right).\n\\end{aligned}\\]\nNow note that for \\(t\\geq1\\) we have the inequality:\n\\[t - 1 -\\log t \\geq 0,\\]\nin which equality holds iff \\(t=1.\\) This can be easily established graphically or by means of elementary calculus. Applying this inequality to \\(f/g\\) and integrating yields\n\\[\\int \\frac{g}{f}-1-\\log\\frac{g}{f}\\;\\geq\\; 0\\]\nwith equality iff \\(f = g\\) a.e. ◻\n\n\n\n2.2.4. Corollary 2.2.4\nFor any pair of random variables \\(X,\\,Y\\): \\[I(X;Y)\\geq 0,\\] with equality if and only if \\(X\\) and \\(Y\\) are independent.\nProof.\n\\[I(X;Y) = D(f_{X,Y}\\,||\\,f_X\\otimes f_Y) \\geq 0\\]\nwith equality iff \\(f_{X,Y} = f_X\\otimes f_Y\\) a.e. i.e. iff \\(X\\) and \\(Y\\) are independent. ◻\n\n\n\n2.2.5. Corollary 2.2.5\nFor any pair of random variables \\(X,\\,Y\\):\n\\[h(X|Y) \\leq h(X),\\]\nwith equality if and only if \\(X\\) and \\(Y\\) are independent.\nProof.\n\\[h(X) - h(X|Y) = I(X;Y)\\,\\geq\\,0,\\]\nwith equality iff \\(X\\) and \\(Y\\) are independent by Corollary 3.2.4. ◻"
  },
  {
    "objectID": "posts/eh2/index.html",
    "href": "posts/eh2/index.html",
    "title": "Visualising the Geographic Distribution of Charity Donors with Interactive Leaflet Maps",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport folium\nfrom branca.colormap import LinearColormap\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nTable of contents\n1. Data Imports\n2. Geocoding Postcodes with ONS data\n3. Visualising Donor Data with Interactive Folium Maps\n3.1. First Map\n3.2. Adding Colour Labelling\n3.3. Adding Pop-Ups\n3.4. Adding Layer Control\n4. Constructing the Fake Data\n5. Remarks and Further Directions"
  },
  {
    "objectID": "posts/eh2/index.html#first-map",
    "href": "posts/eh2/index.html#first-map",
    "title": "Visualising the Geographic Distribution of Charity Donors with Interactive Leaflet Maps",
    "section": "3.1. First Map",
    "text": "3.1. First Map\nProducing a basic folium map with no extra features:\n\nm = folium.Map(location=[52.9548, -1.1581], zoom_start=12)\n\nfor index, row in df.iterrows():  \n    \n    folium.CircleMarker(\n        location=[row['Latitude'], row['Longitude']],\n        radius=5,\n        color='blue', \n        fill=True,\n        fill_color='blue',\n        fill_opacity=0.7,\n    ).add_to(m)\n    \nm.save('FakeDonorMap1.html')\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/eh2/index.html#adding-colour-labelling",
    "href": "posts/eh2/index.html#adding-colour-labelling",
    "title": "Visualising the Geographic Distribution of Charity Donors with Interactive Leaflet Maps",
    "section": "3.2. Adding Colour Labelling",
    "text": "3.2. Adding Colour Labelling\nWe can colour each point by the value of TotalDonated using LinearColormap from branca.colormap:\n\nm = folium.Map(location=[52.9548, -1.1581], zoom_start=12)\n\ncolors = ['green', 'yellow', 'orange', 'red', 'purple']\nlinear_colormap = LinearColormap(colors=colors,\n                                 index=[0, 100, 250, 500, 1000],\n                                 vmin=df['TotalDonated'].min(),\n                                 vmax=df['TotalDonated'].quantile(0.99025))\n\nfor index, row in df.iterrows():    \n    \n    total_don = row['TotalDonated']\n    color = linear_colormap(total_don)\n    \n    folium.CircleMarker(\n        location=[row['Latitude'], row['Longitude']],\n        radius=5,\n        color=color, \n        fill=True,\n        fill_color=color,\n        fill_opacity=1,\n    ).add_to(m)\n\nlinear_colormap.add_to(m)\n\nm.save('FakeDonorMap2.html')\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/eh2/index.html#adding-pop-ups",
    "href": "posts/eh2/index.html#adding-pop-ups",
    "title": "Visualising the Geographic Distribution of Charity Donors with Interactive Leaflet Maps",
    "section": "3.3. Adding Pop-Ups",
    "text": "3.3. Adding Pop-Ups\nNext, I added a pop-up to each data point showing the user the data of that donor by adding an argument popup to folium.CircleMarker:\n\nm = folium.Map(location=[52.9548, -1.1581], zoom_start=12)\n\ncolors = ['green', 'yellow', 'orange', 'red', 'purple']\nlinear_colormap = LinearColormap(colors=colors,\n                                 index=[0, 100, 250, 500, 1000],\n                                 vmin=df['TotalDonated'].min(),\n                                 vmax=df['TotalDonated'].quantile(0.99025))\n\nfor index, row in df.iterrows():    \n    num_don = row['NumberDonations']\n    total_don = row['TotalDonated']\n    news = bool(row['Newsletter'])\n    avg_don = row['AverageDonated']\n    \n    popup_text = f'&lt;div style=\"width: 175px;\"&gt;\\\n                  Total Donated: £{total_don:.2f}&lt;br&gt;\\\n                  Number of Donations: {num_don}&lt;br&gt;\\\n                  Average Donation: £{avg_don:.2f}&lt;br&gt;\\\n                  Subscribed to Newsletter: {news}\\\n                  &lt;/div&gt;'\n    \n    color = linear_colormap(total_don)\n    \n    folium.CircleMarker(\n        location=[row['Latitude'], row['Longitude']],\n        radius=5,\n        color=color, \n        fill=True,\n        fill_color=color,\n        fill_opacity=1,\n        popup=popup_text\n    ).add_to(m)\n\nlinear_colormap.add_to(m)\n\nm.save('FakeDonorMap_noLayerControl.html')\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nShowing a screenshot taken with ShareX using Matplotlib:\n\nplt.figure(figsize=(20,10))\nimg = mpimg.imread('FakeDonorMap_noLayerControl.jpg')\nimgplot = plt.imshow(img)\nplt.show()"
  },
  {
    "objectID": "posts/eh2/index.html#adding-layer-control",
    "href": "posts/eh2/index.html#adding-layer-control",
    "title": "Visualising the Geographic Distribution of Charity Donors with Interactive Leaflet Maps",
    "section": "3.4. Adding Layer Control",
    "text": "3.4. Adding Layer Control\nFinally, I added layer control to the map using folium.map.FeatureGroup and folium.LayerControl.\nThis results in buttons being added to the right of the UI, under the colour bar, allowing the user to show/hide the markers for donors in specific ranges of donation totals.\nI used Microsoft Copilot to assist with this step.\n\nm = folium.Map(location=[52.9548, -1.1581], zoom_start=12)\n\ncolors = ['green', 'yellow', 'orange', 'red', 'purple']\nlinear_colormap = LinearColormap(colors=colors,\n                                 index=[0, 100, 250, 500, 1000],\n                                 vmin=df['TotalDonated'].min(),\n                                 vmax=df['TotalDonated'].quantile(0.99025))\n\n# Create FeatureGroups\nfgroups = [folium.map.FeatureGroup(name=f\"Total Donated:  £{lower}-£{upper}\") for lower, upper in zip([0, 100, 250, 500, 750, 1000], [100, 250, 500, 750, 1000, float('inf')])]\n\nfor index, row in df.iterrows():    \n    num_don = row['NumberDonations']\n    total_don = row['TotalDonated']\n    news = bool(row['Newsletter'])\n    avg_don = row['AverageDonated']\n    \n    popup_text = f'&lt;div style=\"width: 175px;\"&gt;\\\n                  Total Donated: £{total_don:.2f}&lt;br&gt;\\\n                  Number of Donations: {num_don}&lt;br&gt;\\\n                  Average Donation: £{avg_don:.2f}&lt;br&gt;\\\n                  Subscribed to Newsletter: {news}\\\n                  &lt;/div&gt;'\n    \n    color = linear_colormap(total_don)\n    \n    marker = folium.CircleMarker(\n        location=[row['Latitude'], row['Longitude']],\n        radius=5,\n        color=color, \n        fill=True,\n        fill_color=color,\n        fill_opacity=1,\n        popup=popup_text\n    )\n    \n    # Add the marker to the appropriate FeatureGroup\n    for fgroup, (lower, upper) in zip(fgroups, zip([0, 100, 250, 500, 750, 1000], [100, 250, 500, 750, 1000, float('inf')])):\n        if lower &lt;= total_don &lt; upper:\n            fgroup.add_child(marker)\n            break\n\n# Add the FeatureGroups to the map\nfor fgroup in fgroups:\n    m.add_child(fgroup)\n\nlinear_colormap.add_to(m)\nm.add_child(folium.LayerControl())\n\n\nm.save('FakeDonorMap_withLayerControl.html')\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nShowing a screenshot taken with ShareX using Matplotlib:\n\nplt.figure(figsize=(20,10))\nimg = mpimg.imread('FakeDonorMap_withLayerControl.jpg')\nimgplot = plt.imshow(img)\nplt.show()"
  },
  {
    "objectID": "posts/eh4/index.html",
    "href": "posts/eh4/index.html",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "",
    "text": "An artificial dataset of charity donors spanning 3 years is preprocessed for clustering. Preprocessing steps includes feature selection by correlation analysis and dimensionality reduction using PCA. A random search is used to tune the hyperparameters of KMeans, GMM and BIRCH models for k=4 and k=5 clusters. The models are combined into an ensemble with hard voting to cluster donors into groups with similar donation patterns. The results of each clustering experiment are geographically presented through interactive maps and judged through visual inspection and comparing silhouette scores.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nimport folium\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans, Birch\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score, make_scorer\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom collections import Counter\npd.set_option('display.max_columns', None)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nTable of contents\n1.1. FEATURES\n2. PREPROCESSING\n2.1. CORRELATION ANALYSIS\n2.2. NEW FEATURES AFTER REMOVAL\n2.3. TRANSFORMING, SCALING, PCA, OUTLIERS\n2.3.1. Log transforming £ features\n2.3.2. StandardScaler\n2.3.3. PCA\n2.3.4. REMOVING OUTLIERS\n3. TUNING\n3.1. k = 4\n3.1.1. TUNING K MEANS\n3.1.2. TUNING GMM\n3.1.3. TUNING BIRCH\n3.2. k = 5\n3.2.1. TUNING K MEANS\n3.2.2. TUNING GMM\n3.2.3. TUNING BIRCH\n4. CODE TO PRODUCE FOLIUM VISUALISATIONS\n5. TUNED ENSEMBLES\n5.1. k = 4\n5.1.1. ENSEMBLE: k=4, UNTUNED\n5.1.2. ENSEMBLE: k=4, KMEANS TUNED\n5.1.3. ENSEMBLE: k=4, KMEANS AND GMM TUNED\n5.1.4. ENSEMBLE: k=4, ALL TUNED\n5.2. k = 5\n5.2.1. ENSEMBLE: k=5, UNTUNED\n5.2.2. ENSEMBLE: k=5, KMEANS TUNED\n5.2.3. ENSEMBLE: k=5, KMEANS & GMM TUNED\n5.2.4. ENSEMBLE: k=5, ALL TUNED\ndef drop_column(df, col_name):\n    if col_name in df.columns:\n        df = df.drop(columns=[col_name])\n    return df\ndf = pd.read_csv('../eh3/data.csv')\ndf['Transactions_DateOfFirstGift'] = pd.to_datetime(df['Transactions_DateOfFirstGift'])\ndf['Transactions_DateOfLastGift'] = pd.to_datetime(df['Transactions_DateOfLastGift'])"
  },
  {
    "objectID": "posts/eh4/index.html#features",
    "href": "posts/eh4/index.html#features",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "1.1. FEATURES",
    "text": "1.1. FEATURES\n\nunscaled_features = ['Transactions_LifetimeGiftsAmount',\n                    'Transactions_LifetimeGiftsNumber',\n                    'Transactions_AverageGiftAmount',\n\n                    'Transactions_Months1To12GiftsAmount',\n                    'Transactions_Months1To12GiftsNumber',\n                    'Transactions_Months13To24GiftsAmount',\n                    'Transactions_Months13To24GiftsNumber',\n                    'Transactions_Months25To36GiftsAmount',\n                    'Transactions_Months25To36GiftsNumber',\n                    'Transactions_FirstGiftAmount',\n                    'Transactions_LastGiftAmount',\n                    'monthsSinceFirstDonation',\n                    'monthsSinceLastDonation',\n                    'activeMonths',\n                    'Transactions_HighestGiftAmount',\n                    'Transactions_LowestGiftAmount',\n                   ]\n\n\nscaled_features = ['DonationFrequency',\n                   'DonationFrequencyActive',]\n\n\nbinary_features = []\n\n\nfeatures = unscaled_features + scaled_features + binary_features\n\n\nlen(features)\n\n18"
  },
  {
    "objectID": "posts/eh4/index.html#correlation-analysis",
    "href": "posts/eh4/index.html#correlation-analysis",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "2.1. CORRELATION ANALYSIS",
    "text": "2.1. CORRELATION ANALYSIS\n\nplt.figure(figsize=(12, 8))\n\n# Compute the correlation matrix\ncorr = df[features].corr(numeric_only=True)\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool), k=0)\n\n# Draw the heatmap with the mask\nsns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='magma', \n            cbar_kws={\"shrink\": .8}, linewidths=.5)\n\n# Rotate the y labels (ticks) to horizontal\nplt.yticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\n\nprint('15 most highly correlated pairs of features (in absolute value):\\n')\nprint(corr.abs().where(~mask).stack().sort_values(ascending=False).head(15))\n\n15 most highly correlated pairs of features (in absolute value):\n\nTransactions_Months13To24GiftsNumber  Transactions_LifetimeGiftsNumber       0.949797\nTransactions_LastGiftAmount           Transactions_AverageGiftAmount         0.934479\nTransactions_LowestGiftAmount         Transactions_AverageGiftAmount         0.928709\n                                      Transactions_LastGiftAmount            0.915917\nTransactions_FirstGiftAmount          Transactions_AverageGiftAmount         0.905685\nTransactions_Months1To12GiftsNumber   Transactions_LifetimeGiftsNumber       0.899897\nTransactions_Months13To24GiftsAmount  Transactions_LifetimeGiftsAmount       0.890478\nTransactions_Months1To12GiftsAmount   Transactions_LifetimeGiftsAmount       0.868661\nTransactions_LowestGiftAmount         Transactions_FirstGiftAmount           0.855994\nDonationFrequency                     Transactions_Months1To12GiftsNumber    0.834842\nTransactions_Months13To24GiftsNumber  Transactions_Months1To12GiftsNumber    0.823872\nTransactions_HighestGiftAmount        Transactions_AverageGiftAmount         0.816823\nTransactions_Months25To36GiftsNumber  Transactions_LifetimeGiftsNumber       0.811658\nTransactions_LastGiftAmount           Transactions_FirstGiftAmount           0.787498\nTransactions_HighestGiftAmount        Transactions_FirstGiftAmount           0.769152\ndtype: float64"
  },
  {
    "objectID": "posts/eh4/index.html#new-features-after-removal",
    "href": "posts/eh4/index.html#new-features-after-removal",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "2.2. NEW FEATURES AFTER REMOVAL",
    "text": "2.2. NEW FEATURES AFTER REMOVAL\n\nunscaled_features = ['Transactions_LifetimeGiftsAmount',\n                    'Transactions_LifetimeGiftsNumber',\n                    'Transactions_AverageGiftAmount',\n\n\n\n\n                    'Transactions_Months25To36GiftsAmount',\n                     \n                    'Transactions_FirstGiftAmount',\n                    'monthsSinceFirstDonation',\n                    'monthsSinceLastDonation',\n                    'activeMonths',\n\n                   ]\n\n\nscaled_features = ['DonationFrequency',\n                   'DonationFrequencyActive',]\n\n\nbinary_features = []\n\n\nfeatures = unscaled_features + scaled_features + binary_features\n\n\nlen(features)\n\n10"
  },
  {
    "objectID": "posts/eh4/index.html#transforming-scaling-pca-outliers",
    "href": "posts/eh4/index.html#transforming-scaling-pca-outliers",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "2.3. TRANSFORMING, SCALING, PCA, OUTLIERS",
    "text": "2.3. TRANSFORMING, SCALING, PCA, OUTLIERS\n\n2.3.1. Log transforming £ features\n\ndf_log = df[features].copy()\n\n# Logarithmically scaling down features representing donation totals\n# np.log1p is the function x |--&gt; ln(1+x)\n\ndf_log['Transactions_Months1To12GiftsAmount'] = np.log1p(df['Transactions_Months1To12GiftsAmount'])\ndf_log['Transactions_Months13To24GiftsAmount'] = np.log1p(df['Transactions_Months13To24GiftsAmount'])\ndf_log['Transactions_Months25To36GiftsAmount'] = np.log1p(df['Transactions_Months25To36GiftsAmount'])\n\ndf_log['Transactions_FirstGiftAmount'] = np.log1p(df['Transactions_FirstGiftAmount'])\ndf_log['Transactions_LastGiftAmount'] = np.log1p(df['Transactions_LastGiftAmount'])\n\ndf_log['Transactions_HighestGiftAmount'] = np.log1p(df['Transactions_HighestGiftAmount'])\ndf_log['Transactions_LowestGiftAmount'] = np.log1p(df['Transactions_LowestGiftAmount'])\n\ndf_log['Transactions_LifetimeGiftsAmount'] = np.log1p(df['Transactions_LifetimeGiftsAmount'])\n\ndf_log['Transactions_AverageGiftAmount'] = np.log1p(df['Transactions_LifetimeGiftsAmount'])/df['Transactions_LifetimeGiftsNumber']\n\n\ndf_log = df_log[features]\n\n\ndf_log.sample(10)\n\n\n\n\n\n\n\n\nTransactions_LifetimeGiftsAmount\nTransactions_LifetimeGiftsNumber\nTransactions_AverageGiftAmount\nTransactions_Months25To36GiftsAmount\nTransactions_FirstGiftAmount\nmonthsSinceFirstDonation\nmonthsSinceLastDonation\nactiveMonths\nDonationFrequency\nDonationFrequencyActive\n\n\n\n\n391\n4.418841\n3\n1.472947\n3.419037\n3.447763\n27\n3\n25\n0.107143\n0.120000\n\n\n305\n5.931529\n3\n1.977176\n4.624581\n4.593401\n29\n17\n13\n0.100000\n0.230769\n\n\n485\n4.115780\n2\n2.057890\n0.000000\n3.444257\n15\n15\n1\n0.125000\n2.000000\n\n\n533\n4.565805\n4\n1.141451\n3.477541\n3.429137\n27\n4\n24\n0.142857\n0.166667\n\n\n467\n6.584087\n36\n0.182891\n5.380634\n3.043093\n36\n1\n36\n0.972973\n1.000000\n\n\n143\n4.389250\n2\n2.194625\n0.000000\n3.716738\n14\n14\n1\n0.133333\n2.000000\n\n\n147\n3.741946\n2\n1.870973\n0.000000\n3.031099\n6\n6\n1\n0.285714\n2.000000\n\n\n667\n4.145038\n5\n0.829008\n3.088311\n3.088311\n27\n3\n25\n0.178571\n0.200000\n\n\n623\n5.312023\n2\n2.656012\n4.632396\n4.663722\n27\n27\n1\n0.071429\n2.000000\n\n\n97\n5.546466\n10\n0.554647\n4.467172\n3.001714\n31\n1\n31\n0.312500\n0.322581\n\n\n\n\n\n\n\n\n\n2.3.2. StandardScaler\n\n# Scaling non-binary features\nscaler = StandardScaler()\ndf_scaled = pd.DataFrame(scaler.fit_transform(df[unscaled_features]), columns=unscaled_features)\ndf_concat = pd.concat([df_scaled, df[scaled_features], df[binary_features]], axis=1)\n\n\n\n2.3.3. PCA\n\n# Choosing n_components by producing an explained variance plot\npca = PCA()\ndf_pca = pca.fit_transform(df_concat)\n\n# Calculate the cumulative explained variance\nexplained_variance = np.cumsum(pca.explained_variance_ratio_)\n\n# Plot the elbow graph\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\nplt.title('Explained Variance by Components')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\n\n# Add gridlines\nplt.grid(True)\n\n# Ensure non-fractional x labels\nax = plt.gca()\nax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Apply PCA with chosen number of components\npca = PCA(n_components=7)\ndf_pca = pd.DataFrame(pca.fit_transform(df_concat))\n\n\n\n2.3.4. REMOVING OUTLIERS\nWe declare a donor to be an outlier if they have any of Transactions_LifetimeGiftsAmount, Transactions_AverageGiftAmount or Transactions_LifetimeGiftsNumber at more than 3 standard deviations above the mean.\nOutliers are removed before clustering to improve interpretability of results.\n\n# Define thresholds at which to cutoff outliers and remove such rows from the dataframe\namnt_thres = df['Transactions_LifetimeGiftsAmount'].mean() + 3*df['Transactions_LifetimeGiftsAmount'].std()\navg_thres = df['Transactions_AverageGiftAmount'].mean() + 3*df['Transactions_AverageGiftAmount'].std()\nnum_thres = df['Transactions_LifetimeGiftsNumber'].mean() + 3*df['Transactions_LifetimeGiftsNumber'].std()\nmask = ~((df['Transactions_LifetimeGiftsAmount'] &gt; amnt_thres) | (df['Transactions_AverageGiftAmount'] &gt; avg_thres) | (df['Transactions_LifetimeGiftsNumber'] &gt; num_thres))\ndf_pca_without_outliers = df_pca[mask]"
  },
  {
    "objectID": "posts/eh4/index.html#k-4",
    "href": "posts/eh4/index.html#k-4",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "3.1. k = 4",
    "text": "3.1. k = 4\n\n3.1.1. TUNING K MEANS\n\nk = 4\n\n# Define the parameter grid\nparam_grid = {\n    'init': ['k-means++', 'random'],\n    'n_init': list(range(10, 200)), \n    'algorithm': ['auto', 'lloyd', 'elkan'],\n    'max_iter': list(range(100, 500)),\n    'tol': [1e-4, 1e-3, 1e-2, 1e-1, 1],\n}\n\n\n# Create a KMeans instance with a fixed number of clusters\nkmeans = KMeans(n_clusters=k)\n\n# Create a silhouette scorer\nsilhouette_scorer = make_scorer(silhouette_score)\n\n# Create a RandomizedSearchCV instance\nrandom_search = RandomizedSearchCV(kmeans, \n                                   param_grid, \n                                   n_iter=50,\n                                   cv=3, \n                                   random_state=594,\n                                   scoring=silhouette_scorer,\n                                  )\n\n# Fit the RandomizedSearchCV instance \nrandom_search.fit(df_pca_without_outliers)\n\n# Get the best parameters\nbest_params_kmeans_4 = random_search.best_params_\n\n# Print the best parameters\nprint(f'Best params for KMeans with k={k}:\\n')\nprint(best_params_kmeans_4)\n\nBest params for KMeans with k=4:\n\n{'tol': 0.0001, 'n_init': 129, 'max_iter': 238, 'init': 'k-means++', 'algorithm': 'elkan'}\n\n\n\n\n3.1.2. TUNING GMM\n\nk = 4\n\n# Define the parameter grid for GMM\nparam_grid_gmm = {\n    'n_init': list(range(1, 21)), \n    'covariance_type': ['full', 'tied', 'diag', 'spherical'],\n    'tol': [1e-3, 1e-4, 1e-5, 1e-6],\n    'reg_covar': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],\n    'max_iter': list(range(100, 500)) \n}\n\n\n# Create a GMM instance\ngmm = GaussianMixture(n_components=k)\n\n# Create a silhouette scorer\nsilhouette_scorer = make_scorer(silhouette_score)\n\n# Create a RandomizedSearchCV instance for GMM\nrandom_search_gmm = RandomizedSearchCV(gmm, \n                                       param_grid_gmm, \n                                       n_iter=50, \n                                       cv=3, \n                                       random_state=594,\n                                       scoring=silhouette_scorer,\n                                      )\n\n# Fit the RandomizedSearchCV instance to your data\nrandom_search_gmm.fit(df_pca_without_outliers)\n\n# Get the best parameters for GMM\nbest_params_gmm_4 = random_search_gmm.best_params_\n\n# Print the best parameters for GMM\nprint(f'Best params for GMM with k={k}:\\n')\nprint(best_params_gmm_4)\n\nBest params for GMM with k=4:\n\n{'tol': 1e-06, 'reg_covar': 1, 'n_init': 20, 'max_iter': 376, 'covariance_type': 'diag'}\n\n\n\n\n3.1.3. TUNING BIRCH\n\nk = 4\n\n# Define the parameter grid for BIRCH\nparam_grid_birch = {\n    'threshold': [0.1, 0.3, 0.5, 0.7, 0.9],\n    'branching_factor': list(range(20, 100))\n}\n\n# Create a BIRCH instance\nbirch = Birch(n_clusters=k)\n\n# Create a silhouette scorer\nsilhouette_scorer = make_scorer(silhouette_score)\n\n# Create a RandomizedSearchCV instance for BIRCH\nrandom_search_birch = RandomizedSearchCV(birch, \n                                         param_grid_birch, \n                                         n_iter=50, \n                                         cv=3, \n                                         random_state=594,\n                                         scoring=silhouette_scorer,\n                                        )\n\n# Fit the RandomizedSearchCV instance to your data\nrandom_search_birch.fit(df_pca_without_outliers)\n\n# Get the best parameters for BIRCH\nbest_params_birch_4 = random_search_birch.best_params_\n\n# Print the best parameters for BIRCH\nprint(f'Best params for BIRCH with k={k}:\\n')\nprint(best_params_birch_4)\n\nBest params for BIRCH with k=4:\n\n{'threshold': 0.7, 'branching_factor': 51}"
  },
  {
    "objectID": "posts/eh4/index.html#k-5",
    "href": "posts/eh4/index.html#k-5",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "3.2. k = 5",
    "text": "3.2. k = 5\n\n3.2.1. TUNING K MEANS\n\nk = 5\n\nparam_grid = {\n    'init': ['k-means++', 'random'],\n    'n_init': list(range(10, 200)), \n    'algorithm': ['auto', 'lloyd', 'elkan'],\n    'max_iter': list(range(100, 500)),\n    'tol': [1e-4, 1e-3, 1e-2, 1e-1, 1],\n}\n\n\n# Create a KMeans instance with n clusters\nkmeans = KMeans(n_clusters=k)\n\n# Create a silhouette scorer\nsilhouette_scorer = make_scorer(silhouette_score)\n\n# Create a RandomizedSearchCV instance\nrandom_search = RandomizedSearchCV(kmeans, \n                                   param_grid, \n                                   n_iter=50,\n                                   cv=3, \n                                   random_state=594,\n                                   scoring=silhouette_scorer,\n                                  )\n\n# Fit the RandomizedSearchCV instance\nrandom_search.fit(df_pca_without_outliers)\n\n# Get the best parameters\nbest_params_kmeans_5 = random_search.best_params_\n\n# Print the best parameters\nprint(f'Best params for KMeans with k={k}:\\n')\nprint(best_params_kmeans_5)\n\nBest params for KMeans with k=5:\n\n{'tol': 0.0001, 'n_init': 129, 'max_iter': 238, 'init': 'k-means++', 'algorithm': 'elkan'}\n\n\n\n\n3.2.2. TUNING GMM\n\nk = 5\n\n# Define the parameter grid for GMM\nparam_grid_gmm = {\n    'n_init': list(range(1, 21)), \n    'covariance_type': ['full', 'tied', 'diag', 'spherical'],\n    'tol': [1e-3, 1e-4, 1e-5, 1e-6],\n    'reg_covar': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],\n    'max_iter': list(range(100, 500)) \n}\n\n# Create a GMM instance\ngmm = GaussianMixture(n_components=k)\n\n# Create a silhouette scorer\nsilhouette_scorer = make_scorer(silhouette_score)\n\n# Create a RandomizedSearchCV instance for GMM\nrandom_search_gmm = RandomizedSearchCV(gmm, \n                                       param_grid_gmm, \n                                       n_iter=50, \n                                       cv=3, \n                                       random_state=594,\n                                       scoring=silhouette_scorer,\n                                      )\n\n# Fit the RandomizedSearchCV instance to your data\nrandom_search_gmm.fit(df_pca_without_outliers)\n\n# Get the best parameters for GMM\nbest_params_gmm_5 = random_search_gmm.best_params_\n\n# Print the best parameters for GMM\nprint(f'Best params for GMM with k={k}:\\n')\nprint(best_params_gmm_5)\n\nBest params for GMM with k=5:\n\n{'tol': 1e-06, 'reg_covar': 1, 'n_init': 20, 'max_iter': 376, 'covariance_type': 'diag'}\n\n\n\n\n3.2.3. TUNING BIRCH\n\nk = 5\n\n# Define the parameter grid for BIRCH\nparam_grid_birch = {\n    'threshold': np.arange(0.05, 1.05, 0.05),\n    'branching_factor': list(range(20, 200))\n}\n\n# Create a BIRCH instance\nbirch = Birch(n_clusters=k)\n\n# Create a silhouette scorer\nsilhouette_scorer = make_scorer(silhouette_score)\n\n# Create a RandomizedSearchCV instance for BIRCH\nrandom_search_birch = RandomizedSearchCV(birch, \n                                         param_grid_birch, \n                                         n_iter=50, \n                                         cv=3, \n                                         random_state=594,\n                                         scoring=silhouette_scorer,\n                                        )\n\n# Fit the RandomizedSearchCV instance to your data\nrandom_search_birch.fit(df_pca_without_outliers)\n\n# Get the best parameters for BIRCH\nbest_params_birch_5 = random_search_birch.best_params_\n\n# Print the best parameters for BIRCH\nprint(f'Best params for KMeans with k={k}:\\n')\nprint(best_params_birch_5)\n\nBest params for KMeans with k=5:\n\n{'threshold': 0.7500000000000001, 'branching_factor': 80}"
  },
  {
    "objectID": "posts/eh4/index.html#k-4-1",
    "href": "posts/eh4/index.html#k-4-1",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "5.1. k = 4",
    "text": "5.1. k = 4\n\n5.1.1. ENSEMBLE: k=4, UNTUNED\n\nk = 4\n\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594),\n\n    \n    Birch(n_clusters=k),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.32390613387277556\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n3    230\n0    227\n1    202\n2     59\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n5.1.2. ENSEMBLE: k=4, KMEANS TUNED\n\nk = 4\n\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594,\n          **best_params_kmeans_4),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594),\n\n    \n    Birch(n_clusters=k),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.2949886894806673\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n2    231\n3    196\n1    184\n0    107\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n5.1.3. ENSEMBLE: k=4, KMEANS AND GMM TUNED\n\nk = 4\n\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594,\n          **best_params_kmeans_4),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594,\n                **best_params_gmm_4),\n\n    \n    Birch(n_clusters=k),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.15953504480250802\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n2    327\n0    219\n1    155\n3     17\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n5.1.4. ENSEMBLE: k=4, ALL TUNED\n\nk = 4\n\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594,\n          **best_params_kmeans_4),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594,\n                **best_params_gmm_4),\n\n    \n    Birch(n_clusters=k,\n          **best_params_birch_4),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.21204320844898658\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n2    327\n1    188\n0    185\n3     18\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/eh4/index.html#k-5-1",
    "href": "posts/eh4/index.html#k-5-1",
    "title": "Donor Segmentation Using an Ensemble of Clustering Algorithms - Hyperparameter Tuning",
    "section": "5.2. k = 5",
    "text": "5.2. k = 5\n\n5.2.1. ENSEMBLE: k=5, UNTUNED\n\nk = 5\n\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594),\n\n    \n    Birch(n_clusters=k),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.3549508024992924\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n0    224\n3    188\n1    165\n4     76\n2     65\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n5.2.2. ENSEMBLE: k=5, KMEANS TUNED\n\nk = 4\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594,\n          **best_params_kmeans_5),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594),\n\n    \n    Birch(n_clusters=k),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.2949886894806673\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n2    231\n3    196\n1    184\n0    107\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n5.2.3. ENSEMBLE: k=5, KMEANS & GMM TUNED\n\nk = 5\n\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594,\n          **best_params_kmeans_5),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594,\n                **best_params_gmm_5),\n\n    \n    Birch(n_clusters=k),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.39211366933147423\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n2    269\n1    175\n4    148\n0     81\n3     45\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n5.2.4. ENSEMBLE: k=5, ALL TUNED\n\nk = 5\n\n# Drop 'ClusterLabel' if present from a previous run\n# (Otherwise the 'ClusterLabel' feature would be used in the clustering)\ndf = drop_column(df, 'ClusterLabel')\ndf_pca_without_outliers = drop_column(df_pca_without_outliers, 'ClusterLabel')\n\n\n# Create the clustering models\nmodels = [\n    KMeans(n_clusters=k, \n           random_state=594,\n          **best_params_kmeans_5),\n\n    \n    GaussianMixture(n_components=k, \n                    random_state=594,\n                **best_params_gmm_5),\n\n    \n    Birch(n_clusters=k,\n          **best_params_birch_5),\n]\n\n# Fit the models and predict the labels\nlabels = []\nfor model in models:\n    model.fit(df_pca_without_outliers)\n    if isinstance(model, GaussianMixture):\n        label = model.predict(df_pca_without_outliers)  # For GMM, use predict to get hard labels\n    else:\n        label = model.labels_\n    labels.append(label)\n\n# Combine the labels using a majority voting scheme\nlabels_ensemble = []\nfor i in range(len(df_pca_without_outliers)):\n    # Get the labels for the i-th data point from all models\n    labels_i = [labels_j[i] for labels_j in labels]\n    # Get the most common label\n    most_common_label = Counter(labels_i).most_common(1)[0][0]\n    labels_ensemble.append(most_common_label)\n\n# Get and print avg silhouette score\nsilhouette_avg = silhouette_score(df_pca_without_outliers, labels_ensemble)\nprint(\"Average silhouette score of clustering:\", silhouette_avg)\n\nAverage silhouette score of clustering: 0.37188413554296684\n\n\n\n# Assign the ensemble labels to the dataframe\ndf_pca_without_outliers['ClusterLabel'] = labels_ensemble\ndf_pca_without_outliers['ClusterLabel'].value_counts()\n\nClusterLabel\n2    281\n1    177\n4    148\n0     81\n3     31\nName: count, dtype: int64\n\n\n\n# Producing interactive visualisation of clustering results with folium\ngeo = df.merge(df_pca_without_outliers[['ClusterLabel']], left_index=True, right_index=True, how='left')\ngeo = geo.dropna()\ngeo['ClusterLabel'] = geo['ClusterLabel'].astype(int)\nmake_map(geo)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/k2/index.html",
    "href": "posts/k2/index.html",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nimport xgboost as xgb\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\nimport optuna\nprint(\"Python version:\")\n!python --version\n\nPython version:\nPython 3.11.4\nprint(\"Optuna version:\")\nprint(optuna.__version__)\n\nOptuna version:\n3.5.0\nprint(\"TensorFlow version:\")\nprint(tf.__version__)\n\nTensorFlow version:\n2.14.0\nTable of contents\n1. Data Imports\n2. Exploratory Data Analysis\n3. Missing Values and Feature Engineering\n3.1. emp_title and emp_length\n3.2. Filling mort_acc using total_acc\n3.3. Categorical Features\n4. Preprocessing\n4.1. TRAIN/TEST SPLIT\n4.2. Normalising the Data\n5. Model Selection and Hyperparameter Tuning with Optuna\n6. Fitting the Best Model and Predicting on Test Set\n7. Examining Misclassified Points\n7.1. Producing some visualisations investigating the distribution of misclassified points\n8. Remarks & Further Directions"
  },
  {
    "objectID": "posts/k2/index.html#section",
    "href": "posts/k2/index.html#section",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "—-",
    "text": "—-\nWe use a modified LendingClub dataset from Kaggle, sourced from Udemy course by Jose Portilla.\nBefore preprocessing, the dataset has the following features:\n\n\n\n\n\n\nLoanStatNew\n\n\nDescription\n\n\n\n\n\n\n0\n\n\nloan_amnt\n\n\nThe listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\n\n\n\n\n1\n\n\nterm\n\n\nThe number of payments on the loan. Values are in months and can be either 36 or 60.\n\n\n\n\n2\n\n\nint_rate\n\n\nInterest Rate on the loan\n\n\n\n\n3\n\n\ninstallment\n\n\nThe monthly payment owed by the borrower if the loan originates.\n\n\n\n\n4\n\n\ngrade\n\n\nLC assigned loan grade\n\n\n\n\n5\n\n\nsub_grade\n\n\nLC assigned loan subgrade\n\n\n\n\n6\n\n\nemp_title\n\n\nThe job title supplied by the Borrower when applying for the loan.*\n\n\n\n\n7\n\n\nemp_length\n\n\nEmployment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n\n\n\n\n8\n\n\nhome_ownership\n\n\nThe home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER\n\n\n\n\n9\n\n\nannual_inc\n\n\nThe self-reported annual income provided by the borrower during registration.\n\n\n\n\n10\n\n\nverification_status\n\n\nIndicates if income was verified by LC, not verified, or if the income source was verified\n\n\n\n\n11\n\n\nissue_d\n\n\nThe month which the loan was funded\n\n\n\n\n12\n\n\nloan_status\n\n\nCurrent status of the loan\n\n\n\n\n13\n\n\npurpose\n\n\nA category provided by the borrower for the loan request.\n\n\n\n\n14\n\n\ntitle\n\n\nThe loan title provided by the borrower\n\n\n\n\n15\n\n\nzip_code\n\n\nThe first 3 numbers of the zip code provided by the borrower in the loan application.\n\n\n\n\n16\n\n\naddr_state\n\n\nThe state provided by the borrower in the loan application\n\n\n\n\n17\n\n\ndti\n\n\nA ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n\n\n\n\n18\n\n\nearliest_cr_line\n\n\nThe month the borrower’s earliest reported credit line was opened\n\n\n\n\n19\n\n\nopen_acc\n\n\nThe number of open credit lines in the borrower’s credit file.\n\n\n\n\n20\n\n\npub_rec\n\n\nNumber of derogatory public records\n\n\n\n\n21\n\n\nrevol_bal\n\n\nTotal credit revolving balance\n\n\n\n\n22\n\n\nrevol_util\n\n\nRevolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n\n\n\n\n23\n\n\ntotal_acc\n\n\nThe total number of credit lines currently in the borrower’s credit file\n\n\n\n\n24\n\n\ninitial_list_status\n\n\nThe initial listing status of the loan. Possible values are – W, F\n\n\n\n\n25\n\n\napplication_type\n\n\nIndicates whether the loan is an individual application or a joint application with two co-borrowers\n\n\n\n\n26\n\n\nmort_acc\n\n\nNumber of mortgage accounts.\n\n\n\n\n27\n\n\npub_rec_bankruptcies\n\n\nNumber of public record bankruptcies"
  },
  {
    "objectID": "posts/k2/index.html#section-1",
    "href": "posts/k2/index.html#section-1",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "—",
    "text": "—\n\ndf = pd.read_csv('lending_club_loan.csv')\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 396030 entries, 0 to 396029\nData columns (total 27 columns):\n #   Column                Non-Null Count   Dtype  \n---  ------                --------------   -----  \n 0   loan_amnt             396030 non-null  float64\n 1   term                  396030 non-null  object \n 2   int_rate              396030 non-null  float64\n 3   installment           396030 non-null  float64\n 4   grade                 396030 non-null  object \n 5   sub_grade             396030 non-null  object \n 6   emp_title             373103 non-null  object \n 7   emp_length            377729 non-null  object \n 8   home_ownership        396030 non-null  object \n 9   annual_inc            396030 non-null  float64\n 10  verification_status   396030 non-null  object \n 11  issue_d               396030 non-null  object \n 12  loan_status           396030 non-null  object \n 13  purpose               396030 non-null  object \n 14  title                 394274 non-null  object \n 15  dti                   396030 non-null  float64\n 16  earliest_cr_line      396030 non-null  object \n 17  open_acc              396030 non-null  float64\n 18  pub_rec               396030 non-null  float64\n 19  revol_bal             396030 non-null  float64\n 20  revol_util            395754 non-null  float64\n 21  total_acc             396030 non-null  float64\n 22  initial_list_status   396030 non-null  object \n 23  application_type      396030 non-null  object \n 24  mort_acc              358235 non-null  float64\n 25  pub_rec_bankruptcies  395495 non-null  float64\n 26  address               396030 non-null  object \ndtypes: float64(12), object(15)\nmemory usage: 81.6+ MB\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nloan_amnt\nterm\nint_rate\ninstallment\ngrade\nsub_grade\nemp_title\nemp_length\nhome_ownership\nannual_inc\n...\nopen_acc\npub_rec\nrevol_bal\nrevol_util\ntotal_acc\ninitial_list_status\napplication_type\nmort_acc\npub_rec_bankruptcies\naddress\n\n\n\n\n0\n10000.0\n36 months\n11.44\n329.48\nB\nB4\nMarketing\n10+ years\nRENT\n117000.0\n...\n16.0\n0.0\n36369.0\n41.8\n25.0\nw\nINDIVIDUAL\n0.0\n0.0\n0174 Michelle Gateway\\r\\nMendozaberg, OK 22690\n\n\n1\n8000.0\n36 months\n11.99\n265.68\nB\nB5\nCredit analyst\n4 years\nMORTGAGE\n65000.0\n...\n17.0\n0.0\n20131.0\n53.3\n27.0\nf\nINDIVIDUAL\n3.0\n0.0\n1076 Carney Fort Apt. 347\\r\\nLoganmouth, SD 05113\n\n\n2\n15600.0\n36 months\n10.49\n506.97\nB\nB3\nStatistician\n&lt; 1 year\nRENT\n43057.0\n...\n13.0\n0.0\n11987.0\n92.2\n26.0\nf\nINDIVIDUAL\n0.0\n0.0\n87025 Mark Dale Apt. 269\\r\\nNew Sabrina, WV 05113\n\n\n3\n7200.0\n36 months\n6.49\n220.65\nA\nA2\nClient Advocate\n6 years\nRENT\n54000.0\n...\n6.0\n0.0\n5472.0\n21.5\n13.0\nf\nINDIVIDUAL\n0.0\n0.0\n823 Reid Ford\\r\\nDelacruzside, MA 00813\n\n\n4\n24375.0\n60 months\n17.27\n609.33\nC\nC5\nDestiny Management Inc.\n9 years\nMORTGAGE\n55000.0\n...\n13.0\n0.0\n24584.0\n69.8\n43.0\nf\nINDIVIDUAL\n1.0\n0.0\n679 Luna Roads\\r\\nGreggshire, VA 11650\n\n\n\n\n5 rows × 27 columns"
  },
  {
    "objectID": "posts/k2/index.html#emp_title-and-emp_length",
    "href": "posts/k2/index.html#emp_title-and-emp_length",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "3.1. emp_title and emp_length",
    "text": "3.1. emp_title and emp_length\n\ndf['emp_title']\n\n0                        Marketing\n1                  Credit analyst \n2                     Statistician\n3                  Client Advocate\n4          Destiny Management Inc.\n                    ...           \n396025            licensed bankere\n396026                       Agent\n396027                City Carrier\n396028        Gracon Services, Inc\n396029    Internal Revenue Service\nName: emp_title, Length: 396030, dtype: object\n\n\n\ndf['emp_title'].nunique()\n\n173105\n\n\nemp_title has too many unique values to encode\n\ndf = df.drop('emp_title', axis=1)\n\n\ndf['emp_length']\n\n0         10+ years\n1           4 years\n2          &lt; 1 year\n3           6 years\n4           9 years\n            ...    \n396025      2 years\n396026      5 years\n396027    10+ years\n396028    10+ years\n396029    10+ years\nName: emp_length, Length: 396030, dtype: object\n\n\n\nsorted(df['emp_length'].dropna().unique())\n\n['1 year',\n '10+ years',\n '2 years',\n '3 years',\n '4 years',\n '5 years',\n '6 years',\n '7 years',\n '8 years',\n '9 years',\n '&lt; 1 year']\n\n\n\nordered_emp_lengths = ['&lt; 1 year',\n '1 year',\n '2 years',\n '3 years',\n '4 years',\n '5 years',\n '6 years',\n '7 years',\n '8 years',\n '9 years',\n '10+ years']\n\n\nplt.figure(figsize=(12,5))\nsns.countplot(data=df,\n              x='emp_length',\n              order=ordered_emp_lengths,\n              palette='viridis')\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12,5))\nsns.countplot(data=df,\n              x='emp_length',\n              order=ordered_emp_lengths,\n              hue='loan_repaid')\n\n\n\n\n\n\n\n\n\nemp_co = df[df['loan_repaid'] == 0].groupby('emp_length').count()['loan_repaid']\nemp_fp = df[df['loan_repaid'] == 1].groupby('emp_length').count()['loan_repaid']\n\nplt.figure(figsize=(6,2))\n((emp_co/(emp_fp+emp_co))*100).plot(kind='bar')\n\n\n\n\n\n\n\n\nemp_length doesn’t look to tell us anything about loan_repaid. Very similar fractions of all employment lengths did not manage to repay their loans, so we drop this column too.\n\ndf = df.drop('emp_length',axis=1)"
  },
  {
    "objectID": "posts/k2/index.html#filling-mort_acc-using-total_acc",
    "href": "posts/k2/index.html#filling-mort_acc-using-total_acc",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "3.2. Filling mort_acc using total_acc",
    "text": "3.2. Filling mort_acc using total_acc\n\npercentage_null(df)\n\n\nPercentages of values missing:\n\nloan_amnt               0.00\nterm                    0.00\nint_rate                0.00\ninstallment             0.00\ngrade                   0.00\nsub_grade               0.00\nhome_ownership          0.00\nannual_inc              0.00\nverification_status     0.00\nissue_d                 0.00\nloan_status             0.00\npurpose                 0.00\ndti                     0.00\nearliest_cr_line        0.00\nopen_acc                0.00\npub_rec                 0.00\nrevol_bal               0.00\nrevol_util              0.07\ntotal_acc               0.00\ninitial_list_status     0.00\napplication_type        0.00\nmort_acc                9.54\npub_rec_bankruptcies    0.14\naddress                 0.00\nloan_repaid             0.00\ndtype: float64\n\n\n\ndf['mort_acc'].value_counts()\n\nmort_acc\n0.0     139777\n1.0      60416\n2.0      49948\n3.0      38049\n4.0      27887\n5.0      18194\n6.0      11069\n7.0       6052\n8.0       3121\n9.0       1656\n10.0       865\n11.0       479\n12.0       264\n13.0       146\n14.0       107\n15.0        61\n16.0        37\n17.0        22\n18.0        18\n19.0        15\n20.0        13\n24.0        10\n22.0         7\n21.0         4\n25.0         4\n27.0         3\n32.0         2\n31.0         2\n23.0         2\n26.0         2\n28.0         1\n30.0         1\n34.0         1\nName: count, dtype: int64\n\n\n\ndf.corr(numeric_only=True)['mort_acc'].sort_values()\n\nint_rate               -0.082583\ndti                    -0.025439\nrevol_util              0.007514\npub_rec                 0.011552\npub_rec_bankruptcies    0.027239\nloan_repaid             0.073111\nopen_acc                0.109205\ninstallment             0.193694\nrevol_bal               0.194925\nloan_amnt               0.222315\nannual_inc              0.236320\ntotal_acc               0.381072\nmort_acc                1.000000\nName: mort_acc, dtype: float64\n\n\nmort_acc correlates most strongly with total_acc, so we can leverage the fact that total_acc has no missing values to fill the missing values in mort_acc\n\ntotal_acc_avgs = df.groupby('total_acc').mean(numeric_only=True)['mort_acc']\n\n\ndef fill_mort_acc(total_acc,mort_acc):\n    \n    if np.isnan(mort_acc):\n        return total_acc_avgs[total_acc]\n    else:\n        return mort_acc\n\n\ndf['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'],x['mort_acc']),axis=1)\n\n\ndf.isnull().sum()\n\nloan_amnt                 0\nterm                      0\nint_rate                  0\ninstallment               0\ngrade                     0\nsub_grade                 0\nhome_ownership            0\nannual_inc                0\nverification_status       0\nissue_d                   0\nloan_status               0\npurpose                   0\ndti                       0\nearliest_cr_line          0\nopen_acc                  0\npub_rec                   0\nrevol_bal                 0\nrevol_util              276\ntotal_acc                 0\ninitial_list_status       0\napplication_type          0\nmort_acc                  0\npub_rec_bankruptcies    535\naddress                   0\nloan_repaid               0\ndtype: int64\n\n\n\npercentage_null(df)\n\n\nPercentages of values missing:\n\nloan_amnt               0.00\nterm                    0.00\nint_rate                0.00\ninstallment             0.00\ngrade                   0.00\nsub_grade               0.00\nhome_ownership          0.00\nannual_inc              0.00\nverification_status     0.00\nissue_d                 0.00\nloan_status             0.00\npurpose                 0.00\ndti                     0.00\nearliest_cr_line        0.00\nopen_acc                0.00\npub_rec                 0.00\nrevol_bal               0.00\nrevol_util              0.07\ntotal_acc               0.00\ninitial_list_status     0.00\napplication_type        0.00\nmort_acc                0.00\npub_rec_bankruptcies    0.14\naddress                 0.00\nloan_repaid             0.00\ndtype: float64\n\n\nOnly a neglible amount of rows still have missing data points, so we now feel comfortable dropping these rows\n\ndf = df.dropna()\n\n\npercentage_null(df)\n\n\nPercentages of values missing:\n\nloan_amnt               0.0\nterm                    0.0\nint_rate                0.0\ninstallment             0.0\ngrade                   0.0\nsub_grade               0.0\nhome_ownership          0.0\nannual_inc              0.0\nverification_status     0.0\nissue_d                 0.0\nloan_status             0.0\npurpose                 0.0\ndti                     0.0\nearliest_cr_line        0.0\nopen_acc                0.0\npub_rec                 0.0\nrevol_bal               0.0\nrevol_util              0.0\ntotal_acc               0.0\ninitial_list_status     0.0\napplication_type        0.0\nmort_acc                0.0\npub_rec_bankruptcies    0.0\naddress                 0.0\nloan_repaid             0.0\ndtype: float64"
  },
  {
    "objectID": "posts/k2/index.html#categorical-features",
    "href": "posts/k2/index.html#categorical-features",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "3.3. Categorical Features",
    "text": "3.3. Categorical Features\nHere we treat the categorical features such as home_ownership and purpose by either dropping them or one-hot encoding them as to end with a dataframe consisting only of numeric features.\n\ndf.select_dtypes(['object']).columns\n\nIndex(['term', 'grade', 'sub_grade', 'home_ownership', 'verification_status',\n       'issue_d', 'loan_status', 'purpose', 'earliest_cr_line',\n       'initial_list_status', 'application_type', 'address'],\n      dtype='object')\n\n\n\ndf['term'].value_counts()\n\nterm\n 36 months    301247\n 60 months     93972\nName: count, dtype: int64\n\n\n\ndf['term'] = df['term'].apply(lambda x: int(x.split()[0]))\n\n\ndf['term']\n\n0         36\n1         36\n2         36\n3         36\n4         60\n          ..\n396025    60\n396026    36\n396027    36\n396028    60\n396029    36\nName: term, Length: 395219, dtype: int64\n\n\n\ndf['zip_code'] = df['address'].apply(lambda address: address[-5:])\n\n\ndf['zip_code'].value_counts()\n\nzip_code\n70466    56880\n22690    56413\n30723    56402\n48052    55811\n00813    45725\n29597    45393\n05113    45300\n11650    11210\n93700    11126\n86630    10959\nName: count, dtype: int64\n\n\n\ndf['home_ownership'].value_counts()\n\nhome_ownership\nMORTGAGE    198022\nRENT        159395\nOWN          37660\nOTHER          110\nNONE            29\nANY              3\nName: count, dtype: int64\n\n\n\ndf['home_ownership'] = df['home_ownership'].replace(['NONE','ANY'],'OTHER')\n\n\ndf['home_ownership'].value_counts()\n\nhome_ownership\nMORTGAGE    198022\nRENT        159395\nOWN          37660\nOTHER          142\nName: count, dtype: int64\n\n\none-hot encoding with pandas.get_dummies:\n\ncolumns_to_1hot = ['home_ownership',\n                   'verification_status',\n                   'application_type',\n                   'initial_list_status',\n                   'purpose',\n                   'zip_code',\n                   'sub_grade']\n\n\ndf = pd.get_dummies(data=df,\n                    columns=columns_to_1hot,\n                    drop_first=True,\n                    dtype=int)\n\n\ndf.columns\n\nIndex(['loan_amnt', 'term', 'int_rate', 'installment', 'grade', 'annual_inc',\n       'issue_d', 'loan_status', 'dti', 'earliest_cr_line', 'open_acc',\n       'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'mort_acc',\n       'pub_rec_bankruptcies', 'address', 'loan_repaid',\n       'home_ownership_OTHER', 'home_ownership_OWN', 'home_ownership_RENT',\n       'verification_status_Source Verified', 'verification_status_Verified',\n       'application_type_INDIVIDUAL', 'application_type_JOINT',\n       'initial_list_status_w', 'purpose_credit_card',\n       'purpose_debt_consolidation', 'purpose_educational',\n       'purpose_home_improvement', 'purpose_house', 'purpose_major_purchase',\n       'purpose_medical', 'purpose_moving', 'purpose_other',\n       'purpose_renewable_energy', 'purpose_small_business',\n       'purpose_vacation', 'purpose_wedding', 'zip_code_05113',\n       'zip_code_11650', 'zip_code_22690', 'zip_code_29597', 'zip_code_30723',\n       'zip_code_48052', 'zip_code_70466', 'zip_code_86630', 'zip_code_93700',\n       'sub_grade_A2', 'sub_grade_A3', 'sub_grade_A4', 'sub_grade_A5',\n       'sub_grade_B1', 'sub_grade_B2', 'sub_grade_B3', 'sub_grade_B4',\n       'sub_grade_B5', 'sub_grade_C1', 'sub_grade_C2', 'sub_grade_C3',\n       'sub_grade_C4', 'sub_grade_C5', 'sub_grade_D1', 'sub_grade_D2',\n       'sub_grade_D3', 'sub_grade_D4', 'sub_grade_D5', 'sub_grade_E1',\n       'sub_grade_E2', 'sub_grade_E3', 'sub_grade_E4', 'sub_grade_E5',\n       'sub_grade_F1', 'sub_grade_F2', 'sub_grade_F3', 'sub_grade_F4',\n       'sub_grade_F5', 'sub_grade_G1', 'sub_grade_G2', 'sub_grade_G3',\n       'sub_grade_G4', 'sub_grade_G5'],\n      dtype='object')\n\n\n\ndf['earliest_cr_line']\n\n0         Jun-1990\n1         Jul-2004\n2         Aug-2007\n3         Sep-2006\n4         Mar-1999\n            ...   \n396025    Nov-2004\n396026    Feb-2006\n396027    Mar-1997\n396028    Nov-1990\n396029    Sep-1998\nName: earliest_cr_line, Length: 395219, dtype: object\n\n\n\ndf['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda x: int(x.split('-')[-1]))\n\n\ndf['earliest_cr_year']\n\n0         1990\n1         2004\n2         2007\n3         2006\n4         1999\n          ... \n396025    2004\n396026    2006\n396027    1997\n396028    1990\n396029    1998\nName: earliest_cr_year, Length: 395219, dtype: int64\n\n\nDropping categorical features not worth encoding:\n\ncolumns_to_drop = ['earliest_cr_line',\n                   'grade',\n                   'issue_d',\n                   'loan_status',\n                   'address']\n\n\ndf = df.drop(columns_to_drop,axis=1)"
  },
  {
    "objectID": "posts/k2/index.html#traintest-split",
    "href": "posts/k2/index.html#traintest-split",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "4.1. TRAIN/TEST SPLIT",
    "text": "4.1. TRAIN/TEST SPLIT\nSplitting the dataset into a training set and a test set:\n(Note that cross-validation is performed inside the Optuna study, so we don’t need to separate out a validation set here)\n\nX = df.drop('loan_repaid', axis=1).values\ny = df['loan_repaid'].values\n\nfeatures = df.columns.drop('loan_repaid')\ntarget = 'loan_repaid'\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=594)\n\n# Convert to DataFrame and add column names\nX_test_df = pd.DataFrame(X_test, columns=features)\ny_test_df = pd.DataFrame(y_test, columns=[target])\n\ndf_test = pd.concat([X_test_df, y_test_df], axis=1)\n\n\nprint(\"Training set shape: \", X_train.shape, y_train.shape)\nprint(\"Test set shape: \", X_test.shape, y_test.shape)\n\nTraining set shape:  (375458, 78) (375458,)\nTest set shape:  (19761, 78) (19761,)"
  },
  {
    "objectID": "posts/k2/index.html#normalising-the-data",
    "href": "posts/k2/index.html#normalising-the-data",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "4.2. Normalising the Data",
    "text": "4.2. Normalising the Data\nSome machine learning algorithms require all columns of the dataframe to consist of data on comparable scales. We use StandardScalar from scikit-learn to force this to be true.\nThere is rarely a drawback to normalising data like this. If in doubt, normalise.\n\nscaler = StandardScaler()\n\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nX_train\n\narray([[ 0.82301704, -0.55823002, -1.28362299, ..., -0.03076372,\n        -0.02818382, -2.75955691],\n       [-1.45078579, -0.55823002, -0.46740923, ..., -0.03076372,\n        -0.02818382,  0.01934793],\n       [-1.25930765, -0.55823002,  1.14265627, ..., -0.03076372,\n        -0.02818382,  0.5751289 ],\n       ...,\n       [ 0.70334321, -0.55823002, -0.36901634, ..., -0.03076372,\n        -0.02818382,  0.29723841],\n       [ 1.90008154, -0.55823002, -0.33994571, ..., -0.03076372,\n        -0.02818382, -1.78694022],\n       [-0.52929727, -0.55823002, -1.17404909, ..., -0.03076372,\n        -0.02818382, -0.25854255]])"
  },
  {
    "objectID": "posts/k2/index.html#producing-some-visualisations-investigating-the-distribution-of-misclassified-points",
    "href": "posts/k2/index.html#producing-some-visualisations-investigating-the-distribution-of-misclassified-points",
    "title": "Model Selection and Hyperparameter Tuning using Optuna for Loan Charge-Off Prediction",
    "section": "7.1. Producing some visualisations investigating the distribution of misclassified points",
    "text": "7.1. Producing some visualisations investigating the distribution of misclassified points\n\nmisclassified = df.iloc[np.concatenate((fp_indices[0], fn_indices[0]))].copy()\nmisclassified.loc[:, 'type'] = ['FP' if i in fp_indices[0] else 'FN' for i in misclassified.index]\n\n\n# Boxplot of loan_amnt\nplt.figure(figsize=(10,6))\nsns.boxplot(x='type', y='loan_amnt', data=misclassified)\nplt.title('Loan Amount Spread for Misclassified Points')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Boxplot of int_rate\nplt.figure(figsize=(10,6))\nsns.boxplot(x='type', y='int_rate', data=misclassified)\nplt.title('Interest Rate Spread for Misclassified Points')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Boxplot of annual_inc\nplt.figure(figsize=(10,6))\nsns.boxplot(x='type', y='annual_inc', data=misclassified)\nplt.title('Annual Income Spread for Misclassified Points')\nplt.ylim(0,200000)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Boxplot of revol_bal\nplt.figure(figsize=(10,6))\nsns.boxplot(x='type', y='revol_bal', data=misclassified)\nplt.title('Revolving Balance Spread for Misclassified Points')\nplt.ylim(0,60000)\nplt.show()"
  },
  {
    "objectID": "posts/yt1/index.html",
    "href": "posts/yt1/index.html",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\ncolor_pal = sns.color_palette('magma')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit\n\nimport xgboost as xgb\nprint(\"Python version:\")\n!python --version\n\nPython version:\nPython 3.11.4\nTable of contents\n1. Data\n2. Periodic Trends\n3. Outlier Analysis and Removal\n4. Feature Creation\n5. Lag Features\n6. Visualising Feature/Target Relationships\n7. Train/Test Split\n8. Aside - Time Series Cross Validation\n8.1. Training using Cross-Validation\n9. Predicting on Test Set\n9.1. Fitting an XGBRegressor\n9.2. Feature Importances\n9.3. Predictions\n10. Forecasting the Future"
  },
  {
    "objectID": "posts/yt1/index.html#training-using-cross-validation",
    "href": "posts/yt1/index.html#training-using-cross-validation",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "8.1. Training using Cross-Validation",
    "text": "8.1. Training using Cross-Validation\nThe following code trains an XGBRegressor on each of the above 5 folds, saving the score (RMSE) in a list scores.\n\nfold = 0\npreds = []\nscores = []\nfor train_idx, val_idx in tss.split(df):\n    train = df.iloc[train_idx]\n    test = df.iloc[val_idx]\n\n    train = create_features(train)\n    test = create_features(test)\n\n    FEATURES = ['dayofyear', 'hour', 'dayofweek', 'quarter', 'month','year', 'lag1', 'lag2', 'lag3']\n    TARGET = 'PJME_MW'\n\n    X_train = train[FEATURES]\n    y_train = train[TARGET]\n\n    X_test = test[FEATURES]\n    y_test = test[TARGET]\n\n    reg = xgb.XGBRegressor(base_score=0.5, \n                           booster='gbtree',    \n                           n_estimators=1000,\n                           early_stopping_rounds=50,\n                           objective='reg:squarederror',\n                           max_depth=3,\n                           learning_rate=0.01)\n    \n    reg.fit(X_train, \n            y_train,\n            eval_set=[(X_train, y_train), (X_test, y_test)],\n            verbose=100)\n\n    y_pred = reg.predict(X_test)\n    preds.append(y_pred)\n    score = np.sqrt(mean_squared_error(y_test, y_pred))\n    scores.append(score)\n\n[0] validation_0-rmse:32732.50147   validation_1-rmse:31956.66494\n[100]   validation_0-rmse:12532.10915   validation_1-rmse:11906.70125\n[200]   validation_0-rmse:5739.78666    validation_1-rmse:5352.86754\n[300]   validation_0-rmse:3868.29390    validation_1-rmse:3891.32148\n[400]   validation_0-rmse:3428.85875    validation_1-rmse:3753.95996\n[456]   validation_0-rmse:3349.18480    validation_1-rmse:3761.64093\n[0] validation_0-rmse:32672.16154   validation_1-rmse:32138.88680\n[100]   validation_0-rmse:12513.25338   validation_1-rmse:12222.97626\n[200]   validation_0-rmse:5755.14393    validation_1-rmse:5649.54800\n[300]   validation_0-rmse:3909.18294    validation_1-rmse:3930.98277\n[400]   validation_0-rmse:3477.91771    validation_1-rmse:3603.77859\n[500]   validation_0-rmse:3356.63775    validation_1-rmse:3534.18452\n[600]   validation_0-rmse:3299.24378    validation_1-rmse:3495.69013\n[700]   validation_0-rmse:3258.86466    validation_1-rmse:3470.24780\n[800]   validation_0-rmse:3222.68998    validation_1-rmse:3446.36557\n[900]   validation_0-rmse:3195.04645    validation_1-rmse:3438.00845\n[999]   validation_0-rmse:3169.68251    validation_1-rmse:3434.35289\n[0] validation_0-rmse:32631.19070   validation_1-rmse:31073.24659\n[100]   validation_0-rmse:12498.56469   validation_1-rmse:11133.47932\n[200]   validation_0-rmse:5749.48268    validation_1-rmse:4812.56835\n[300]   validation_0-rmse:3915.69493    validation_1-rmse:3552.97165\n[400]   validation_0-rmse:3493.17887    validation_1-rmse:3492.55244\n[415]   validation_0-rmse:3467.76622    validation_1-rmse:3500.17489\n[0] validation_0-rmse:32528.44140   validation_1-rmse:31475.37803\n[100]   validation_0-rmse:12461.95683   validation_1-rmse:12016.24890\n[200]   validation_0-rmse:5736.08201    validation_1-rmse:5800.02075\n[300]   validation_0-rmse:3913.36576    validation_1-rmse:4388.02984\n[400]   validation_0-rmse:3495.35688    validation_1-rmse:4177.05330\n[500]   validation_0-rmse:3380.70922    validation_1-rmse:4123.43863\n[600]   validation_0-rmse:3321.42955    validation_1-rmse:4110.84393\n[700]   validation_0-rmse:3280.93068    validation_1-rmse:4096.40531\n[800]   validation_0-rmse:3249.14336    validation_1-rmse:4095.30547\n[809]   validation_0-rmse:3246.14826    validation_1-rmse:4094.38398\n[0] validation_0-rmse:32462.05402   validation_1-rmse:31463.86930\n[100]   validation_0-rmse:12445.22753   validation_1-rmse:11954.79556\n[200]   validation_0-rmse:5750.85887    validation_1-rmse:5616.16472\n[300]   validation_0-rmse:3949.92308    validation_1-rmse:4154.55799\n[400]   validation_0-rmse:3538.33857    validation_1-rmse:3996.70155\n[448]   validation_0-rmse:3471.50174    validation_1-rmse:4005.60241\n\n\n\nscores\n\n[3753.2775219986684,\n 3434.3528874818867,\n 3475.9138463312997,\n 4093.3608331481823,\n 3996.298054855067]\n\n\n\nprint(f'Mean score across folds: {np.mean(scores):0.4f}')\nprint(f'Fold scores:\\n{scores}')\n\nMean score across folds: 3750.6406\nFold scores:\n[3753.2775219986684, 3434.3528874818867, 3475.9138463312997, 4093.3608331481823, 3996.298054855067]"
  },
  {
    "objectID": "posts/yt1/index.html#fitting-an-xgbregressor",
    "href": "posts/yt1/index.html#fitting-an-xgbregressor",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "9.1. Fitting an XGBRegressor",
    "text": "9.1. Fitting an XGBRegressor\n\nmodel = xgb.XGBRegressor(base_score=0.5, \n                         booster='gbtree',    \n                         n_estimators=1000,\n                         early_stopping_rounds=50,\n                         objective='reg:squarederror',\n                         max_depth=3,\n                         learning_rate=0.01)\n\n\nmodel.fit(X_train, \n          y_train, \n          eval_set=[(X_train,y_train),(X_test,y_test)],\n          verbose=100)\n\n[0] validation_0-rmse:32462.05402   validation_1-rmse:31463.86930\n[100]   validation_0-rmse:12445.22753   validation_1-rmse:11954.79556\n[200]   validation_0-rmse:5750.85887    validation_1-rmse:5616.16472\n[300]   validation_0-rmse:3949.92308    validation_1-rmse:4154.55799\n[400]   validation_0-rmse:3538.33857    validation_1-rmse:3996.70155\n[447]   validation_0-rmse:3472.46884    validation_1-rmse:4004.66554\n\n\nXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=50,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=50,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)"
  },
  {
    "objectID": "posts/yt1/index.html#feature-importances",
    "href": "posts/yt1/index.html#feature-importances",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "9.2. Feature Importances",
    "text": "9.2. Feature Importances\n\nfi = pd.DataFrame(data=model.feature_importances_,\n                  index=model.feature_names_in_,\n                  columns=['Importance'])\n\n\nfi.sort_values('Importance').plot(kind='barh',title='Feature Importance',color='blue',legend=False)"
  },
  {
    "objectID": "posts/yt1/index.html#predictions",
    "href": "posts/yt1/index.html#predictions",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "9.3. Predictions",
    "text": "9.3. Predictions\n\ntrain\n\n\n\n\n\n\n\n\nPJME_MW\nhour\ndayofweek\nquarter\nmonth\nyear\ndayofyear\nlag1\nlag2\nlag3\n\n\nDatetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2002-01-01 01:00:00\n30393.0\n1\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 02:00:00\n29265.0\n2\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 03:00:00\n28357.0\n3\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 04:00:00\n27899.0\n4\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 05:00:00\n28057.0\n5\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2017-08-01 20:00:00\n45090.0\n20\n1\n3\n8\n2017\n213\n41056.0\n46225.0\n43934.0\n\n\n2017-08-01 21:00:00\n43843.0\n21\n1\n3\n8\n2017\n213\n40151.0\n44510.0\n42848.0\n\n\n2017-08-01 22:00:00\n41850.0\n22\n1\n3\n8\n2017\n213\n38662.0\n42467.0\n40861.0\n\n\n2017-08-01 23:00:00\n38473.0\n23\n1\n3\n8\n2017\n213\n35583.0\n38646.0\n37361.0\n\n\n2017-08-02 00:00:00\n35126.0\n0\n2\n3\n8\n2017\n214\n32181.0\n34829.0\n33743.0\n\n\n\n\n136567 rows × 10 columns\n\n\n\n\ntest\n\n\n\n\n\n\n\n\nPJME_MW\nhour\ndayofweek\nquarter\nmonth\nyear\ndayofyear\nlag1\nlag2\nlag3\n\n\nDatetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-08-03 01:00:00\n29189.0\n1\n3\n3\n8\n2017\n215\n28809.0\n29952.0\n28465.0\n\n\n2017-08-03 02:00:00\n27584.0\n2\n3\n3\n8\n2017\n215\n27039.0\n27934.0\n26712.0\n\n\n2017-08-03 03:00:00\n26544.0\n3\n3\n3\n8\n2017\n215\n25881.0\n26659.0\n25547.0\n\n\n2017-08-03 04:00:00\n26012.0\n4\n3\n3\n8\n2017\n215\n25300.0\n25846.0\n24825.0\n\n\n2017-08-03 05:00:00\n26187.0\n5\n3\n3\n8\n2017\n215\n25412.0\n25898.0\n24927.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2018-08-02 20:00:00\n44057.0\n20\n3\n3\n8\n2018\n214\n42256.0\n41485.0\n38804.0\n\n\n2018-08-02 21:00:00\n43256.0\n21\n3\n3\n8\n2018\n214\n41210.0\n40249.0\n38748.0\n\n\n2018-08-02 22:00:00\n41552.0\n22\n3\n3\n8\n2018\n214\n39525.0\n38698.0\n37330.0\n\n\n2018-08-02 23:00:00\n38500.0\n23\n3\n3\n8\n2018\n214\n36490.0\n35406.0\n34552.0\n\n\n2018-08-03 00:00:00\n35486.0\n0\n4\n3\n8\n2018\n215\n33539.0\n32094.0\n31695.0\n\n\n\n\n8760 rows × 10 columns\n\n\n\n\ntest['prediction'] = model.predict(X_test)\ndf = df.merge(test[['prediction']],how='left',left_index=True,right_index=True)\n\n\ntest['prediction']\n\nDatetime\n2017-08-03 01:00:00    27884.035156\n2017-08-03 02:00:00    27147.710938\n2017-08-03 03:00:00    26344.050781\n2017-08-03 04:00:00    25737.550781\n2017-08-03 05:00:00    25737.550781\n                           ...     \n2018-08-02 20:00:00    40988.347656\n2018-08-02 21:00:00    40045.542969\n2018-08-02 22:00:00    38405.371094\n2018-08-02 23:00:00    36211.242188\n2018-08-03 00:00:00    30370.074219\nName: prediction, Length: 8760, dtype: float32\n\n\n\ntest['prediction'].describe()\n\ncount     8760.000000\nmean     30520.908203\nstd       5277.272949\nmin      21005.292969\n25%      26730.913086\n50%      30010.917969\n75%      33361.808594\nmax      46170.230469\nName: prediction, dtype: float64\n\n\nWe can visualise the predicted energy consumption for a particular week:\n\nstart_date = '04-01-2018'\nend_date = '04-08-2018'\nfiltered_df = df.loc[(df.index &gt; start_date) & (df.index &lt; end_date)]\n\nplt.figure(figsize=(15, 5))\nax = sns.lineplot(data=filtered_df, x=filtered_df.index, y='PJME_MW', label='Truth')\nsns.scatterplot(data=filtered_df, x=filtered_df.index, y='prediction', label='Prediction', marker='.',color='orange')\nplt.title(f'Predicted vs. Actual Energy Consumption: {start_date} to {end_date}')\n\nText(0.5, 1.0, 'Predicted vs. Actual Energy Consumption: 04-01-2018 to 04-08-2018')"
  },
  {
    "objectID": "posts/bte3/index.html",
    "href": "posts/bte3/index.html",
    "title": "The Boltzmann Equation - 3. Information Theory",
    "section": "",
    "text": "Table of contents\n1. Information Theory: Entropy, Mutual Information & KL Divergence\n2. Differential entropy\n2.1.1. Definition 2.1.1 - Differential Entropy\n2.1.2. Example 2.1.2 - Entropy of a univariate normal distribution\n2.1.3. Theorem 2.1.3 - Translation Invariance\n2.1.4. Theorem 2.1.4\n2.1.5. Corollary 2.1.5\n2.1.6. Definition 2.1.6 - Joint & Conditional Entropy\n2.1.7. Example 2.1.7 - Entropy of a multivariate normal distribution\n2.2. - Relative Entropy & Mutual Information\n2.2.1. Definition 2.2.1 - KL Divergence\n2.2.2. Definition 2.2.2 - Mutual Information\n2.2.3. Theorem 2.2.3 - Information inequality\n2.2.4. Corollary 2.2.4\n2.2.5. Corollary 2.2.5\n3. References"
  }
]