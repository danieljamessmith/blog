[
  {
    "objectID": "posts/Spaceship Titanic/The Spaceship Titanic with LightGBM.html",
    "href": "posts/Spaceship Titanic/The Spaceship Titanic with LightGBM.html",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import randint, uniform\n\n# Visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Model\nfrom lightgbm import LGBMClassifier"
  },
  {
    "objectID": "posts/Spaceship Titanic/The Spaceship Titanic with LightGBM.html#filling-homeplanet-destination-and-vip",
    "href": "posts/Spaceship Titanic/The Spaceship Titanic with LightGBM.html#filling-homeplanet-destination-and-vip",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "Filling HomePlanet, Destination and VIP",
    "text": "Filling HomePlanet, Destination and VIP\n\ndf_train['HomePlanet'].value_counts()\n\nHomePlanet\nEarth     4602\nEuropa    2131\nMars      1759\nName: count, dtype: int64\n\n\n\ndf_test['HomePlanet'].value_counts()\n\nHomePlanet\nEarth     2263\nEuropa    1002\nMars       925\nName: count, dtype: int64\n\n\nThe mode for HomePlanet for both the train and test sets is “Earth”, so we use this to fill the null values\n\ndata['HomePlanet'] = data['HomePlanet'].fillna('Earth')\n\n\ndf_train['Destination'].value_counts()\n\nDestination\nTRAPPIST-1e      5915\n55 Cancri e      1800\nPSO J318.5-22     796\nName: count, dtype: int64\n\n\n\ndf_test['Destination'].value_counts()\n\nDestination\nTRAPPIST-1e      2956\n55 Cancri e       841\nPSO J318.5-22     388\nName: count, dtype: int64\n\n\nThe mode for Destination for both the train and test sets is “TRAPPIST-1e”, so we use this to fill the null values\n\ndata['Destination'] = data['Destination'].fillna('TRAPPIST-1e')\n\n\ndf_train['VIP'].value_counts()\n\nVIP\nFalse    8291\nTrue      199\nName: count, dtype: int64\n\n\n\ndf_test['VIP'].value_counts()\n\nVIP\nFalse    4110\nTrue       74\nName: count, dtype: int64\n\n\n\ndata['VIP'] = data['VIP'].fillna(False)\n\n\ndata.isna().sum()\n\nPassengerId            0\nHomePlanet             0\nCryoSleep              0\nCabin                299\nDestination            0\nAge                  270\nVIP                    0\nRoomService          170\nFoodCourt            180\nShoppingMall         175\nSpa                  177\nVRDeck               177\nName                 294\nTransported         4277\nTotalExpenditure       0\ndtype: int64"
  },
  {
    "objectID": "posts/Spaceship Titanic/The Spaceship Titanic with LightGBM.html#filling-age-and-the-expenditure-features",
    "href": "posts/Spaceship Titanic/The Spaceship Titanic with LightGBM.html#filling-age-and-the-expenditure-features",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "Filling Age and the expenditure features",
    "text": "Filling Age and the expenditure features\nTo fill the remaining null values in Age and Expenses_features we will use the median, to reduce the influence of outliers. This requires seperating data back into constituent train and test sets, to avoid data leakage.\n\ntrain = data[:len(df_train)]\ntest = data[len(df_train):].drop('Transported', axis=1)\n\n\nprint(len(train) == len(df_train))\n\nTrue\n\n\n\ntrain.loc[:, 'Age'] = train['Age'].fillna(train['Age'].median())\ntest.loc[:, 'Age'] = test['Age'].fillna(test['Age'].median())\n\n\ntrain.loc[:,Expenses_features] = train[Expenses_features].fillna(train[Expenses_features].median())\ntest.loc[:,Expenses_features] = test[Expenses_features].fillna(test[Expenses_features].median())\n\n\nprint('Remaining null values in train:\\n')\nprint(train.isna().sum())\nprint('\\nRemaining null values in test:\\n')\nprint(test.isna().sum())\n\nRemaining null values in train:\n\nPassengerId           0\nHomePlanet            0\nCryoSleep             0\nCabin               199\nDestination           0\nAge                   0\nVIP                   0\nRoomService           0\nFoodCourt             0\nShoppingMall          0\nSpa                   0\nVRDeck                0\nName                200\nTransported           0\nTotalExpenditure      0\ndtype: int64\n\nRemaining null values in test:\n\nPassengerId           0\nHomePlanet            0\nCryoSleep             0\nCabin               100\nDestination           0\nAge                   0\nVIP                   0\nRoomService           0\nFoodCourt             0\nShoppingMall          0\nSpa                   0\nVRDeck                0\nName                 94\nTotalExpenditure      0\ndtype: int64\n\n\nRedefine data as the concatenation of train and test\n\ndata = pd.concat([train,test], axis=0)"
  },
  {
    "objectID": "posts/Spaceship Titanic/The Spaceship Titanic with LightGBM.html#new-features---agegroup-cabinside-and-groupsize",
    "href": "posts/Spaceship Titanic/The Spaceship Titanic with LightGBM.html#new-features---agegroup-cabinside-and-groupsize",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "New features - AgeGroup, CabinSide and GroupSize",
    "text": "New features - AgeGroup, CabinSide and GroupSize\nCreate a new feature AgeGroup by binning the Age feature into 8 different categories.\n\ndata['Age'].max()\n\n79.0\n\n\n\ndata['AgeGroup'] = 0\nfor i in range(8):\n    data.loc[(data.Age &gt;= 10*i) & (data.Age &lt; 10*(i + 1)), 'AgeGroup'] = i\n\n\ndata['AgeGroup'].value_counts()\n\nAgeGroup\n2    4460\n3    2538\n1    2235\n4    1570\n0     980\n5     809\n6     312\n7      66\nName: count, dtype: int64\n\n\nCreate a dummy feature Group by extracting the first character from the PassengerId column. Use Group to define a new feature GroupSize indicating how many people are in the passengers group. Drop the feature Group as it has too many values to be useful.\n\ndata['Group'] = data['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\ndata['GroupSize'] = data['Group'].map(lambda x: data['Group'].value_counts()[x])\ndata = data.drop('Group', axis=1)\n\nCreate a new boolean feature Solo, indicating if a passenger is in a group just by themselves\n\ndata['Solo'] = (data['GroupSize'] == 1).astype(int)\n\nWe won’t use Cabin directly, but we engineer a new feature CabinSide by taking the last character of Cabin. “P” for port and “S” for starboard. To implement this we fill Cabin with a placeholder value.\n\ndata['Cabin'] = data['Cabin'].fillna('T/0/P')\n\n\ndata['CabinSide'] = data['Cabin'].apply(lambda x: x.split('/')[-1])"
  },
  {
    "objectID": "posts/Spaceship Titanic/The Spaceship Titanic with LightGBM.html#finishing-preprocessing---dropping-features-and-splitting-into-train-and-test-sets",
    "href": "posts/Spaceship Titanic/The Spaceship Titanic with LightGBM.html#finishing-preprocessing---dropping-features-and-splitting-into-train-and-test-sets",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "Finishing preprocessing - dropping features and splitting into train and test sets",
    "text": "Finishing preprocessing - dropping features and splitting into train and test sets\n\ndata = data.drop(['PassengerId','Cabin','Name'], axis=1)\n\n\ndata.isna().sum()\n\nHomePlanet             0\nCryoSleep              0\nDestination            0\nAge                    0\nVIP                    0\nRoomService            0\nFoodCourt              0\nShoppingMall           0\nSpa                    0\nVRDeck                 0\nTransported         4277\nTotalExpenditure       0\nAgeGroup               0\nGroupSize              0\nSolo                   0\nCabinSide              0\ndtype: int64\n\n\n\ndata\n\n\n\n\n\n\n\n\nHomePlanet\nCryoSleep\nDestination\nAge\nVIP\nRoomService\nFoodCourt\nShoppingMall\nSpa\nVRDeck\nTransported\nTotalExpenditure\nAgeGroup\nGroupSize\nSolo\nCabinSide\n\n\n\n\n0\nEuropa\nFalse\nTRAPPIST-1e\n39.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nFalse\n0.0\n3\n1\n1\nP\n\n\n1\nEarth\nFalse\nTRAPPIST-1e\n24.0\nFalse\n109.0\n9.0\n25.0\n549.0\n44.0\nTrue\n736.0\n2\n1\n1\nS\n\n\n2\nEuropa\nFalse\nTRAPPIST-1e\n58.0\nTrue\n43.0\n3576.0\n0.0\n6715.0\n49.0\nFalse\n10383.0\n5\n2\n0\nS\n\n\n3\nEuropa\nFalse\nTRAPPIST-1e\n33.0\nFalse\n0.0\n1283.0\n371.0\n3329.0\n193.0\nFalse\n5176.0\n3\n2\n0\nS\n\n\n4\nEarth\nFalse\nTRAPPIST-1e\n16.0\nFalse\n303.0\n70.0\n151.0\n565.0\n2.0\nTrue\n1091.0\n1\n1\n1\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4272\nEarth\nTrue\nTRAPPIST-1e\n34.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n0.0\n3\n2\n0\nS\n\n\n4273\nEarth\nFalse\nTRAPPIST-1e\n42.0\nFalse\n0.0\n847.0\n17.0\n10.0\n144.0\nNaN\n1018.0\n4\n1\n1\nP\n\n\n4274\nMars\nTrue\n55 Cancri e\n26.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n0.0\n2\n1\n1\nP\n\n\n4275\nEuropa\nFalse\nTRAPPIST-1e\n26.0\nFalse\n0.0\n2680.0\n0.0\n0.0\n523.0\nNaN\n3203.0\n2\n1\n1\nP\n\n\n4276\nEarth\nTrue\nPSO J318.5-22\n43.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n0.0\n4\n1\n1\nS\n\n\n\n\n12970 rows × 16 columns\n\n\n\n\ntrain = data[:len(df_train)]\ntest = data[len(df_train):].drop('Transported', axis=1)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nHomePlanet\nCryoSleep\nDestination\nAge\nVIP\nRoomService\nFoodCourt\nShoppingMall\nSpa\nVRDeck\nTransported\nTotalExpenditure\nAgeGroup\nGroupSize\nSolo\nCabinSide\n\n\n\n\n0\nEuropa\nFalse\nTRAPPIST-1e\n39.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nFalse\n0.0\n3\n1\n1\nP\n\n\n1\nEarth\nFalse\nTRAPPIST-1e\n24.0\nFalse\n109.0\n9.0\n25.0\n549.0\n44.0\nTrue\n736.0\n2\n1\n1\nS\n\n\n2\nEuropa\nFalse\nTRAPPIST-1e\n58.0\nTrue\n43.0\n3576.0\n0.0\n6715.0\n49.0\nFalse\n10383.0\n5\n2\n0\nS\n\n\n3\nEuropa\nFalse\nTRAPPIST-1e\n33.0\nFalse\n0.0\n1283.0\n371.0\n3329.0\n193.0\nFalse\n5176.0\n3\n2\n0\nS\n\n\n4\nEarth\nFalse\nTRAPPIST-1e\n16.0\nFalse\n303.0\n70.0\n151.0\n565.0\n2.0\nTrue\n1091.0\n1\n1\n1\nS\n\n\n\n\n\n\n\n\ntest.head()\n\n\n\n\n\n\n\n\nHomePlanet\nCryoSleep\nDestination\nAge\nVIP\nRoomService\nFoodCourt\nShoppingMall\nSpa\nVRDeck\nTotalExpenditure\nAgeGroup\nGroupSize\nSolo\nCabinSide\n\n\n\n\n0\nEarth\nTrue\nTRAPPIST-1e\n27.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n1\n1\nS\n\n\n1\nEarth\nFalse\nTRAPPIST-1e\n19.0\nFalse\n0.0\n9.0\n0.0\n2823.0\n0.0\n2832.0\n1\n1\n1\nS\n\n\n2\nEuropa\nTrue\n55 Cancri e\n31.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3\n1\n1\nS\n\n\n3\nEuropa\nFalse\nTRAPPIST-1e\n38.0\nFalse\n0.0\n6652.0\n0.0\n181.0\n585.0\n7418.0\n3\n1\n1\nS\n\n\n4\nEarth\nFalse\nTRAPPIST-1e\n20.0\nFalse\n10.0\n0.0\n635.0\n0.0\n0.0\n645.0\n2\n1\n1\nS\n\n\n\n\n\n\n\nThese are our final dataframes for the train and test set. We have engineered new features TotalExpenditure, AgeGroup, GroupSize, Solo and CabinSide. We have filled all null values, and are now nearly ready to train a model"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Logistic Regression with a Neural Network Mindset\n\n\n\n\n\n\n\nPython\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nDaniel Smith\n\n\n\n\n\n\n  \n\n\n\n\nForecasting Energy Consumption with XGBoost\n\n\n\n\n\n\n\nPython\n\n\nXGBoost\n\n\nTime Series Forecasting\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2023\n\n\nDaniel Smith\n\n\n\n\n\n\n  \n\n\n\n\nThe Spaceship Titanic with LightGBM\n\n\n\n\n\n\n\nPython\n\n\nLightGBM\n\n\n\n\nWe train a LightGBM classifier with hyperparameters tuned using a random search to achieve &gt;80% classification accuracy on the Spaceship Titanic dataset from Kaggle, an impressive score for a simple model.\n\n\n\n\n\n\nNov 23, 2023\n\n\nDaniel Smith\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This portfolio is made with Quarto, an open source techinical publishing system that provides functionality to render Jupyter Notebooks as blog posts. I am using GitHub Pages to host."
  },
  {
    "objectID": "posts/Energy Forecasting with XGBoost/post - predicting energy consumption with XGBoost.html",
    "href": "posts/Energy Forecasting with XGBoost/post - predicting energy consumption with XGBoost.html",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "",
    "text": "Informed by YouTube videos of Rob Mulla we use XGBoost to forecast energy consumption in the eastern US.\nimport numpy as np \nimport pandas as pd\n\n# Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\ncolor_pal = sns.color_palette('magma')\n\n# Sklearn\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit\n\n# Model\nimport xgboost as xgb"
  },
  {
    "objectID": "posts/Energy Forecasting with XGBoost/post - predicting energy consumption with XGBoost.html#training-using-cross-validation",
    "href": "posts/Energy Forecasting with XGBoost/post - predicting energy consumption with XGBoost.html#training-using-cross-validation",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "Training using Cross-Validation",
    "text": "Training using Cross-Validation\nThe following code trains an XGBRegressor on each of the above 5 folds, saving the score (RMSE) in a list scores.\n\nfold = 0\npreds = []\nscores = []\nfor train_idx, val_idx in tss.split(df):\n    train = df.iloc[train_idx]\n    test = df.iloc[val_idx]\n\n    train = create_features(train)\n    test = create_features(test)\n\n    FEATURES = ['dayofyear', 'hour', 'dayofweek', 'quarter', 'month','year', 'lag1', 'lag2', 'lag3']\n    TARGET = 'PJME_MW'\n\n    X_train = train[FEATURES]\n    y_train = train[TARGET]\n\n    X_test = test[FEATURES]\n    y_test = test[TARGET]\n\n    reg = xgb.XGBRegressor(base_score=0.5, \n                           booster='gbtree',    \n                           n_estimators=1000,\n                           early_stopping_rounds=50,\n                           objective='reg:squarederror',\n                           max_depth=3,\n                           learning_rate=0.01)\n    \n    reg.fit(X_train, \n            y_train,\n            eval_set=[(X_train, y_train), (X_test, y_test)],\n            verbose=100)\n\n    y_pred = reg.predict(X_test)\n    preds.append(y_pred)\n    score = np.sqrt(mean_squared_error(y_test, y_pred))\n    scores.append(score)\n\n[0] validation_0-rmse:32732.50147   validation_1-rmse:31956.66494\n[100]   validation_0-rmse:12532.10915   validation_1-rmse:11906.70125\n[200]   validation_0-rmse:5739.78666    validation_1-rmse:5352.86754\n[300]   validation_0-rmse:3868.29390    validation_1-rmse:3891.32148\n[400]   validation_0-rmse:3428.85875    validation_1-rmse:3753.95996\n[456]   validation_0-rmse:3349.18480    validation_1-rmse:3761.64093\n[0] validation_0-rmse:32672.16154   validation_1-rmse:32138.88680\n[100]   validation_0-rmse:12513.25338   validation_1-rmse:12222.97626\n[200]   validation_0-rmse:5755.14393    validation_1-rmse:5649.54800\n[300]   validation_0-rmse:3909.18294    validation_1-rmse:3930.98277\n[400]   validation_0-rmse:3477.91771    validation_1-rmse:3603.77859\n[500]   validation_0-rmse:3356.63775    validation_1-rmse:3534.18452\n[600]   validation_0-rmse:3299.24378    validation_1-rmse:3495.69013\n[700]   validation_0-rmse:3258.86466    validation_1-rmse:3470.24780\n[800]   validation_0-rmse:3222.68998    validation_1-rmse:3446.36557\n[900]   validation_0-rmse:3195.04645    validation_1-rmse:3438.00845\n[999]   validation_0-rmse:3169.68251    validation_1-rmse:3434.35289\n[0] validation_0-rmse:32631.19070   validation_1-rmse:31073.24659\n[100]   validation_0-rmse:12498.56469   validation_1-rmse:11133.47932\n[200]   validation_0-rmse:5749.48268    validation_1-rmse:4812.56835\n[300]   validation_0-rmse:3915.69493    validation_1-rmse:3552.97165\n[400]   validation_0-rmse:3493.17887    validation_1-rmse:3492.55244\n[415]   validation_0-rmse:3467.76622    validation_1-rmse:3500.17489\n[0] validation_0-rmse:32528.44140   validation_1-rmse:31475.37803\n[100]   validation_0-rmse:12461.95683   validation_1-rmse:12016.24890\n[200]   validation_0-rmse:5736.08201    validation_1-rmse:5800.02075\n[300]   validation_0-rmse:3913.36576    validation_1-rmse:4388.02984\n[400]   validation_0-rmse:3495.35688    validation_1-rmse:4177.05330\n[500]   validation_0-rmse:3380.70922    validation_1-rmse:4123.43863\n[600]   validation_0-rmse:3321.42955    validation_1-rmse:4110.84393\n[700]   validation_0-rmse:3280.93068    validation_1-rmse:4096.40531\n[800]   validation_0-rmse:3249.14336    validation_1-rmse:4095.30547\n[809]   validation_0-rmse:3246.14826    validation_1-rmse:4094.38398\n[0] validation_0-rmse:32462.05402   validation_1-rmse:31463.86930\n[100]   validation_0-rmse:12445.22753   validation_1-rmse:11954.79556\n[200]   validation_0-rmse:5750.85887    validation_1-rmse:5616.16472\n[300]   validation_0-rmse:3949.92308    validation_1-rmse:4154.55799\n[400]   validation_0-rmse:3538.33857    validation_1-rmse:3996.70155\n[448]   validation_0-rmse:3471.50174    validation_1-rmse:4005.60241\n\n\n\nscores\n\n[3753.2775219986684,\n 3434.3528874818867,\n 3475.9138463312997,\n 4093.3608331481823,\n 3996.298054855067]\n\n\n\nprint(f'Mean score across folds: {np.mean(scores):0.4f}')\nprint(f'Fold scores:\\n{scores}')\n\nMean score across folds: 3750.6406\nFold scores:\n[3753.2775219986684, 3434.3528874818867, 3475.9138463312997, 4093.3608331481823, 3996.298054855067]"
  },
  {
    "objectID": "posts/Energy Forecasting with XGBoost/post - predicting energy consumption with XGBoost.html#fitting-an-xgbregressor",
    "href": "posts/Energy Forecasting with XGBoost/post - predicting energy consumption with XGBoost.html#fitting-an-xgbregressor",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "Fitting an XGBRegressor",
    "text": "Fitting an XGBRegressor\n\nmodel = xgb.XGBRegressor(base_score=0.5, \n                         booster='gbtree',    \n                         n_estimators=1000,\n                         early_stopping_rounds=50,\n                         objective='reg:squarederror',\n                         max_depth=3,\n                         learning_rate=0.01)\n\n\nmodel.fit(X_train, \n          y_train, \n          eval_set=[(X_train,y_train),(X_test,y_test)],\n          verbose=100)\n\n[0] validation_0-rmse:32462.05402   validation_1-rmse:31463.86930\n[100]   validation_0-rmse:12445.22753   validation_1-rmse:11954.79556\n[200]   validation_0-rmse:5750.85887    validation_1-rmse:5616.16472\n[300]   validation_0-rmse:3949.92308    validation_1-rmse:4154.55799\n[400]   validation_0-rmse:3538.33857    validation_1-rmse:3996.70155\n[447]   validation_0-rmse:3472.46884    validation_1-rmse:4004.66554\n\n\nXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=50,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=50,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)"
  },
  {
    "objectID": "posts/Energy Forecasting with XGBoost/post - predicting energy consumption with XGBoost.html#feature-importances",
    "href": "posts/Energy Forecasting with XGBoost/post - predicting energy consumption with XGBoost.html#feature-importances",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "Feature Importances",
    "text": "Feature Importances\n\nfi = pd.DataFrame(data=model.feature_importances_,\n                  index=model.feature_names_in_,\n                  columns=['Importance'])\n\n\nfi.sort_values('Importance').plot(kind='barh',title='Feature Importance',color='blue',legend=False)\n\n&lt;Axes: title={'center': 'Feature Importance'}&gt;"
  },
  {
    "objectID": "posts/Energy Forecasting with XGBoost/post - predicting energy consumption with XGBoost.html#predictions",
    "href": "posts/Energy Forecasting with XGBoost/post - predicting energy consumption with XGBoost.html#predictions",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "Predictions",
    "text": "Predictions\n\ntrain\n\n\n\n\n\n\n\n\nPJME_MW\nhour\ndayofweek\nquarter\nmonth\nyear\ndayofyear\nlag1\nlag2\nlag3\n\n\nDatetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2002-01-01 01:00:00\n30393.0\n1\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 02:00:00\n29265.0\n2\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 03:00:00\n28357.0\n3\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 04:00:00\n27899.0\n4\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 05:00:00\n28057.0\n5\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2017-08-01 20:00:00\n45090.0\n20\n1\n3\n8\n2017\n213\n41056.0\n46225.0\n43934.0\n\n\n2017-08-01 21:00:00\n43843.0\n21\n1\n3\n8\n2017\n213\n40151.0\n44510.0\n42848.0\n\n\n2017-08-01 22:00:00\n41850.0\n22\n1\n3\n8\n2017\n213\n38662.0\n42467.0\n40861.0\n\n\n2017-08-01 23:00:00\n38473.0\n23\n1\n3\n8\n2017\n213\n35583.0\n38646.0\n37361.0\n\n\n2017-08-02 00:00:00\n35126.0\n0\n2\n3\n8\n2017\n214\n32181.0\n34829.0\n33743.0\n\n\n\n\n136567 rows × 10 columns\n\n\n\n\ntest\n\n\n\n\n\n\n\n\nPJME_MW\nhour\ndayofweek\nquarter\nmonth\nyear\ndayofyear\nlag1\nlag2\nlag3\n\n\nDatetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-08-03 01:00:00\n29189.0\n1\n3\n3\n8\n2017\n215\n28809.0\n29952.0\n28465.0\n\n\n2017-08-03 02:00:00\n27584.0\n2\n3\n3\n8\n2017\n215\n27039.0\n27934.0\n26712.0\n\n\n2017-08-03 03:00:00\n26544.0\n3\n3\n3\n8\n2017\n215\n25881.0\n26659.0\n25547.0\n\n\n2017-08-03 04:00:00\n26012.0\n4\n3\n3\n8\n2017\n215\n25300.0\n25846.0\n24825.0\n\n\n2017-08-03 05:00:00\n26187.0\n5\n3\n3\n8\n2017\n215\n25412.0\n25898.0\n24927.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2018-08-02 20:00:00\n44057.0\n20\n3\n3\n8\n2018\n214\n42256.0\n41485.0\n38804.0\n\n\n2018-08-02 21:00:00\n43256.0\n21\n3\n3\n8\n2018\n214\n41210.0\n40249.0\n38748.0\n\n\n2018-08-02 22:00:00\n41552.0\n22\n3\n3\n8\n2018\n214\n39525.0\n38698.0\n37330.0\n\n\n2018-08-02 23:00:00\n38500.0\n23\n3\n3\n8\n2018\n214\n36490.0\n35406.0\n34552.0\n\n\n2018-08-03 00:00:00\n35486.0\n0\n4\n3\n8\n2018\n215\n33539.0\n32094.0\n31695.0\n\n\n\n\n8760 rows × 10 columns\n\n\n\n\ntest['prediction'] = model.predict(X_test)\ndf = df.merge(test[['prediction']],how='left',left_index=True,right_index=True)\n\n\ntest['prediction']\n\nDatetime\n2017-08-03 01:00:00    27884.035156\n2017-08-03 02:00:00    27147.710938\n2017-08-03 03:00:00    26344.050781\n2017-08-03 04:00:00    25737.550781\n2017-08-03 05:00:00    25737.550781\n                           ...     \n2018-08-02 20:00:00    40988.347656\n2018-08-02 21:00:00    40045.542969\n2018-08-02 22:00:00    38405.371094\n2018-08-02 23:00:00    36211.242188\n2018-08-03 00:00:00    30370.074219\nName: prediction, Length: 8760, dtype: float32\n\n\n\ntest['prediction'].describe()\n\ncount     8760.000000\nmean     30520.908203\nstd       5277.272949\nmin      21005.292969\n25%      26730.913086\n50%      30010.917969\n75%      33361.808594\nmax      46170.230469\nName: prediction, dtype: float64\n\n\nWe can visualise the predicted energy consumption for a particular week:\n\nstart_date = '04-01-2018'\nend_date = '04-08-2018'\nfiltered_df = df.loc[(df.index &gt; start_date) & (df.index &lt; end_date)]\n\nplt.figure(figsize=(15, 5))\nax = sns.lineplot(data=filtered_df, x=filtered_df.index, y='PJME_MW', label='Truth')\nsns.scatterplot(data=filtered_df, x=filtered_df.index, y='prediction', label='Prediction', marker='.',color='orange')\nplt.title(f'Predicted vs. Actual Energy Consumption: {start_date} to {end_date}')\n\nText(0.5, 1.0, 'Predicted vs. Actual Energy Consumption: 04-01-2018 to 04-08-2018')"
  },
  {
    "objectID": "posts/Log Reg with NN Mindset/LogReg with NN Mindset.html",
    "href": "posts/Log Reg with NN Mindset/LogReg with NN Mindset.html",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "",
    "text": "We implement logistic regression from scratch (i.e. using only NumPy) and interpret it as a 0-layer Neural Network. We use the resulting model to identify cats in an image classification problem, and experiment with tuning the learning rate of gradient descent.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nfrom PIL import Image\n\n%matplotlib inline"
  },
  {
    "objectID": "posts/Log Reg with NN Mindset/LogReg with NN Mindset.html#mathematical-expression-of-the-algorithm",
    "href": "posts/Log Reg with NN Mindset/LogReg with NN Mindset.html#mathematical-expression-of-the-algorithm",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "3.1 - Mathematical expression of the algorithm",
    "text": "3.1 - Mathematical expression of the algorithm\nFor one training example \\(\\left(x^{(i)},y^{(i)}\\right)\\) in the training set \\(\\left\\{ \\left(x^{(i)} , y^{(i)} \\right) \\right\\} _{i=1}^{m}\\) and a choice of parameters \\(w\\in\\mathbb{R}^{n}\\), \\(b\\in\\mathbb{R}\\)\n\\[z^{(i)} = w^T x^{(i)} + b\\]\n\\[\\hat{y}^{(i)} = a^{(i)} = \\sigma\\left(z^{(i)}\\right)\\]\nwhere \\(\\sigma: \\mathbb{R} \\to \\left(0,1\\right)\\) is the sigmoid defined above, \\(n\\) is the number of features and \\(m = m_{\\text{train}}\\) is the number of training examples.\nThe prediction \\(\\hat{y}^{(i)}\\in \\left(0,1\\right)\\) is interpreted as the probability that \\(x^{(i)}\\) is in class 1 (i.e. is an image of a cat) given the parameters \\(w\\), \\(b\\)\n\\[\\hat{y}^{(i)} = \\mathbb{P}\\left( y^{(i)}=1 \\,|\\, x^{(i)} \\,; \\,w, b \\right)\\]\nWe can extract a binary prediction in \\(\\{0,1\\} \\simeq \\{\\text{non-cat},\\text{cat}\\}\\) from the prediction \\(\\hat{y}^{(i)}\\) by applying a threshold \\[y^{(i)}_{\\text{pred}} = \\mathbb{1} {\\left\\{a^{(i)} &gt; 0.5\\right\\}} = \\begin{cases}\n      1 & \\text{if}\\ a^{(i)} &gt; 0.5 \\\\\n      0 & \\text{otherwise}\n    \\end{cases}\n\\]\nSuch a threshold can be implemented in code using, for example, numpy.round.\nWe use the binary cross entropy loss function \\(\\mathcal{L}\\), defined as\n\\[\\begin{align*}\n\\mathcal{L}\\left(a^{(i)}, y^{(i)}\\right) &=  - y^{(i)}  \\log\\left(a^{(i)}\\right) - \\left(1-y^{(i)} \\right)  \\log\\left(1-a^{(i)}\\right)\\\\[0.2cm]\n&= \\begin{cases}\n- \\log\\left(a^{(i)}\\right) & \\text{if } y^{(i)} = 1 \\\\\n- \\log\\left(1 - a^{(i)}\\right) & \\text{if } y^{(i)} = 0\n\\end{cases}\n\\end{align*}\\]\nThe cost \\(J\\) is then computed by summing over all training examples:\n\\[\\begin{align*}\nJ &= \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}\\left(a^{(i)}, y^{(i)}\\right)\\\\[0.2cm]\n  &= \\frac{-1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)}  \\log\\left(a^{(i)}\\right) + \\left(1-y^{(i)} \\right)  \\log\\left(1-a^{(i)}\\right) \\right]\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/Log Reg with NN Mindset/LogReg with NN Mindset.html#vectorization",
    "href": "posts/Log Reg with NN Mindset/LogReg with NN Mindset.html#vectorization",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "3.2 - Vectorization",
    "text": "3.2 - Vectorization\nLooping over all the \\(m\\) training examples \\(\\left (x^{(i)},y^{(i)} \\right)\\) in turn to calculate \\(\\hat{y}^{(i)} = a^{(i)} = \\sigma\\left(z^{(i)}\\right) = \\sigma\\left( w^T x^{(i)} + b\\right)\\) and \\(\\mathcal{L}\\left(a^{(i)}, y^{(i)}\\right)\\) is computationally inefficient if \\(m\\) is large \\(\\left(\\text{e.g.}\\,\\, m\\sim10^6\\right)\\) as is common in modern industry applications.\nBy turning to a so called Vectorized implementation we can take advantage of NumPy’s powerful numerical linear algebra capabilities. Define vectors \\(Z = \\left( z^{(1)}, z^{(2)}, \\dots, z^{(m)} \\right) \\in \\mathbb{R}^m\\) and \\(A = \\left( a^{(1)}, a^{(2)}, \\dots, a^{(m)} \\right) \\in \\mathbb{R}^m\\). Define the \\(n\\,\\times\\,m\\) matrix \\(X\\) with \\(i^{\\text{th}}\\) column \\(x^{(i)}\\). That is,\n\\[\\begin{equation}\nX = \\begin{bmatrix}\n    | & | & \\cdots & | \\\\\n    x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n    | & | & \\cdots & |\n\\end{bmatrix}\\in \\mathcal{M}_{n,m} \\left(\\mathbb{R}\\right)\n\\end{equation}\\]\nThen\n\\[\\begin{align*}\nw^T X + \\left(b,b,\\dots,b\\right) &= \\left( w^T x^{(1)}+b, \\,w^T x^{(2)}+b, \\dots ,\\,w^T x^{(n)}+b \\right)\\\\[0.2cm]\n                                 &= \\left( z^{(1)}, z^{(2)}, \\dots, z^{(m)} \\right)\\\\[0.2cm]\n                                 &= Z\n\\end{align*}\\]\nSo if \\(\\mathbf{b} = \\left(b,b,\\dots,b\\right)\\) then \\(Z = w^T X + \\mathbf{b}\\). We can implement this in code as Z = np.dot(w.T,X) + b where we have taken advantage of python broadcasting to add the scalar b to the array np.dot(w.T,X). NumPy then interprets this addition as element-wise. We then have \\(A = \\left( a^{(1)}, a^{(2)}, \\dots, a^{(m)} \\right) = \\sigma (Z)\\) since the sigmoid \\(\\sigma\\) acts on arrays element-wise."
  },
  {
    "objectID": "posts/Log Reg with NN Mindset/LogReg with NN Mindset.html#gradient-descent",
    "href": "posts/Log Reg with NN Mindset/LogReg with NN Mindset.html#gradient-descent",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "3.3 - Gradient Descent",
    "text": "3.3 - Gradient Descent\nThe optimization problem \\[\n\\min_{w,b} J(w, b)\n\\]\nis numerically solved through application of gradient descent. For our purposes, gradient descent comprises of recursively updating \\(b\\) and the components \\(w_j\\) of \\(w\\) according to\n\\[\\begin{align*}\nw_j &\\rightarrow w_j - \\alpha \\frac{\\partial J}{\\partial w_j}\\\\[0.1cm]\nb &\\rightarrow b - \\alpha \\frac{\\partial J}{\\partial b}\n\\end{align*}\\]\nwhere \\(\\alpha &lt;&lt; 1\\) is a fixed hyperparameter called the learning rate. Another free hyperparameter introduced with gradient descent is the number of iterations to repeat this updating process."
  },
  {
    "objectID": "posts/Log Reg with NN Mindset/LogReg with NN Mindset.html#errors-and-learning-curves",
    "href": "posts/Log Reg with NN Mindset/LogReg with NN Mindset.html#errors-and-learning-curves",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "5.1 - Errors and learning curves",
    "text": "5.1 - Errors and learning curves\n\n# Example of a 'cat' that was inaccuractely classified as 'not-cat' (False Negative)\nindex = 10\nplt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", predicted as \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 1, predicted as \"non-cat\" picture.\n\n\n\n\n\n\n# Example of a 'not-cat' that was inaccuractely classified as 'cat' (False Positive)\nindex = 34\nplt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", predicted as \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 0, predicted as \"cat\" picture.\n\n\n\n\n\n\n# Plot learning curve (with costs)\ncosts = np.squeeze(logistic_regression_model['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate = \" + str(logistic_regression_model[\"learning_rate\"]))\nplt.show()"
  }
]