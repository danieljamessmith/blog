[
  {
    "objectID": "posts/4 - Sentiment Analysis with VADER and RoBERTa/index.html",
    "href": "posts/4 - Sentiment Analysis with VADER and RoBERTa/index.html",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\ncolor_pal = sns.color_palette(\"mako\")\n\nfrom wordcloud import WordCloud\nfrom tqdm.notebook import tqdm  #Progress Bar\n\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom scipy.special import softmax\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch._utils\")\nprint(\"Python version:\")\n!python --version\n\nPython version:\nPython 3.11.4\nSentiment analysis is a standard application of natural language processing (NLP) in which a machine learning algorithm is trained to classify text as having either positive, negative or neutral emotional tone.\nAs a subfield of NLP, sentiment analysis is closely related to computational linguistics. Sentiment analysis has also been found to be useful in the implementation of recommender systems, in which an automated understanding of the emotional content of reviews proves to be crucial for accurate and personalized content recommendation.\nIn this post sentiment analysis is performed on a dataset of Amazon reviews using both NLTK’s VADER sentiment analysis tool and the Hugging Face transformer-based model RoBERTa."
  },
  {
    "objectID": "posts/4 - Sentiment Analysis with VADER and RoBERTa/index.html#eda",
    "href": "posts/4 - Sentiment Analysis with VADER and RoBERTa/index.html#eda",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "1.1 - EDA",
    "text": "1.1 - EDA\n\nax = df['Score'].value_counts().sort_index() \\\n    .plot(kind='bar',\n          title='Count of Reviews by Stars',\n          figsize=(10,5),\n          color = color_pal[3])\nax.set_xlabel('Review Stars')\nax.set_ylabel('Number of Reviews')\n\nText(0, 0.5, 'Number of Reviews')\n\n\n\n\n\n\n# Generating a worldcloud of all the text in the 'Text' column of the dataframe\n\ntext = ' '.join(df['Text'])\nwordcloud = WordCloud(width=800, height=400, background_color='white',colormap='mako').generate(text)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\n\n(-0.5, 799.5, 399.5, -0.5)\n\n\n\n\n\n\ndf['ReviewLength'] = df['Text'].apply(len)\n\nplt.figure(figsize=(10, 5))\nplt.hist(df['ReviewLength'], bins=50, color=color_pal[2], edgecolor='black')\nplt.title('Distribution of Review Lengths')\nplt.xlabel('Review Length (Number of Characters)')\nplt.ylabel('Number of Reviews')\nplt.show()\n\n\n\n\n\nvectorizer = CountVectorizer(stop_words='english', max_features=20)\nword_matrix = vectorizer.fit_transform(df['Text'])\nword_frequency = word_matrix.sum(axis=0)\n\nwords = vectorizer.get_feature_names_out()\ncounts = word_frequency.A1\n\nsorted_indices = counts.argsort()[::-1]\nwords = [words[i] for i in sorted_indices]\ncounts = counts[sorted_indices]\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=words, y=counts, palette='mako')\nplt.title('Top 20 Most Common Words in Reviews')\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45, ha='right');"
  },
  {
    "objectID": "posts/4 - Sentiment Analysis with VADER and RoBERTa/index.html#cleaning-text",
    "href": "posts/4 - Sentiment Analysis with VADER and RoBERTa/index.html#cleaning-text",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "1.2 - Cleaning Text",
    "text": "1.2 - Cleaning Text\n“br” is such a common word beacuse it is present in many of the reviews as a html tag: &lt;br&gt;&lt;/br&gt;\nWe can fix this using the BeautifulSoup library.\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\ndef clean_text(text):\n    # Remove HTML tags\n    soup = BeautifulSoup(str(text), 'html.parser')\n    cleaned_text = soup.get_text()\n\n    return cleaned_text\n\n\ndf['Text'] = df['Text'].apply(clean_text)\n\n\n# Generating a worldcloud of all the text in the 'Text' column of the dataframe\n\ntext = ' '.join(df['Text'])\nwordcloud = WordCloud(width=800, height=400, background_color='white',colormap='mako').generate(text)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\n\n(-0.5, 799.5, 399.5, -0.5)\n\n\n\n\n\n\nvectorizer = CountVectorizer(stop_words='english', max_features=20)\nword_matrix = vectorizer.fit_transform(df['Text'])\nword_frequency = word_matrix.sum(axis=0)\n\nwords = vectorizer.get_feature_names_out()\ncounts = word_frequency.A1\n\nsorted_indices = counts.argsort()[::-1]\nwords = [words[i] for i in sorted_indices]\ncounts = counts[sorted_indices]\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=words, y=counts, palette='mako')\nplt.title('Top 20 Most Common Words in Reviews')\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45, ha='right');"
  },
  {
    "objectID": "posts/4 - Sentiment Analysis with VADER and RoBERTa/index.html#basic-nltk",
    "href": "posts/4 - Sentiment Analysis with VADER and RoBERTa/index.html#basic-nltk",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "2.1 - Basic NLTK",
    "text": "2.1 - Basic NLTK\nNLTK stands for Natural Language Toolkit. It is a comprehensive library in Python that provides tools and resources for working text data. NLTK includes various modules and packages for tasks such as tokenization, stemming, tagging, parsing, and sentiment analysis, making it a valuable resource for NLP tasks.\n\nexample = df['Text'][50]\nprint(example)\n\nThis oatmeal is not good. Its mushy, soft, I don't like it. Quaker Oats is the way to go.\n\n\n\ntokens = nltk.word_tokenize(example)\ntokens[:10]\n\n['This', 'oatmeal', 'is', 'not', 'good', '.', 'Its', 'mushy', ',', 'soft']\n\n\n\ntagged = nltk.pos_tag(tokens)\ntagged[:10]\n\n[('This', 'DT'),\n ('oatmeal', 'NN'),\n ('is', 'VBZ'),\n ('not', 'RB'),\n ('good', 'JJ'),\n ('.', '.'),\n ('Its', 'PRP$'),\n ('mushy', 'NN'),\n (',', ','),\n ('soft', 'JJ')]\n\n\n\nentities = nltk.chunk.ne_chunk(tagged)\nentities.pprint()\n\n(S\n  This/DT\n  oatmeal/NN\n  is/VBZ\n  not/RB\n  good/JJ\n  ./.\n  Its/PRP$\n  mushy/NN\n  ,/,\n  soft/JJ\n  ,/,\n  I/PRP\n  do/VBP\n  n't/RB\n  like/VB\n  it/PRP\n  ./.\n  (ORGANIZATION Quaker/NNP Oats/NNPS)\n  is/VBZ\n  the/DT\n  way/NN\n  to/TO\n  go/VB\n  ./.)"
  },
  {
    "objectID": "posts/4 - Sentiment Analysis with VADER and RoBERTa/index.html#sentimentintensityanalyzer",
    "href": "posts/4 - Sentiment Analysis with VADER and RoBERTa/index.html#sentimentintensityanalyzer",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "2.2 - SentimentIntensityAnalyzer",
    "text": "2.2 - SentimentIntensityAnalyzer\nSentimentIntensityAnalyzer is a class in the NLTK library’s VADER module. It is designed for sentiment analysis and provides a pre-trained machine learning model to assess the sentiment of a piece of text by analyzing the intensity of positive, negative, and neutral sentiments.\nThis uses a “bag of words” approach in which: 1. Stop words are removed 2. Each word is scored, and combined to give a total score\n\nsia = SentimentIntensityAnalyzer()\n\n\nsia.polarity_scores('This is a positive sentence.')\n\n{'neg': 0.0, 'neu': 0.29, 'pos': 0.71, 'compound': 0.5994}\n\n\n\nsia.polarity_scores('This is a negative sentence.')\n\n{'neg': 0.529, 'neu': 0.286, 'pos': 0.186, 'compound': -0.5267}\n\n\n\nsia.polarity_scores('I am so happy!')\n\n{'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.6468}\n\n\n\nsia.polarity_scores('I hate this product')\n\n{'neg': 0.649, 'neu': 0.351, 'pos': 0.0, 'compound': -0.5719}\n\n\n\nexample\n\n\"This oatmeal is not good. Its mushy, soft, I don't like it. Quaker Oats is the way to go.\"\n\n\n\nsia.polarity_scores(example)\n\n{'neg': 0.22, 'neu': 0.78, 'pos': 0.0, 'compound': -0.5448}\n\n\n\n# Run the polarity score on the entire dataset\nres = {}\nfor i, row in tqdm(df.iterrows(), total=len(df)):\n    text = row['Text']\n    myid = row['Id']\n    res[myid] = sia.polarity_scores(text)\n\n\n\n\n\n# Convert res to a dataframe\nvaders = pd.DataFrame(res).T\nvaders = vaders.reset_index().rename(columns={'index':'Id'})\nvaders = vaders.merge(df,how='left')\n\n\n# Now we have sentiment score and metadata\nvaders\n\n\n\n\n\n\n\n\nId\nneg\nneu\npos\ncompound\nProductId\nUserId\nProfileName\nHelpfulnessNumerator\nHelpfulnessDenominator\nScore\nTime\nSummary\nText\nReviewLength\n\n\n\n\n0\n1\n0.000\n0.695\n0.305\n0.9441\nB001E4KFG0\nA3SGXH7AUHU8GW\ndelmartian\n1\n1\n5\n1303862400\nGood Quality Dog Food\nI have bought several of the Vitality canned d...\n263\n\n\n1\n2\n0.138\n0.862\n0.000\n-0.5664\nB00813GRG4\nA1D87F6ZCVE5NK\ndll pa\n0\n0\n1\n1346976000\nNot as Advertised\nProduct arrived labeled as Jumbo Salted Peanut...\n190\n\n\n2\n3\n0.091\n0.754\n0.155\n0.8265\nB000LQOCH0\nABXLMWJIXXAIN\nNatalia Corres \"Natalia Corres\"\n1\n1\n4\n1219017600\n\"Delight\" says it all\nThis is a confection that has been around a fe...\n509\n\n\n3\n4\n0.000\n1.000\n0.000\n0.0000\nB000UA0QIQ\nA395BORC6FGVXV\nKarl\n3\n3\n2\n1307923200\nCough Medicine\nIf you are looking for the secret ingredient i...\n219\n\n\n4\n5\n0.000\n0.552\n0.448\n0.9468\nB006K2ZZ7K\nA1UQRSCLF8GW1T\nMichael D. Bigham \"M. Wassir\"\n0\n0\n5\n1350777600\nGreat taffy\nGreat taffy at a great price. There was a wid...\n140\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n996\n0.026\n0.716\n0.257\n0.9788\nB006F2NYI2\nA1D3F6UI1RTXO0\nSwopes\n1\n1\n5\n1331856000\nHot & Flavorful\nBLACK MARKET HOT SAUCE IS WONDERFUL.... My hus...\n477\n\n\n996\n997\n0.000\n0.786\n0.214\n0.9309\nB006F2NYI2\nAF50D40Y85TV3\nMike A.\n1\n1\n5\n1328140800\nGreat Hot Sauce and people who run it!\nMan what can i say, this salsa is the bomb!! i...\n305\n\n\n997\n998\n0.000\n0.673\n0.327\n0.9634\nB006F2NYI2\nA3G313KLWDG3PW\nkefka82\n1\n1\n5\n1324252800\nthis sauce is the shiznit\nthis sauce is so good with just about anything...\n265\n\n\n998\n999\n0.063\n0.874\n0.062\n-0.0129\nB006F2NYI2\nA3NIDDT7E7JIFW\nV. B. Brookshaw\n1\n2\n1\n1336089600\nNot Hot\nNot hot at all. Like the other low star review...\n280\n\n\n999\n1000\n0.032\n0.928\n0.041\n-0.1027\nB006F2NYI2\nA132DJVI37RB4X\nScottdrum\n2\n5\n2\n1332374400\nNot hot, not habanero\nI have to admit, I was a sucker for the large ...\n563\n\n\n\n\n1000 rows × 15 columns"
  },
  {
    "objectID": "posts/4 - Sentiment Analysis with VADER and RoBERTa/index.html#plotting-vader-results",
    "href": "posts/4 - Sentiment Analysis with VADER and RoBERTa/index.html#plotting-vader-results",
    "title": "Sentiment Analysis with NLTK and Hugging Face Transformers",
    "section": "2.3 - Plotting VADER Results",
    "text": "2.3 - Plotting VADER Results\n\nax = sns.barplot(data=vaders,\n                 x = 'Score',\n                 y = 'compound',\n                 palette = color_pal)\nax.set_title('Compound Score by Amazon Star Review')\n\nText(0.5, 1.0, 'Compound Score by Amazon Star Review')\n\n\n\n\n\nUnsuprisingly there is a positive correlation between review score (stars out of five) and the compound score from VADER.\n\nfig, axs = plt.subplots(1, 3, figsize=(15,5))\nsns.barplot(data=vaders, x = 'Score', y = 'pos', ax=axs[0], palette = color_pal)\naxs[0].set_title('Positive Score')\nsns.barplot(data=vaders, x = 'Score', y = 'neu', ax=axs[1], palette = color_pal)\naxs[1].set_title('Neutral Score')\nsns.barplot(data=vaders, x = 'Score', y = 'neg', ax=axs[2], palette = color_pal)\naxs[2].set_title('Negative Score')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/2 - Energy Forecasting with XGBoost/index.html",
    "href": "posts/2 - Energy Forecasting with XGBoost/index.html",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\ncolor_pal = sns.color_palette('magma')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit\n\nimport xgboost as xgb\nprint(\"Python version:\")\n!python --version\n\nPython version:\nPython 3.11.4"
  },
  {
    "objectID": "posts/2 - Energy Forecasting with XGBoost/index.html#training-using-cross-validation",
    "href": "posts/2 - Energy Forecasting with XGBoost/index.html#training-using-cross-validation",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "Training using Cross-Validation",
    "text": "Training using Cross-Validation\nThe following code trains an XGBRegressor on each of the above 5 folds, saving the score (RMSE) in a list scores.\n\nfold = 0\npreds = []\nscores = []\nfor train_idx, val_idx in tss.split(df):\n    train = df.iloc[train_idx]\n    test = df.iloc[val_idx]\n\n    train = create_features(train)\n    test = create_features(test)\n\n    FEATURES = ['dayofyear', 'hour', 'dayofweek', 'quarter', 'month','year', 'lag1', 'lag2', 'lag3']\n    TARGET = 'PJME_MW'\n\n    X_train = train[FEATURES]\n    y_train = train[TARGET]\n\n    X_test = test[FEATURES]\n    y_test = test[TARGET]\n\n    reg = xgb.XGBRegressor(base_score=0.5, \n                           booster='gbtree',    \n                           n_estimators=1000,\n                           early_stopping_rounds=50,\n                           objective='reg:squarederror',\n                           max_depth=3,\n                           learning_rate=0.01)\n    \n    reg.fit(X_train, \n            y_train,\n            eval_set=[(X_train, y_train), (X_test, y_test)],\n            verbose=100)\n\n    y_pred = reg.predict(X_test)\n    preds.append(y_pred)\n    score = np.sqrt(mean_squared_error(y_test, y_pred))\n    scores.append(score)\n\n[0] validation_0-rmse:32732.50147   validation_1-rmse:31956.66494\n[100]   validation_0-rmse:12532.10915   validation_1-rmse:11906.70125\n[200]   validation_0-rmse:5739.78666    validation_1-rmse:5352.86754\n[300]   validation_0-rmse:3868.29390    validation_1-rmse:3891.32148\n[400]   validation_0-rmse:3428.85875    validation_1-rmse:3753.95996\n[456]   validation_0-rmse:3349.18480    validation_1-rmse:3761.64093\n[0] validation_0-rmse:32672.16154   validation_1-rmse:32138.88680\n[100]   validation_0-rmse:12513.25338   validation_1-rmse:12222.97626\n[200]   validation_0-rmse:5755.14393    validation_1-rmse:5649.54800\n[300]   validation_0-rmse:3909.18294    validation_1-rmse:3930.98277\n[400]   validation_0-rmse:3477.91771    validation_1-rmse:3603.77859\n[500]   validation_0-rmse:3356.63775    validation_1-rmse:3534.18452\n[600]   validation_0-rmse:3299.24378    validation_1-rmse:3495.69013\n[700]   validation_0-rmse:3258.86466    validation_1-rmse:3470.24780\n[800]   validation_0-rmse:3222.68998    validation_1-rmse:3446.36557\n[900]   validation_0-rmse:3195.04645    validation_1-rmse:3438.00845\n[999]   validation_0-rmse:3169.68251    validation_1-rmse:3434.35289\n[0] validation_0-rmse:32631.19070   validation_1-rmse:31073.24659\n[100]   validation_0-rmse:12498.56469   validation_1-rmse:11133.47932\n[200]   validation_0-rmse:5749.48268    validation_1-rmse:4812.56835\n[300]   validation_0-rmse:3915.69493    validation_1-rmse:3552.97165\n[400]   validation_0-rmse:3493.17887    validation_1-rmse:3492.55244\n[415]   validation_0-rmse:3467.76622    validation_1-rmse:3500.17489\n[0] validation_0-rmse:32528.44140   validation_1-rmse:31475.37803\n[100]   validation_0-rmse:12461.95683   validation_1-rmse:12016.24890\n[200]   validation_0-rmse:5736.08201    validation_1-rmse:5800.02075\n[300]   validation_0-rmse:3913.36576    validation_1-rmse:4388.02984\n[400]   validation_0-rmse:3495.35688    validation_1-rmse:4177.05330\n[500]   validation_0-rmse:3380.70922    validation_1-rmse:4123.43863\n[600]   validation_0-rmse:3321.42955    validation_1-rmse:4110.84393\n[700]   validation_0-rmse:3280.93068    validation_1-rmse:4096.40531\n[800]   validation_0-rmse:3249.14336    validation_1-rmse:4095.30547\n[809]   validation_0-rmse:3246.14826    validation_1-rmse:4094.38398\n[0] validation_0-rmse:32462.05402   validation_1-rmse:31463.86930\n[100]   validation_0-rmse:12445.22753   validation_1-rmse:11954.79556\n[200]   validation_0-rmse:5750.85887    validation_1-rmse:5616.16472\n[300]   validation_0-rmse:3949.92308    validation_1-rmse:4154.55799\n[400]   validation_0-rmse:3538.33857    validation_1-rmse:3996.70155\n[448]   validation_0-rmse:3471.50174    validation_1-rmse:4005.60241\n\n\n\nscores\n\n[3753.2775219986684,\n 3434.3528874818867,\n 3475.9138463312997,\n 4093.3608331481823,\n 3996.298054855067]\n\n\n\nprint(f'Mean score across folds: {np.mean(scores):0.4f}')\nprint(f'Fold scores:\\n{scores}')\n\nMean score across folds: 3750.6406\nFold scores:\n[3753.2775219986684, 3434.3528874818867, 3475.9138463312997, 4093.3608331481823, 3996.298054855067]"
  },
  {
    "objectID": "posts/2 - Energy Forecasting with XGBoost/index.html#fitting-an-xgbregressor",
    "href": "posts/2 - Energy Forecasting with XGBoost/index.html#fitting-an-xgbregressor",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "Fitting an XGBRegressor",
    "text": "Fitting an XGBRegressor\n\nmodel = xgb.XGBRegressor(base_score=0.5, \n                         booster='gbtree',    \n                         n_estimators=1000,\n                         early_stopping_rounds=50,\n                         objective='reg:squarederror',\n                         max_depth=3,\n                         learning_rate=0.01)\n\n\nmodel.fit(X_train, \n          y_train, \n          eval_set=[(X_train,y_train),(X_test,y_test)],\n          verbose=100)\n\n[0] validation_0-rmse:32462.05402   validation_1-rmse:31463.86930\n[100]   validation_0-rmse:12445.22753   validation_1-rmse:11954.79556\n[200]   validation_0-rmse:5750.85887    validation_1-rmse:5616.16472\n[300]   validation_0-rmse:3949.92308    validation_1-rmse:4154.55799\n[400]   validation_0-rmse:3538.33857    validation_1-rmse:3996.70155\n[447]   validation_0-rmse:3472.46884    validation_1-rmse:4004.66554\n\n\nXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=50,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=50,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=None, ...)"
  },
  {
    "objectID": "posts/2 - Energy Forecasting with XGBoost/index.html#feature-importances",
    "href": "posts/2 - Energy Forecasting with XGBoost/index.html#feature-importances",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "Feature Importances",
    "text": "Feature Importances\n\nfi = pd.DataFrame(data=model.feature_importances_,\n                  index=model.feature_names_in_,\n                  columns=['Importance'])\n\n\nfi.sort_values('Importance').plot(kind='barh',title='Feature Importance',color='blue',legend=False)\n\n&lt;Axes: title={'center': 'Feature Importance'}&gt;"
  },
  {
    "objectID": "posts/2 - Energy Forecasting with XGBoost/index.html#predictions",
    "href": "posts/2 - Energy Forecasting with XGBoost/index.html#predictions",
    "title": "Forecasting Energy Consumption with XGBoost",
    "section": "Predictions",
    "text": "Predictions\n\ntrain\n\n\n\n\n\n\n\n\nPJME_MW\nhour\ndayofweek\nquarter\nmonth\nyear\ndayofyear\nlag1\nlag2\nlag3\n\n\nDatetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2002-01-01 01:00:00\n30393.0\n1\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 02:00:00\n29265.0\n2\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 03:00:00\n28357.0\n3\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 04:00:00\n27899.0\n4\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n2002-01-01 05:00:00\n28057.0\n5\n1\n1\n1\n2002\n1\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2017-08-01 20:00:00\n45090.0\n20\n1\n3\n8\n2017\n213\n41056.0\n46225.0\n43934.0\n\n\n2017-08-01 21:00:00\n43843.0\n21\n1\n3\n8\n2017\n213\n40151.0\n44510.0\n42848.0\n\n\n2017-08-01 22:00:00\n41850.0\n22\n1\n3\n8\n2017\n213\n38662.0\n42467.0\n40861.0\n\n\n2017-08-01 23:00:00\n38473.0\n23\n1\n3\n8\n2017\n213\n35583.0\n38646.0\n37361.0\n\n\n2017-08-02 00:00:00\n35126.0\n0\n2\n3\n8\n2017\n214\n32181.0\n34829.0\n33743.0\n\n\n\n\n136567 rows × 10 columns\n\n\n\n\ntest\n\n\n\n\n\n\n\n\nPJME_MW\nhour\ndayofweek\nquarter\nmonth\nyear\ndayofyear\nlag1\nlag2\nlag3\n\n\nDatetime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-08-03 01:00:00\n29189.0\n1\n3\n3\n8\n2017\n215\n28809.0\n29952.0\n28465.0\n\n\n2017-08-03 02:00:00\n27584.0\n2\n3\n3\n8\n2017\n215\n27039.0\n27934.0\n26712.0\n\n\n2017-08-03 03:00:00\n26544.0\n3\n3\n3\n8\n2017\n215\n25881.0\n26659.0\n25547.0\n\n\n2017-08-03 04:00:00\n26012.0\n4\n3\n3\n8\n2017\n215\n25300.0\n25846.0\n24825.0\n\n\n2017-08-03 05:00:00\n26187.0\n5\n3\n3\n8\n2017\n215\n25412.0\n25898.0\n24927.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2018-08-02 20:00:00\n44057.0\n20\n3\n3\n8\n2018\n214\n42256.0\n41485.0\n38804.0\n\n\n2018-08-02 21:00:00\n43256.0\n21\n3\n3\n8\n2018\n214\n41210.0\n40249.0\n38748.0\n\n\n2018-08-02 22:00:00\n41552.0\n22\n3\n3\n8\n2018\n214\n39525.0\n38698.0\n37330.0\n\n\n2018-08-02 23:00:00\n38500.0\n23\n3\n3\n8\n2018\n214\n36490.0\n35406.0\n34552.0\n\n\n2018-08-03 00:00:00\n35486.0\n0\n4\n3\n8\n2018\n215\n33539.0\n32094.0\n31695.0\n\n\n\n\n8760 rows × 10 columns\n\n\n\n\ntest['prediction'] = model.predict(X_test)\ndf = df.merge(test[['prediction']],how='left',left_index=True,right_index=True)\n\n\ntest['prediction']\n\nDatetime\n2017-08-03 01:00:00    27884.035156\n2017-08-03 02:00:00    27147.710938\n2017-08-03 03:00:00    26344.050781\n2017-08-03 04:00:00    25737.550781\n2017-08-03 05:00:00    25737.550781\n                           ...     \n2018-08-02 20:00:00    40988.347656\n2018-08-02 21:00:00    40045.542969\n2018-08-02 22:00:00    38405.371094\n2018-08-02 23:00:00    36211.242188\n2018-08-03 00:00:00    30370.074219\nName: prediction, Length: 8760, dtype: float32\n\n\n\ntest['prediction'].describe()\n\ncount     8760.000000\nmean     30520.908203\nstd       5277.272949\nmin      21005.292969\n25%      26730.913086\n50%      30010.917969\n75%      33361.808594\nmax      46170.230469\nName: prediction, dtype: float64\n\n\nWe can visualise the predicted energy consumption for a particular week:\n\nstart_date = '04-01-2018'\nend_date = '04-08-2018'\nfiltered_df = df.loc[(df.index &gt; start_date) & (df.index &lt; end_date)]\n\nplt.figure(figsize=(15, 5))\nax = sns.lineplot(data=filtered_df, x=filtered_df.index, y='PJME_MW', label='Truth')\nsns.scatterplot(data=filtered_df, x=filtered_df.index, y='prediction', label='Prediction', marker='.',color='orange')\nplt.title(f'Predicted vs. Actual Energy Consumption: {start_date} to {end_date}')\n\nText(0.5, 1.0, 'Predicted vs. Actual Energy Consumption: 04-01-2018 to 04-08-2018')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Sentiment Analysis with NLTK and Hugging Face Transformers\n\n\n\n\n\n\n\nPython\n\n\nNatural Language Processing\n\n\nNLTK\n\n\nHugging Face\n\n\nSentiment Analysis\n\n\n\n\nSentiment analysis is performed on a dataset of Amazon reviews using NLTK’s VADER and a RoBERTa-base model from Hugging Face.\n\n\n\n\n\n\nJan 23, 2024\n\n\nDaniel Smith\n\n\n\n\n\n\n  \n\n\n\n\nLogistic Regression with a Neural Network Mindset\n\n\n\n\n\n\n\nPython\n\n\nMachine Learning\n\n\nNumPy\n\n\n\n\nLogistic regression is implemented in NumPy and interpreted as a perceptron with sigmoid activation. The resulting model is used to detect cats in an image classification problem. Overfitting to the training data is counteracted by including a regularization term in the cost function. The regularization parameter is tuned to improve accuracy on the validation data.\n\n\n\n\n\n\nJan 16, 2024\n\n\nDaniel Smith\n\n\n\n\n\n\n  \n\n\n\n\nForecasting Energy Consumption with XGBoost\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nXGBoost\n\n\nTime Series Forecasting\n\n\n\n\nInformed by YouTube videos of Rob Mulla we use XGBoost to forecast energy consumption in the eastern US.\n\n\n\n\n\n\nDec 22, 2023\n\n\nDaniel Smith\n\n\n\n\n\n\n  \n\n\n\n\nThe Spaceship Titanic with LightGBM\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nLightGBM\n\n\n\n\nA LightGBM classifier is trained with hyperparameters tuned using a random search to achieve &gt;80% classification accuracy on the Spaceship Titanic dataset.\n\n\n\n\n\n\nNov 23, 2023\n\n\nDaniel Smith\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This portfolio is made with Quarto, an open source techinical publishing system that provides functionality to render Jupyter Notebooks as blog posts. I am using GitHub Pages to host."
  },
  {
    "objectID": "posts/1 - Spaceship Titanic with LightGBM/index.html",
    "href": "posts/1 - Spaceship Titanic with LightGBM/index.html",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import randint, uniform\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nfrom lightgbm import LGBMClassifier\nimport sys\nprint(\"Python version:\")\nprint(sys.version)\n\nPython version:\n3.11.4 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 13:38:37) [MSC v.1916 64 bit (AMD64)]"
  },
  {
    "objectID": "posts/1 - Spaceship Titanic with LightGBM/index.html#filling-homeplanet-destination-and-vip",
    "href": "posts/1 - Spaceship Titanic with LightGBM/index.html#filling-homeplanet-destination-and-vip",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "Filling HomePlanet, Destination and VIP",
    "text": "Filling HomePlanet, Destination and VIP\n\ndf_train['HomePlanet'].value_counts()\n\nHomePlanet\nEarth     4602\nEuropa    2131\nMars      1759\nName: count, dtype: int64\n\n\n\ndf_test['HomePlanet'].value_counts()\n\nHomePlanet\nEarth     2263\nEuropa    1002\nMars       925\nName: count, dtype: int64\n\n\nThe mode for HomePlanet for both the train and test sets is “Earth”, so we use this to fill the null values\n\ndata['HomePlanet'] = data['HomePlanet'].fillna('Earth')\n\n\ndf_train['Destination'].value_counts()\n\nDestination\nTRAPPIST-1e      5915\n55 Cancri e      1800\nPSO J318.5-22     796\nName: count, dtype: int64\n\n\n\ndf_test['Destination'].value_counts()\n\nDestination\nTRAPPIST-1e      2956\n55 Cancri e       841\nPSO J318.5-22     388\nName: count, dtype: int64\n\n\nThe mode for Destination for both the train and test sets is “TRAPPIST-1e”, so we use this to fill the null values\n\ndata['Destination'] = data['Destination'].fillna('TRAPPIST-1e')\n\n\ndf_train['VIP'].value_counts()\n\nVIP\nFalse    8291\nTrue      199\nName: count, dtype: int64\n\n\n\ndf_test['VIP'].value_counts()\n\nVIP\nFalse    4110\nTrue       74\nName: count, dtype: int64\n\n\n\ndata['VIP'] = data['VIP'].fillna(False)\n\n\ndata.isna().sum()\n\nPassengerId            0\nHomePlanet             0\nCryoSleep              0\nCabin                299\nDestination            0\nAge                  270\nVIP                    0\nRoomService          170\nFoodCourt            180\nShoppingMall         175\nSpa                  177\nVRDeck               177\nName                 294\nTransported         4277\nTotalExpenditure       0\ndtype: int64"
  },
  {
    "objectID": "posts/1 - Spaceship Titanic with LightGBM/index.html#filling-age-and-the-expenditure-features",
    "href": "posts/1 - Spaceship Titanic with LightGBM/index.html#filling-age-and-the-expenditure-features",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "Filling Age and the expenditure features",
    "text": "Filling Age and the expenditure features\nTo fill the remaining null values in Age and Expenses_features we will use the median, to reduce the influence of outliers. This requires seperating data back into constituent train and test sets, to avoid data leakage.\n\ntrain = data[:len(df_train)]\ntest = data[len(df_train):].drop('Transported', axis=1)\n\n\nprint(len(train) == len(df_train))\n\nTrue\n\n\n\ntrain.loc[:, 'Age'] = train['Age'].fillna(train['Age'].median())\ntest.loc[:, 'Age'] = test['Age'].fillna(test['Age'].median())\n\n\ntrain.loc[:,Expenses_features] = train[Expenses_features].fillna(train[Expenses_features].median())\ntest.loc[:,Expenses_features] = test[Expenses_features].fillna(test[Expenses_features].median())\n\n\nprint('Remaining null values in train:\\n')\nprint(train.isna().sum())\nprint('\\nRemaining null values in test:\\n')\nprint(test.isna().sum())\n\nRemaining null values in train:\n\nPassengerId           0\nHomePlanet            0\nCryoSleep             0\nCabin               199\nDestination           0\nAge                   0\nVIP                   0\nRoomService           0\nFoodCourt             0\nShoppingMall          0\nSpa                   0\nVRDeck                0\nName                200\nTransported           0\nTotalExpenditure      0\ndtype: int64\n\nRemaining null values in test:\n\nPassengerId           0\nHomePlanet            0\nCryoSleep             0\nCabin               100\nDestination           0\nAge                   0\nVIP                   0\nRoomService           0\nFoodCourt             0\nShoppingMall          0\nSpa                   0\nVRDeck                0\nName                 94\nTotalExpenditure      0\ndtype: int64\n\n\nRedefine data as the concatenation of train and test\n\ndata = pd.concat([train,test], axis=0)"
  },
  {
    "objectID": "posts/1 - Spaceship Titanic with LightGBM/index.html#new-features---agegroup-cabinside-and-groupsize",
    "href": "posts/1 - Spaceship Titanic with LightGBM/index.html#new-features---agegroup-cabinside-and-groupsize",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "New features - AgeGroup, CabinSide and GroupSize",
    "text": "New features - AgeGroup, CabinSide and GroupSize\nCreate a new feature AgeGroup by binning the Age feature into 8 different categories.\n\ndata['Age'].max()\n\n79.0\n\n\n\ndata['AgeGroup'] = 0\nfor i in range(8):\n    data.loc[(data.Age &gt;= 10*i) & (data.Age &lt; 10*(i + 1)), 'AgeGroup'] = i\n\n\ndata['AgeGroup'].value_counts()\n\nAgeGroup\n2    4460\n3    2538\n1    2235\n4    1570\n0     980\n5     809\n6     312\n7      66\nName: count, dtype: int64\n\n\nCreate a dummy feature Group by extracting the first character from the PassengerId column. Use Group to define a new feature GroupSize indicating how many people are in the passengers group. Drop the feature Group as it has too many values to be useful.\n\ndata['Group'] = data['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\ndata['GroupSize'] = data['Group'].map(lambda x: data['Group'].value_counts()[x])\ndata = data.drop('Group', axis=1)\n\nCreate a new boolean feature Solo, indicating if a passenger is in a group just by themselves\n\ndata['Solo'] = (data['GroupSize'] == 1).astype(int)\n\nWe won’t use Cabin directly, but we engineer a new feature CabinSide by taking the last character of Cabin. “P” for port and “S” for starboard. To implement this we fill Cabin with a placeholder value.\n\ndata['Cabin'] = data['Cabin'].fillna('T/0/P')\n\n\ndata['CabinSide'] = data['Cabin'].apply(lambda x: x.split('/')[-1])"
  },
  {
    "objectID": "posts/1 - Spaceship Titanic with LightGBM/index.html#finishing-preprocessing---dropping-features-and-splitting-into-train-and-test-sets",
    "href": "posts/1 - Spaceship Titanic with LightGBM/index.html#finishing-preprocessing---dropping-features-and-splitting-into-train-and-test-sets",
    "title": "The Spaceship Titanic with LightGBM",
    "section": "Finishing preprocessing - dropping features and splitting into train and test sets",
    "text": "Finishing preprocessing - dropping features and splitting into train and test sets\n\ndata = data.drop(['PassengerId','Cabin','Name'], axis=1)\n\n\ndata.isna().sum()\n\nHomePlanet             0\nCryoSleep              0\nDestination            0\nAge                    0\nVIP                    0\nRoomService            0\nFoodCourt              0\nShoppingMall           0\nSpa                    0\nVRDeck                 0\nTransported         4277\nTotalExpenditure       0\nAgeGroup               0\nGroupSize              0\nSolo                   0\nCabinSide              0\ndtype: int64\n\n\n\ndata\n\n\n\n\n\n\n\n\nHomePlanet\nCryoSleep\nDestination\nAge\nVIP\nRoomService\nFoodCourt\nShoppingMall\nSpa\nVRDeck\nTransported\nTotalExpenditure\nAgeGroup\nGroupSize\nSolo\nCabinSide\n\n\n\n\n0\nEuropa\nFalse\nTRAPPIST-1e\n39.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nFalse\n0.0\n3\n1\n1\nP\n\n\n1\nEarth\nFalse\nTRAPPIST-1e\n24.0\nFalse\n109.0\n9.0\n25.0\n549.0\n44.0\nTrue\n736.0\n2\n1\n1\nS\n\n\n2\nEuropa\nFalse\nTRAPPIST-1e\n58.0\nTrue\n43.0\n3576.0\n0.0\n6715.0\n49.0\nFalse\n10383.0\n5\n2\n0\nS\n\n\n3\nEuropa\nFalse\nTRAPPIST-1e\n33.0\nFalse\n0.0\n1283.0\n371.0\n3329.0\n193.0\nFalse\n5176.0\n3\n2\n0\nS\n\n\n4\nEarth\nFalse\nTRAPPIST-1e\n16.0\nFalse\n303.0\n70.0\n151.0\n565.0\n2.0\nTrue\n1091.0\n1\n1\n1\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4272\nEarth\nTrue\nTRAPPIST-1e\n34.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n0.0\n3\n2\n0\nS\n\n\n4273\nEarth\nFalse\nTRAPPIST-1e\n42.0\nFalse\n0.0\n847.0\n17.0\n10.0\n144.0\nNaN\n1018.0\n4\n1\n1\nP\n\n\n4274\nMars\nTrue\n55 Cancri e\n26.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n0.0\n2\n1\n1\nP\n\n\n4275\nEuropa\nFalse\nTRAPPIST-1e\n26.0\nFalse\n0.0\n2680.0\n0.0\n0.0\n523.0\nNaN\n3203.0\n2\n1\n1\nP\n\n\n4276\nEarth\nTrue\nPSO J318.5-22\n43.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n0.0\n4\n1\n1\nS\n\n\n\n\n12970 rows × 16 columns\n\n\n\n\ntrain = data[:len(df_train)]\ntest = data[len(df_train):].drop('Transported', axis=1)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nHomePlanet\nCryoSleep\nDestination\nAge\nVIP\nRoomService\nFoodCourt\nShoppingMall\nSpa\nVRDeck\nTransported\nTotalExpenditure\nAgeGroup\nGroupSize\nSolo\nCabinSide\n\n\n\n\n0\nEuropa\nFalse\nTRAPPIST-1e\n39.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\nFalse\n0.0\n3\n1\n1\nP\n\n\n1\nEarth\nFalse\nTRAPPIST-1e\n24.0\nFalse\n109.0\n9.0\n25.0\n549.0\n44.0\nTrue\n736.0\n2\n1\n1\nS\n\n\n2\nEuropa\nFalse\nTRAPPIST-1e\n58.0\nTrue\n43.0\n3576.0\n0.0\n6715.0\n49.0\nFalse\n10383.0\n5\n2\n0\nS\n\n\n3\nEuropa\nFalse\nTRAPPIST-1e\n33.0\nFalse\n0.0\n1283.0\n371.0\n3329.0\n193.0\nFalse\n5176.0\n3\n2\n0\nS\n\n\n4\nEarth\nFalse\nTRAPPIST-1e\n16.0\nFalse\n303.0\n70.0\n151.0\n565.0\n2.0\nTrue\n1091.0\n1\n1\n1\nS\n\n\n\n\n\n\n\n\ntest.head()\n\n\n\n\n\n\n\n\nHomePlanet\nCryoSleep\nDestination\nAge\nVIP\nRoomService\nFoodCourt\nShoppingMall\nSpa\nVRDeck\nTotalExpenditure\nAgeGroup\nGroupSize\nSolo\nCabinSide\n\n\n\n\n0\nEarth\nTrue\nTRAPPIST-1e\n27.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n1\n1\nS\n\n\n1\nEarth\nFalse\nTRAPPIST-1e\n19.0\nFalse\n0.0\n9.0\n0.0\n2823.0\n0.0\n2832.0\n1\n1\n1\nS\n\n\n2\nEuropa\nTrue\n55 Cancri e\n31.0\nFalse\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3\n1\n1\nS\n\n\n3\nEuropa\nFalse\nTRAPPIST-1e\n38.0\nFalse\n0.0\n6652.0\n0.0\n181.0\n585.0\n7418.0\n3\n1\n1\nS\n\n\n4\nEarth\nFalse\nTRAPPIST-1e\n20.0\nFalse\n10.0\n0.0\n635.0\n0.0\n0.0\n645.0\n2\n1\n1\nS\n\n\n\n\n\n\n\nThese are our final dataframes for the train and test set. We have engineered new features TotalExpenditure, AgeGroup, GroupSize, Solo and CabinSide. We have filled all null values, and are now nearly ready to train a model"
  },
  {
    "objectID": "posts/3 - Log Reg with NN Mindset/index.html",
    "href": "posts/3 - Log Reg with NN Mindset/index.html",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nfrom PIL import Image\n\n%matplotlib inline\nprint(\"Python version:\")\n!python --version\n\nPython version:\nPython 3.11.4\nThe data and basic architecture of this post is taken from an exercise in the first course of Andrew Ng’s Deep Learning Specialization:\nhttps://www.coursera.org/specializations/deep-learning"
  },
  {
    "objectID": "posts/3 - Log Reg with NN Mindset/index.html#mathematical-expression-of-the-algorithm",
    "href": "posts/3 - Log Reg with NN Mindset/index.html#mathematical-expression-of-the-algorithm",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "Mathematical expression of the algorithm",
    "text": "Mathematical expression of the algorithm\nFor one training example \\(\\left(x^{(i)},y^{(i)}\\right)\\in \\mathbb{R}^n \\times \\{0,1\\}\\) in the training set \\(\\left\\{ \\left(x^{(i)} , y^{(i)} \\right) \\right\\} _{i=1}^{m}\\) and a choice of parameters \\(w\\in\\mathbb{R}^{n}\\,\\), \\(b\\in\\mathbb{R}\\), forward propagation consists of computing the prediction \\(\\hat y^{(i)}\\in(0,1)\\) of the model as \\(x \\mapsto \\left(w\\cdot x + b\\right) \\mapsto \\sigma\\left( w\\cdot x + b \\right)\\). That is,\n\\[z^{(i)} = w \\cdot x^{(i)} + b\\]\n\\[\\hat{y}^{(i)} = a^{(i)} = \\sigma\\left(z^{(i)}\\right)\\]\nwhere \\(\\sigma: \\mathbb{R} \\to \\left(0,1\\right)\\) is the sigmoid defined above, \\(n\\) is the number of features (length of any of the vectors \\(x\\)) and \\(m = m_{\\text{train}}\\) is the number of training examples. The scalar \\(a^{(i)}\\) is the activation of the perceptron, which is numerically equal to the prediction \\(\\hat y^{(i)}\\) for a neural network with no hidden layers such as logistic regression.\nThe prediction \\(\\hat{y}^{(i)}\\in \\left(0,1\\right)\\) is interpreted as the probability that \\(x^{(i)}\\) is in class 1 (i.e. is an image of a cat)\n\\[\\hat{y}^{(i)} = \\mathbb{P}\\left( y^{(i)}=1 \\,|\\, x^{(i)} \\,; \\,w, b \\right)\\]\nWe can extract a binary prediction in \\(\\{0,1\\} \\equiv \\{\\text{non-cat},\\text{cat}\\}\\) from the prediction \\(\\hat{y}^{(i)}\\) by applying a threshold \\[y^{(i)}_{\\text{pred}} = \\mathbb{1} {\\left\\{a^{(i)} &gt; 0.5\\right\\}} = \\begin{cases}\n      1 & \\text{if}\\ a^{(i)} &gt; 0.5 \\\\\n      0 & \\text{otherwise}\n    \\end{cases}\n\\]\nSuch a threshold can be implemented in code using, for example, numpy.round.\nTraining the model consists of using the training data to find parameters \\(w,\\, b\\) solving the optimization problem \\[\n\\text{minimize}_{w\\in\\mathbb{R}^n,\\,b\\in\\mathbb{R}} J(w, b)\n\\] where \\(J = J(w,b): \\mathbb{R}^n \\times \\mathbb{R} \\rightarrow [0,\\infty)\\) is the cost function defined in terms of a loss function \\(\\mathcal{L}\\). The loss function \\(\\mathcal{L}\\) measures the error between the model’s prediction \\(\\hat y^{(i)} = a^{(i)}\\in(0,1)\\) for one of the training examples \\(x^{(i)}\\in\\mathbb{R}^n\\) and the true label \\(y^{(i)}\\in\\{0,1\\}.\\)\nWe use the binary cross entropy (negative log-loss) loss function \\(\\mathcal{L}\\), defined as\n\\[\\begin{align*}\n\\mathcal{L}\\left(a, y\\right) &=  - y  \\log a - \\left(1-y \\right)  \\log\\left(1-a\\right)\\\\[0.2cm]\n&= \\begin{cases}\n- \\log\\left(a\\right) & \\text{if } y = 1 \\\\\n- \\log\\left(1 - a\\right) & \\text{if } y = 0\n\\end{cases}\n\\end{align*}\\]\nThe (unregularized) cost \\(J\\) is then computed by summing over all training examples:\n\\[\\begin{align*}\nJ(w,b) &= \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}\\left(a^{(i)}, y^{(i)}\\right)\\\\[0.2cm]\n  &= -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)}  \\log\\left(a^{(i)}\\right) + \\left(1-y^{(i)} \\right)  \\log\\left(1-a^{(i)}\\right) \\right]\n\\end{align*}\\]\nTo counteract overfitting of the model to the training data one can include a regularization term in the cost function to penalise large weights \\(w\\). For example, with a \\(L_2\\)-regularization term:\n\\[J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}\\left(a^{(i)}, y^{(i)}\\right) + \\frac{\\lambda}{2m}||w||_2^2\\]\nwhere \\(\\lambda \\geq 0\\) is the regularization parameter and \\(||w||_2^2 = w_1^2 + w_2^2 + \\dots + w_n^2 = w \\cdot w\\) denotes the squared Euclidean norm of \\(w\\).\nBackpropagation consists of computing the derivatives\n\\[\\frac{\\partial J (w,b)}{\\partial w_j},\\, \\frac{\\partial J(w,b)}{\\partial b}\\]\nfor use when optimizing \\((w,b)\\) using gradient descent. It is easy to compute that in the unregularized case:\n\\[\\begin{align*}\n\\frac{\\partial J (w,b)}{\\partial w_j} &= \\frac{1}{m}\\sum_{i=1}^m \\left(a^{(i)} - y^{(i)}\\right)x_j^{(i)}\\\\\n\\frac{\\partial J(w,b)}{\\partial b} &= \\frac{1}{m}\\sum_{i=1}^m \\left(a^{(i)} - y^{(i)}\\right)\n\\end{align*}\\]\nWhile in the \\(L_2\\)-regularized case:\n\\[\\begin{align*}\n\\frac{\\partial J (w,b)}{\\partial w_j} &= \\frac{1}{m}\\sum_{i=1}^m \\left(a^{(i)} - y^{(i)}\\right)x_j^{(i)} +\\frac{\\lambda}{m}w_j  \\\\\n\\frac{\\partial J(w,b)}{\\partial b} &= \\frac{1}{m}\\sum_{i=1}^m \\left(a^{(i)} - y^{(i)}\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/3 - Log Reg with NN Mindset/index.html#vectorization",
    "href": "posts/3 - Log Reg with NN Mindset/index.html#vectorization",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "Vectorization",
    "text": "Vectorization\nLooping over all the \\(m\\) training examples \\(\\left (x^{(i)},y^{(i)} \\right)\\) in turn to calculate \\(\\hat{y}^{(i)} = a^{(i)} = \\sigma\\left(z^{(i)}\\right) = \\sigma\\left( w \\cdot x^{(i)} + b\\right)\\) and \\(\\mathcal{L}\\left(a^{(i)}, y^{(i)}\\right)\\) is computationally inefficient if \\(m\\) is large \\(\\left(\\text{e.g.}\\,\\, m\\sim10^6\\right)\\) as is common in modern industry applications.\nBy turning to a so called vectorized implementation we can take advantage of NumPy’s powerful numerical linear algebra capabilities to implement forward propagation more efficiently.\nDefine vectors \\(Z = \\left( z^{(1)}, z^{(2)}, \\dots, z^{(m)} \\right) \\in \\mathbb{R}^m\\) and \\(A = \\left( a^{(1)}, a^{(2)}, \\dots, a^{(m)} \\right) \\in \\mathbb{R}^m\\). Define the \\(n\\,\\times\\,m\\) matrix \\(X\\) with \\(i^{\\text{th}}\\) column \\(x^{(i)}\\). That is,\n\\[\\begin{equation}\nX = \\begin{bmatrix}\n    | & | & \\cdots & | \\\\\n    x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n    | & | & \\cdots & |\n\\end{bmatrix}\\in \\mathcal{M}_{n,m} \\left(\\mathbb{R}\\right)\n\\end{equation}\\]\nThen\n\\[\\begin{align*}\nw^T X + \\left(b,b,\\dots,b\\right) &= \\left( w^T x^{(1)}+b, \\,w^T x^{(2)}+b, \\dots ,\\,w^T x^{(n)}+b \\right)\\\\[0.2cm]\n                                 &= \\left( z^{(1)}, z^{(2)}, \\dots, z^{(m)} \\right)\\\\[0.2cm]\n                                 &= Z\n\\end{align*}\\]\nSo if \\(\\mathbf{b} = \\left(b,b,\\dots,b\\right)\\) then \\(Z = w^T X + \\mathbf{b}\\).\nWe can implement this in code as Z = np.dot(w.T,X) + b where we have taken advantage of python broadcasting to add the scalar b to the array np.dot(w.T,X). NumPy then interprets this addition as element-wise. We then have \\[A = \\left( a^{(1)}, a^{(2)}, \\dots, a^{(m)} \\right) = \\sigma (Z)\\] since the sigmoid \\(\\sigma\\) acts on arrays element-wise.\nA = sigmoid(np.dot(w.T,X) + b) is a computationally efficient implementation of forward propagation across the entire training set at once. In particular, this is more efficient than using a for loop to iterate over each training example in turn."
  },
  {
    "objectID": "posts/3 - Log Reg with NN Mindset/index.html#gradient-descent",
    "href": "posts/3 - Log Reg with NN Mindset/index.html#gradient-descent",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nThe optimization problem \\[\n\\text{minimize}_{w\\in\\mathbb{R}^n,\\,b\\in\\mathbb{R}} J(w, b)\n\\]\nis numerically solved by gradient descent. For our purposes, gradient descent comprises of iteratively and simultaneously updating \\(b\\) and the components \\(w_j\\) of \\(w\\) according to\n\\[\\begin{align*}\nw_j &\\mapsto w_j - \\alpha \\frac{\\partial J(w,b)}{\\partial w_j}\\\\[0.1cm]\nb &\\mapsto b - \\alpha \\frac{\\partial J(w,b)}{\\partial b}\n\\end{align*}\\]\nwhere \\(\\alpha &lt;&lt; 1\\) is a fixed hyperparameter called the learning rate. Another free hyperparameter introduced with gradient descent is the number of iterations to repeat this updating process."
  },
  {
    "objectID": "posts/3 - Log Reg with NN Mindset/index.html#helper-functions",
    "href": "posts/3 - Log Reg with NN Mindset/index.html#helper-functions",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "Helper Functions",
    "text": "Helper Functions\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    Create a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (int)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias) of type float\n    \"\"\"\n    w = np.zeros((dim,1))\n    b = float(0)\n    \n    return w, b\n\n\ndef forward_propagate(w, b, X):\n    \"\"\"\n    Implements forward propogation across the training set X, computing the activation matrix A \n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n\n    Returns:\n    A -- activation of the neuron, numpy array of size (num_px * num_px * 3, number of examples)\n    \"\"\"\n    A = sigmoid(np.dot(w.T,X)+b)\n    return A\n\n\ndef compute_cost(w, b, X, Y):\n    '''\n    Computes the negative log-likelihood cost J(w,b) across the training set    \n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    \n    Returns:\n    cost -- negative log-likelihood cost for logistic regression\n    '''\n    m = X.shape[1]\n    A = forward_propagate(w, b, X)\n    cost = (-1/m) * (np.dot(Y, np.log(A).T) + np.dot((1 - Y), np.log(1 - A).T))\n    cost = np.squeeze(np.array(cost))  \n    return cost\n\n\ndef backward_propagate(w, b, X, Y):\n    '''\n    Calculates the gradient of the cost function J with respect to the parameters w, b\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    \n    Returns:\n    grads -- dictionary containing the gradients of J w.r.t. the weights and bias\n            (dw -- gradient of the loss with respect to w, thus same shape as w)\n            (db -- gradient of the loss with respect to b, thus same shape as b)\n    '''\n    m = X.shape[1]\n    \n    A = forward_propagate(w, b, X)\n    \n    dw = (1/m)*np.dot(X,(A-Y).T)\n    db = (1/m)*np.sum(A-Y)\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads\n\n\ndef optimize(w, b, X, Y, num_iterations=3000, learning_rate=0.005, print_cost=False):\n    \"\"\"\n    Optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        cost = compute_cost(w, b, X, Y)\n        grads = backward_propagate(w, b, X, Y)\n        \n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n \n        if i % 100 == 0:\n            costs.append(cost)\n        \n            # Print the cost every 100 training iterations\n            if print_cost:\n                print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs\n\n\ndef predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) outputted from `optimize`\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1, m))\n    \n    w = w.reshape(X.shape[0], 1)\n\n    A = sigmoid(np.dot(w.T,X)+b) \n\n    for i in range(A.shape[1]):        \n        Y_prediction[0,i] = np.round(A[0,i]) # Applies a threshold of 0.5\n    \n    return Y_prediction"
  },
  {
    "objectID": "posts/3 - Log Reg with NN Mindset/index.html#combining-into-logistic-regression",
    "href": "posts/3 - Log Reg with NN Mindset/index.html#combining-into-logistic-regression",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "Combining into logistic regression",
    "text": "Combining into logistic regression\nCombining the previously defined functions intialize_with_zeros, propagate, optimize and predict into the logistic regression model\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations=3000, learning_rate=0.005, print_cost=False):\n    \"\"\"\n    Combines the helper functions to construct the unregularized logistic regression model\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to True to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    w, b = initialize_with_zeros(X_train.shape[0])\n    \n    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost=False)\n    \n    w = params['w']\n    b = params['b']\n    \n    Y_prediction_test = predict(w,b,X_test)\n    Y_prediction_train = predict(w,b,X_train)\n\n    # Print train/test Errors\n    if print_cost:\n        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d\n\n\nlogistic_regression_model = model(train_set_x, \n                                  train_set_y, \n                                  test_set_x,\n                                  test_set_y,\n                                  num_iterations=3000,\n                                  learning_rate=0.005,\n                                  print_cost=True)\n\ntrain accuracy: 99.52153110047847 %\ntest accuracy: 68.0 %\n\n\nThe model accuractely classified &gt;99% of the images in the training set and 70% of the images in the test set, suggesting that we are experiences overfitting to our training data."
  },
  {
    "objectID": "posts/3 - Log Reg with NN Mindset/index.html#errors-and-learning-curves",
    "href": "posts/3 - Log Reg with NN Mindset/index.html#errors-and-learning-curves",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "Errors and learning curves",
    "text": "Errors and learning curves\n\n# Example of a 'cat' that was inaccuractely classified as 'not-cat' (False Negative)\nindex = 10\nplt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", predicted as \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 1, predicted as \"non-cat\" picture.\n\n\n\n\n\n\n# Example of a 'not-cat' that was inaccuractely classified as 'cat' (False Positive)\nindex = 34\nplt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", predicted as \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 0, predicted as \"cat\" picture.\n\n\n\n\n\n\n# Plot learning curve (with costs)\ncosts = np.squeeze(logistic_regression_model['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate = \" + str(logistic_regression_model[\"learning_rate\"]))\nplt.show()"
  },
  {
    "objectID": "posts/3 - Log Reg with NN Mindset/index.html#animal-testing",
    "href": "posts/3 - Log Reg with NN Mindset/index.html#animal-testing",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "Animal Testing",
    "text": "Animal Testing\n\ndef is_cat(image_str):\n    '''\n    Applies the trained logistic regression model to predict if an inputted image is a cat (y=1) or a non-cat (y=0)\n    \n    Arguments:\n    image_str - a string encoding the file name of the .jpg file, \n                e.g. 'cat.jpg' if cat.jpg is the file name of an image saved in the same directory as this notebook.\n    Returns:\n    None\n    '''\n    # Read the original image\n    original_image = np.array(Image.open(image_str))\n    \n    # Show the original image\n    plt.subplot(1, 2, 1)\n    plt.imshow(original_image)\n    plt.title(\"Original Image\")\n    \n    # Resize the image to 64x64\n    resized_image = np.array(Image.open(image_str).resize((num_px, num_px)))\n    \n    # Show the resized image\n    plt.subplot(1, 2, 2)\n    plt.imshow(resized_image)\n    plt.title(\"Resized Image (64x64)\")\n    \n    # Standardize and flatten the resized image \n    image = resized_image / 255.\n    image = image.reshape((1, num_px * num_px * 3)).T\n    \n    # Predict label using training logistic regression model\n    my_predicted_image = predict(logistic_regression_model[\"w\"], logistic_regression_model[\"b\"], image)\n    \n    # Print the prediction for the resized image\n    print(\"y = \" + str(int(np.squeeze(my_predicted_image))) + \", the model predicts this is a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n\nLet’s test the function is_cat on an image of my own cat, William.\nHe’s middle-aged and overweight.\n\nis_cat('william.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\nUnsuprising. He’s always been a disappointment.\nObserving on some other images in my camera roll:\n\nis_cat('flora.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat('ginger_greek_cat.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\n\nis_cat('william_yawning.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat('cambridge_cat.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat('lexi.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\n\nis_cat('toby.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat('mr_president.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\n\nis_cat('bojack.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat('american_football.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\nOh dear. Time to do some tuning."
  },
  {
    "objectID": "posts/3 - Log Reg with NN Mindset/index.html#regularizing-the-helper-functions",
    "href": "posts/3 - Log Reg with NN Mindset/index.html#regularizing-the-helper-functions",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "Regularizing the helper functions",
    "text": "Regularizing the helper functions\n\ndef compute_cost_regularized(w, b, X, Y, lambda_):\n    '''\n    Computes the L2-regularized negative log-likelihood cost J(w,b) across the training set\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    lambda_ -- regularization hyperparameter\n    \n    Returns:\n    cost -- L2-regularized negative log-likelihood cost for logistic regression\n    '''\n    m = X.shape[1]\n    reg_cost = compute_cost(w, b, X, Y) + (lambda_/m)*(np.linalg.norm(w)**2)\n    \n    return reg_cost\n\n\ndef backward_propagate_regularized(w, b, X, Y, lambda_):\n    '''\n    Calculates the gradient of the L2-regularized cost function J with respect to the parameters w, b\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    lambda_ -- regularization hyperparameter\n    \n    Returns:\n    grads -- dictionary containing the gradients of J w.r.t. the weights and bias\n            (dw -- gradient of the loss with respect to w, thus same shape as w)\n            (db -- gradient of the loss with respect to b, thus same shape as b)\n    '''\n    m = X.shape[1]\n    grads = backward_propagate(w, b, X, Y)\n    grads['dw'] = grads['dw'] + (lambda_/m)*w\n    \n    return grads\n\n\ndef optimize_regularized(w, b, X, Y, num_iterations=3000, learning_rate=0.005, lambda_=0, print_cost=False):\n    \"\"\"\n    Optimizes w and b by running a gradient descent algorithm on the regularized cost function\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    lambda_ -- regularization hyperparameter\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        cost = compute_cost_regularized(w, b, X, Y, lambda_)\n        grads = backward_propagate_regularized(w, b, X, Y, lambda_)\n        \n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n \n        if i % 100 == 0:\n            costs.append(cost)\n        \n            # Print the cost every 100 training iterations\n            if print_cost:\n                print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs"
  },
  {
    "objectID": "posts/3 - Log Reg with NN Mindset/index.html#combining-into-regularized-logistic-regression",
    "href": "posts/3 - Log Reg with NN Mindset/index.html#combining-into-regularized-logistic-regression",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "Combining into regularized logistic regression",
    "text": "Combining into regularized logistic regression\n\ndef regularized_model(X_train, Y_train, X_test, Y_test, num_iterations=3000, learning_rate=0.5, lambda_=0, print_cost=False):\n    \"\"\"\n    Combines the helper functions to construct the regularized model\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    lambda_ -- regularization hyperparameter\n    print_cost -- Set to True to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    w, b = initialize_with_zeros(X_train.shape[0])\n    \n    params, grads, costs = optimize_regularized(w, b, X_train, Y_train, num_iterations, learning_rate, lambda_, print_cost=False)\n    \n    w = params['w']\n    b = params['b']\n    \n    Y_prediction_test = predict(w,b,X_test)\n    Y_prediction_train = predict(w,b,X_train)\n\n    # Print train/test Errors\n    if print_cost:\n        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d\n\n\nlogistic_regression_model_regularized = regularized_model(train_set_x,\n                                                          train_set_y,\n                                                          test_set_x,\n                                                          test_set_y,\n                                                          num_iterations=3000,\n                                                          learning_rate=0.005,\n                                                          lambda_=100,\n                                                          print_cost=True)\n\ntrain accuracy: 89.47368421052632 %\ntest accuracy: 74.0 %"
  },
  {
    "objectID": "posts/3 - Log Reg with NN Mindset/index.html#tuning-the-regularization-parameter",
    "href": "posts/3 - Log Reg with NN Mindset/index.html#tuning-the-regularization-parameter",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "Tuning the regularization parameter",
    "text": "Tuning the regularization parameter\n\ndef tune_lambda(lambdas):\n    '''\n    Trains the regularized model with a choice of different regularization hyperparameters lambda_\n    \n    Arguments:\n    lambdas - a list of regularization hyperparameters lambda_\n    \n    Returns:\n    None\n    '''\n    for lambda_ in lambdas:\n        print(f\"Training a model with regularization parameter lambda = {lambda_}\")\n        regularized_model(train_set_x,\n                          train_set_y,\n                          test_set_x,\n                          test_set_y,\n                          num_iterations=3000,\n                          learning_rate=0.005,\n                          lambda_=lambda_,\n                          print_cost=True)\n        print(\"\\n\") \n\n\nlambdas = [0, 50, 100, 150, 200, 250]\ntune_lambda(lambdas)\n\nTraining a model with regularization parameter lambda = 0\ntrain accuracy: 99.52153110047847 %\ntest accuracy: 68.0 %\n\n\nTraining a model with regularization parameter lambda = 50\ntrain accuracy: 95.69377990430623 %\ntest accuracy: 74.0 %\n\n\nTraining a model with regularization parameter lambda = 100\ntrain accuracy: 89.47368421052632 %\ntest accuracy: 74.0 %\n\n\nTraining a model with regularization parameter lambda = 150\ntrain accuracy: 84.21052631578948 %\ntest accuracy: 80.0 %\n\n\nTraining a model with regularization parameter lambda = 200\ntrain accuracy: 75.59808612440192 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 250\ntrain accuracy: 72.72727272727273 %\ntest accuracy: 82.0 %\n\n\n\n\n\nlambdas = [130, 140, 150, 160, 170]\ntune_lambda(lambdas)\n\nTraining a model with regularization parameter lambda = 130\ntrain accuracy: 87.08133971291866 %\ntest accuracy: 80.0 %\n\n\nTraining a model with regularization parameter lambda = 140\ntrain accuracy: 87.08133971291866 %\ntest accuracy: 80.0 %\n\n\nTraining a model with regularization parameter lambda = 150\ntrain accuracy: 84.21052631578948 %\ntest accuracy: 80.0 %\n\n\nTraining a model with regularization parameter lambda = 160\ntrain accuracy: 81.33971291866028 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 170\ntrain accuracy: 80.38277511961722 %\ntest accuracy: 82.0 %\n\n\n\n\n\nlambdas = [150, 152, 154, 156, 158, 160]\ntune_lambda(lambdas)\n\nTraining a model with regularization parameter lambda = 150\ntrain accuracy: 84.21052631578948 %\ntest accuracy: 80.0 %\n\n\nTraining a model with regularization parameter lambda = 152\ntrain accuracy: 83.73205741626793 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 154\ntrain accuracy: 83.25358851674642 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 156\ntrain accuracy: 83.25358851674642 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 158\ntrain accuracy: 82.29665071770334 %\ntest accuracy: 82.0 %\n\n\nTraining a model with regularization parameter lambda = 160\ntrain accuracy: 81.33971291866028 %\ntest accuracy: 82.0 %"
  },
  {
    "objectID": "posts/3 - Log Reg with NN Mindset/index.html#more-animal-testing",
    "href": "posts/3 - Log Reg with NN Mindset/index.html#more-animal-testing",
    "title": "Logistic Regression with a Neural Network Mindset",
    "section": "More animal testing",
    "text": "More animal testing\n\ntuned_regularized_model = regularized_model(train_set_x,\n                                            train_set_y,\n                                            test_set_x,\n                                            test_set_y,\n                                            num_iterations=3000,\n                                            learning_rate=0.005,\n                                            lambda_=152,\n                                            print_cost=True)\n\ntrain accuracy: 83.73205741626793 %\ntest accuracy: 82.0 %\n\n\n\ndef is_cat_tuned_regularized(image_str):\n    '''\n    Applies the trained regularized & tuned logistic regression model to predict if an inputted image is a cat (y=1) or a non-cat (y=0)\n    \n    Arguments:\n    image_str - a string encoding the file name of the .jpg file, \n                e.g. 'cat.jpg' if cat.jpg is the file name of an image saved in the same directory as this notebook.\n    Returns:\n    None\n    '''\n    # Read the original image\n    original_image = np.array(Image.open(image_str))\n    \n    # Show the original image\n    plt.subplot(1, 2, 1)\n    plt.imshow(original_image)\n    plt.title(\"Original Image\")\n    \n    # Resize the image to 64x64\n    resized_image = np.array(Image.open(image_str).resize((num_px, num_px)))\n    \n    # Show the resized image\n    plt.subplot(1, 2, 2)\n    plt.imshow(resized_image)\n    plt.title(\"Resized Image (64x64)\")\n    \n    # Standardize and flatten the resized image \n    image = resized_image / 255.\n    image = image.reshape((1, num_px * num_px * 3)).T\n    \n    # Predict label using training logistic regression model\n    my_predicted_image = predict(tuned_regularized_model[\"w\"], tuned_regularized_model[\"b\"], image)\n    \n    # Print the prediction for the resized image\n    print(\"y = \" + str(int(np.squeeze(my_predicted_image))) + \", the model predicts this is a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n\n\nis_cat_tuned_regularized('william.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('flora.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('ginger_greek_cat.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('william_yawning.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('cambridge_cat.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('lexi.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('toby.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('mr_president.jpg')\n\ny = 0, the model predicts this is a \"non-cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('bojack.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\n\nis_cat_tuned_regularized('american_football.jpg')\n\ny = 1, the model predicts this is a \"cat\" picture.\n\n\n\n\n\nAn improvement to be sure, but the model is still far from perfect. It seems that we have improved the validation accuracy at the expense of increasing the prevelence of false positives.\nThis might be because the validation set has a lot of cat images and we are overfitting to it, or because the images I am testing on are drawn from a different probability distribution than the images in the validation set.\nWe could investigate further by performing a more rigorous cross-validation process.\nAdding hidden layers to the neural network would result in a more expressive model capable of telling apart cats and anthropomorphic cartoon horses or midwest emo album covers. We will explore these possiblities in later blog posts."
  }
]