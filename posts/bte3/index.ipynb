{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc2223d",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"The Boltzmann Equation - 3. Information Theory\"\n",
    "author: \"Daniel Smith\"\n",
    "date: \"2024-03-23\"\n",
    "categories: [Mathematics, Probability Theory, Information Theory, Boltzmann Equation]\n",
    "title-block-banner: false\n",
    "image: 'preview.png'\n",
    "draft: false\n",
    "description:  \"We introduce key notions from information theory such as differential entropy and the Kullback-Liebler (KL) divergence and prove the information inequality.\"\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e7ec4",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "1. [Information Theory: Entropy, Mutual Information & KL Divergence](#toc1_)    \n",
    "2. [Differential entropy](#toc2_)    \n",
    "2.1.1. [Definition 2.1.1 - Differential Entropy](#toc2_1_1_)    \n",
    "2.1.2. [Example 2.1.2 - Entropy of a univariate normal distribution](#toc2_1_2_)    \n",
    "2.1.3. [Theorem 2.1.3 - Translation Invariance](#toc2_1_3_)    \n",
    "2.1.4. [Theorem 2.1.4](#toc2_1_4_)    \n",
    "2.1.5. [Corollary 2.1.5](#toc2_1_5_)    \n",
    "2.1.6. [Definition 2.1.6 - Joint & Conditional Entropy](#toc2_1_6_)    \n",
    "2.1.7. [Example 2.1.7  - Entropy of a multivariate normal distribution](#toc2_1_7_)    \n",
    "2.2. [- Relative Entropy & Mutual Information](#toc2_2_)    \n",
    "2.2.1. [Definition 2.2.1 - KL Divergence](#toc2_2_1_)    \n",
    "2.2.2. [Definition 2.2.2 - Mutual Information](#toc2_2_2_)    \n",
    "2.2.3. [Theorem 2.2.3 - Information inequality](#toc2_2_3_)    \n",
    "2.2.4. [Corollary 2.2.4](#toc2_2_4_)    \n",
    "2.2.5. [Corollary 2.2.5](#toc2_2_5_)    \n",
    "3. [References](#toc3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=true\n",
    "\tminLevel=1\n",
    "\tmaxLevel=5\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5b1ca8",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Information Theory: Entropy, Mutual Information & KL Divergence](#toc0_)\n",
    "\n",
    "---\n",
    "\n",
    "# 2. <a id='toc2_'></a>[Differential entropy](#toc0_)\n",
    "\n",
    "The physicist's definition of the entropy $S$ of a system with a\n",
    "continuum of possible states with density $f$ is \n",
    "\n",
    "$S = -\\int f\\log f.$ \n",
    "\n",
    "Note that for a probability density $f$ representing particle distribution in phase space the entropy $S$ coincides with Boltzmann's H functional $H(f)$ up to a change of sign:\n",
    "\n",
    "$H(f) = \\int f\\log f.$\n",
    "\n",
    "\n",
    "\n",
    "We can generalise this definition to any probability density $f$ (for\n",
    "which the integral makes sense) and the resulting quantity is called the\n",
    "**differential entropy** of $f$.\n",
    "\n",
    "Intuitively, the entropy measures how\n",
    "localized a probability density is. A density that concentrates most of\n",
    "its mass in a small region will have low entropy, while a density that\n",
    "distributes its mass over a large region will have high entropy. This is\n",
    "simply a more precise statement of the commonly used heuristic that low\n",
    "entropy states are and high entropy states are .\n",
    "\n",
    "Throughout we fix a continuous random variable $X$ with density $f$ and\n",
    "denote the support set of $f$ by $S$.\n",
    "i.e. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "S &= \\text{supp}\\,(f)\\\\\n",
    "  &= \\left\\{ x\\in\\mathbb{R} \\,\\vert\\, f(x)\\neq0 \\right\\}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe31a9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.1.1. <a id='toc2_1_1_'></a>[Definition 2.1.1 - Differential Entropy](#toc0_)\n",
    "\n",
    "The **differential entropy** of the random variable $X$ is denoted by\n",
    "$h(X)$ or $h(f)$ (as it only depends on the density $f$) and is defined\n",
    "by \n",
    "\n",
    "$$h(f) = -\\int_S f(x) \\log f(x)\\,\\text{d} x.$$ \n",
    "\n",
    "If $\\log = \\ln$ then we say\n",
    "the differential entropy is measured in **nats**.\\\n",
    "If $\\log = \\log_2$ then we say the differential entropy is measured in\n",
    "**bits**.\n",
    "\n",
    "Unless otherwise specified, $\\log$ denotes the natural logarithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0093cbb",
   "metadata": {},
   "source": [
    "### 2.1.2. <a id='toc2_1_2_'></a>[Example 2.1.2 - Entropy of a univariate normal distribution](#toc0_)\n",
    "\n",
    "Let $X \\sim\\mathcal{N}(0,\\sigma^2)$. That is, suppose $X$ is a random variable with density\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{x^2}{2\\sigma^2}}.$$\n",
    "\n",
    "Then we compute \n",
    "$$\\begin{aligned}\n",
    "    h(f) &= -\\int f\\log f\\\\\n",
    "    &= -\\int_{\\mathbb{R}}f(x)\\left[-\\frac{x^2}{2\\sigma^2}-\\frac{1}{2}\\log 2\\pi\\sigma^2\\right]\\,\\text{d} x\\\\\n",
    "    &= \\frac{\\mathbb{E}[X^2]}{2\\sigma^2}+ \\frac{1}{2}\\log 2\\pi\\sigma^2\\\\\n",
    "    &= \\frac{1}{2}+\\frac{1}{2}\\log 2\\pi\\sigma^2\\\\\n",
    "    &= \\frac{1}{2}\\log 2\\pi e\\sigma^2\\;\\, \\text{nats}\\\\\n",
    "     &= \\frac{1}{2}\\log_2 2\\pi e\\sigma^2\\;\\, \\text{bits.}\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a1e13",
   "metadata": {},
   "source": [
    "### 2.1.3. <a id='toc2_1_3_'></a>[Theorem 2.1.3 - Translation Invariance](#toc0_)\n",
    "\n",
    "For any $c\\in\\mathbb{R}$ it holds that\n",
    "\n",
    "$$h(X+c) = h(X).$$\n",
    "\n",
    "\n",
    "*Proof.*\n",
    "\n",
    " $$\\begin{aligned}\n",
    "    h(X) &= -\\int f(x)\\log f(x)\\,\\text{d} x\\\\\n",
    "    &= - \\int f(x-c) \\log f(x-c)\\,\\text{d} x\\\\\n",
    "    &= h(X+c)\n",
    "\\end{aligned}$$ \n",
    "◻\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b362b1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.1.4. <a id='toc2_1_4_'></a>[Theorem 2.1.4](#toc0_)\n",
    "\n",
    "For $a\\in\\mathbb{R}$ it holds that\n",
    "\n",
    "$$h(aX) = h(X) + \\log|a|.$$\n",
    "\n",
    "\n",
    "*Proof.*  \n",
    "\n",
    "Define the random variable $Y = aX$ with density\n",
    "\n",
    "$$f_Y(y) = \\frac{1}{|a|}f_X\\left(\\frac{y}{a}\\right).$$\n",
    "\n",
    "Then we can directly compute \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    h(aX) &= -\\int f_Y(y)\\log f_Y(y)\\,\\text{d} y\\\\\n",
    "    &= -\\int \\frac{1}{|a|}f_X\\left(\\frac{y}{a}\\right)\\log\\left[\\frac{1}{|a|}f_X\\left(\\frac{y}{a}\\right)\\right]\\,\\text{d} y\\\\\n",
    "    &= -\\int f_X(x)\\log f_X(x)\\,\\text{d} x +\\log|a|\\\\\n",
    "    &= h(X) +\\log|a|.\n",
    "\\end{aligned}$$ \n",
    "\n",
    "◻\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613c12e",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1.5. <a id='toc2_1_5_'></a>[Corollary 2.1.5](#toc0_)\n",
    "\n",
    "$$h(A\\mathbf{X}) = h(\\mathbf{X})+\\log|\\det A|$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13182282",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1.6. <a id='toc2_1_6_'></a>[Definition 2.1.6 - Joint & Conditional Entropy](#toc0_)\n",
    "\n",
    "The **joint differential entropy** of a set $X_1,\\dots,X_n$ of (jointly\n",
    "continuous) random variables with joint density $f = f(x_1,\\dots,x_n)$\n",
    "is defined as\n",
    "\n",
    "$$h(X_1,\\dots,X_n) = -\\int f(x_1,\\dots,x_n) \\log f(x_1,\\dots,x_n)\\,\\text{d}^{n}x.$$\n",
    "\n",
    "If $X$ and $Y$ have joint density $f_{X,Y}(x,y)$ then we define the\n",
    "**conditional differential entropy** as\n",
    "\n",
    "$$h(X|Y) = -\\int f_{X,Y}(x,y) \\log f_{X|Y}(x|y)\\,\\text{d} x\\,\\text{d} y.$$\n",
    "\n",
    "Since in general $f_{X|Y}(x|y) = f_{X,Y}(x,y)/f_Y(y)$ we immediately\n",
    "obtain the **chain rule**:\n",
    "\n",
    "$$h(X|Y) = h(X,Y) - h(Y),$$\n",
    "\n",
    "which may fail to hold if any of the concerned entropies are infinite or\n",
    "do not exist.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36779882",
   "metadata": {},
   "source": [
    "### 2.1.7. <a id='toc2_1_7_'></a>[Example 2.1.7  - Entropy of a multivariate normal distribution](#toc0_)\n",
    "\n",
    "Let $X_1,\\dots,X_n$ have a multivariate Gaussian distribution with mean\n",
    "$\\mu$ and covariance matrix $K$. Then\n",
    "\n",
    "$$h(X_1,\\dots, X_n) = \\frac{1}{2}\\log\\left[(2\\pi e)^n\\det K\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a63c1",
   "metadata": {},
   "source": [
    "## 2.2. <a id='toc2_2_'></a>[- Relative Entropy & Mutual Information](#toc0_)\n",
    "\n",
    "### 2.2.1. <a id='toc2_2_1_'></a>[Definition 2.2.1 - KL Divergence](#toc0_)\n",
    "\n",
    "The **relative entropy** (also known as the **Kullback-Liebler\n",
    "divergence**) $D(f\\,||\\,g)$ between two densities $f$ and $g$ is defined\n",
    "as\n",
    "\n",
    "$$D(f\\,||\\,g) = \\int f(x)\\log\\frac{f(x)}{g(x)}\\,\\text{d}x.$$\n",
    "\n",
    "---\n",
    "\n",
    "*Note:*\n",
    "$$D(f\\,||\\,g) < \\infty \\Longleftrightarrow \\text{supp}\\,f\\subseteq\\text{supp}\\,g.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dc96a",
   "metadata": {},
   "source": [
    "### 2.2.2. <a id='toc2_2_2_'></a>[Definition 2.2.2 - Mutual Information](#toc0_)\n",
    "\n",
    "Given two random variables $X$ and $Y$ with joint density $f_{X,Y}$\n",
    "define the **mutual information** $I(X;Y)$ between $X$ and $Y$ by\n",
    "\n",
    "$$I(X;Y) = \\int f_{X,Y}(x,y)\\log\\left[\\frac{f_{X,Y}(x,y)}{f_X(x)f_Y(y)}\\right]\\,\\text{d}x\\,\\text{d}y.$$\n",
    "\n",
    "---\n",
    "\n",
    "From the definition it is clear that we have the formulas\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    I(X;Y) &= h(X) - h(X|Y)\\\\\n",
    "    &= h(Y) - h(Y|X)\\\\\n",
    "    &= h(X) + h(Y) - h(X,Y).\n",
    "\\end{aligned}$$\n",
    "\n",
    "Along with\n",
    "\n",
    "$$I(X;Y) = D(f_{X,Y}\\,||\\,f_X\\otimes f_Y).$$\n",
    "\n",
    "Note the special cases \n",
    "\n",
    "$$\\begin{aligned}\n",
    "I(X;Y) &= I(Y;X),\\\\\n",
    "I(X;X) &= h(X).\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea2bad0",
   "metadata": {},
   "source": [
    "### 2.2.3. <a id='toc2_2_3_'></a>[Theorem 2.2.3 - Information inequality](#toc0_)\n",
    "\n",
    "For any pair of densities $f,\\,g$:\n",
    "\n",
    "$$D(f\\,||\\,g) \\geq 0,$$\n",
    "\n",
    "with equality if and only if $f = g$ a.e.\n",
    "\n",
    "\n",
    "*Proof.*  \n",
    "\n",
    "Without loss of generality assume $f/g\\geq1$.\n",
    "Use the fact that $\\int f = \\int g = 1$ to rewrite \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    D(f\\,||\\,g) &= \\int f\\log\\frac{f}{g}\\\\\n",
    "    &= \\int f\\left(\\frac{g}{f}-1-\\log\\frac{g}{f}\\right).\n",
    "\\end{aligned}$$\n",
    "\n",
    "Now note that for $t\\geq1$ we have the inequality:\n",
    "\n",
    "$$t - 1 -\\log t \\geq 0,$$\n",
    "\n",
    "in which equality holds iff $t=1.$ This can be easily established\n",
    "graphically or by means of elementary calculus. Applying this inequality\n",
    "to $f/g$ and integrating yields\n",
    "\n",
    "$$\\int \\frac{g}{f}-1-\\log\\frac{g}{f}\\;\\geq\\; 0$$\n",
    "\n",
    "with equality iff $f = g$ a.e. ◻\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7c740",
   "metadata": {},
   "source": [
    "### 2.2.4. <a id='toc2_2_4_'></a>[Corollary 2.2.4](#toc0_)\n",
    "\n",
    "For any pair of random variables $X,\\,Y$: $$I(X;Y)\\geq 0,$$ with\n",
    "equality if and only if $X$ and $Y$ are independent.\n",
    "\n",
    "\n",
    "*Proof.*  \n",
    "\n",
    "$$I(X;Y) = D(f_{X,Y}\\,||\\,f_X\\otimes f_Y) \\geq 0$$ \n",
    "\n",
    "with equality iff\n",
    "$f_{X,Y} = f_X\\otimes f_Y$ a.e. i.e. iff $X$ and $Y$ are independent. ◻\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effcb538",
   "metadata": {},
   "source": [
    "### 2.2.5. <a id='toc2_2_5_'></a>[Corollary 2.2.5](#toc0_)\n",
    "\n",
    "For any pair of random variables $X,\\,Y$:\n",
    "\n",
    "$$h(X|Y) \\leq h(X),$$\n",
    "\n",
    "with equality if and only if $X$ and $Y$ are independent.\n",
    "\n",
    "\n",
    "*Proof.* \n",
    "\n",
    "$$h(X) - h(X|Y) = I(X;Y)\\,\\geq\\,0,$$ \n",
    "\n",
    "with equality iff $X$ and\n",
    "$Y$ are independent by Corollary 3.2.4. ◻\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[References](#toc0_)\n",
    "\n",
    "- [1] Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
