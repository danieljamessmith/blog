{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"The Boltzmann Equation - 4. Maximum Entropy\"\n",
    "author: \"Daniel J Smith\"\n",
    "date: \"2024-04-04\"\n",
    "categories: [Mathematics, Probability Theory, Information Theory, Boltzmann Equation]\n",
    "title-block-banner: false\n",
    "image: 'preview.png'\n",
    "draft: false\n",
    "description:  \"We prove Cover's theorem from 'Elements of Information Theory' that the distribution function maximising the entropy over functions with given moment contraints takes the form of an exponential function. This result, combined with the H-theorem, provides a rigorous justification for our physical belief that the limiting form of a solution of the Boltzmann equation is a Maxwell-Boltzmann distribution.\"\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum entropy\n",
    "\n",
    "Consider the following problem as posed by Cover[1]:\n",
    "\n",
    "> Maximise the differential entropy $h(f)$ over all densities $f$ satisfying\n",
    "> \n",
    "> 1.  $f(x)\\geq0$ with equality outside of a given set $S$. <br/><br/>\n",
    ">\n",
    ">\n",
    "> 2.  $\\int_Sf(x)\\,\\text{d}x = 1$ <br/><br/>\n",
    ">\n",
    "> \n",
    "> 3.  $\\int_S f(x)r_i(x)\\,\\text{d}x = \\alpha_i$ \n",
    ">\n",
    ">     for given functions $r_i$,\n",
    ">        $\\,i = 1,\\dots,m.$\n",
    "\n",
    "That is, we wish to maximise the entropy over all probability\n",
    "distributions supported on the set $S$ satisfying the $m$ given moment\n",
    "constraints $\\mathbb{E}[r_i(X)] = \\alpha_i, \\,\\,i = 1,\\dots,m.$\n",
    "\n",
    "It is natural to conjecture that the solution to this\n",
    "optimization problem takes the form of an exponential function. Indeed,\n",
    "we prove this in the following theorem by using the [Information Inequality](https://danieljamessmith.github.io/blog/posts/bte3/#theorem-2.2.3---information-inequality):\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theorem 1.1\n",
    "For $x\\in S$ define\n",
    "\n",
    "$$f^*(x) = \\exp{\\left[\\lambda_0 + \\sum_{i=0}^m \\lambda_i r_i(x)\\right]}$$\n",
    "\n",
    "where $\\lambda_0, \\lambda_1,\\dots,\\lambda_m$ are chosen such that \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\int_Sf^*&=1\\\\\n",
    "\\quad\\int_Sf^*r_i&=\\alpha_i.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then $f^*$ uniquely maximises the entropy $h(f)$ over all densities $f$ satisfying conditions 1, 2 and 3 as stated above.\n",
    "\n",
    "> *Proof.*\n",
    "> \n",
    ">\n",
    "> Let $g$ satisfy conditions 1, 2 and 3. Then:\n",
    ">\n",
    "> \\begin{align*}\n",
    "> h(g) &= -\\int g\\log g\\\\\n",
    "> &= -\\int g\\log\\frac{g}{f^*}f^*\\\\\n",
    "> &= -D(g\\,||\\,f^*) -\\int g\\log f^*\\\\\n",
    "> &\\leq -\\int g\\log f^*\\\\\n",
    "> &= -\\int g\\left(\\lambda_0 + \\sum_{i=1}^m\\lambda_ir_i\\right)\\\\\n",
    "> &= -\\int f^*\\left(\\lambda_0 + \\sum_{i=1}^m\\lambda_ir_i\\right)\\\\\n",
    "> &= -\\int f^* \\log f^*\\\\\n",
    "> &= h(f^*)\n",
    "> \\end{align*}\n",
    ">\n",
    "> in which we have equality iff we have equality in the information equality. i.e. iff $g = f^*$ a.e.\n",
    ">\n",
    ">â—»\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark 1.2\n",
    "\n",
    "The maximum entropy can be infinite. For example, consider\n",
    "$S = \\mathbb{R}$ with constraint $\\mathbb{E}[X] = \\mu$ some fixed $\\mu\\in\\mathbb{R}.$ Then Gaussian\n",
    "distributions $X \\sim\\mathcal{N}(\\mu,\\sigma^2)$ \n",
    "satisfy the constraint for any variance $\\sigma^2>0$ . By\n",
    "[Example 2.1.2](https://danieljamessmith.github.io/blog/posts/bte3/#example-2.1.2---entropy-of-a-univariate-normal-distribution) from a previous post we have\n",
    "$$h(X) = \\frac{1}{2}\\log2\\pi e\\sigma^2 \\xrightarrow{\\: \\sigma^2 \\to \\infty \\: }\\infty.$$\n",
    "\n",
    "In words, we can construct probability densities on $\\mathbb{R}$ with arbitrarily\n",
    "large differential entropy satisfying the first moment constraint of having mean $\\mu$ by\n",
    "considering Gaussian distributions $\\mathcal{N}(\\mu,\\sigma^2)$ with fixed mean $\\mu$ and increasing\n",
    "variance $\\sigma^2$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark 1.3\n",
    "\n",
    "Even if the maximum entropy is finite it need not be attained. That is,\n",
    "the constants $\\lambda_i$ introduced in\n",
    "[Theorem 1.1](#toc1_1_) need not exist. \n",
    "\n",
    "For example, consider\n",
    "probability densities $f$ on $S = \\mathbb{R}$ with moment constraints up to\n",
    "third order:\n",
    " $$\\begin{aligned}\n",
    "\\int_{-\\infty}^\\infty f(x) \\,\\text{d} x &= 1,\\\\\n",
    "\\int_{-\\infty}^\\infty x^if(x)\\,\\text{d} x &= \\alpha_i,\\quad i=1,2,3.\n",
    "\\end{aligned}$$ \n",
    "\n",
    "Then by\n",
    "[Theorem 1.1](#toc1_1_) the maximum entropy distribution (if it exists)\n",
    "looks like\n",
    "\n",
    "$$f(x) = \\exp\\left[\\lambda_0 + \\lambda_1x + \\lambda_2x^2 + \\lambda_3x^3\\right].$$\n",
    "\n",
    "However, $f\\in L^1(\\mathbb{R})$ only if $\\lambda_3 = 0.$ Then we have\n",
    "four equations in three unknowns and thus it is in general not possible\n",
    "to determine the $\\lambda_i$. \n",
    "\n",
    "The failure of our technique in this case\n",
    "is simply explained, although $\\sup h(f) <\\infty$ there is not a\n",
    "probability density $f$ satisfying our constraints that achieves this\n",
    "supremum. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark 1.4\n",
    "\n",
    "The Maxwell-Boltzmann distribution $M^f$ associated to a particle distribution function $f$ is\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    M^f(v) = \\frac{\\rho}{(2\\pi T)^{d/2}}\\,\\mathrm{exp}\\left[-\\frac{1}{2T}|v-u|^2\\right],\n",
    "\\end{aligned}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\begin{split}\n",
    "    \\rho &= \\int_{\\mathbb{R}^d}f\\,\\text{d}v,\\\\\n",
    "    u &= \\frac{1}{\\rho}\\int_{\\mathbb{R}^d}vf\\,\\text{d}v,\\\\\n",
    "    T &= \\frac{1}{\\rho d}\\int_{\\mathbb{R}^d}|v-u|^2f\\,\\text{d}v.\n",
    "\\end{split}\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "This is of the form of the maximum entropy distribution $f^*$ given in [Theorem 1.1](#toc1_1_) with respect to the moment contraints equivalent to fixed total energy and fixed particle number.\n",
    "\n",
    "[Boltzmann's $H$-theorem](https://danieljamessmith.github.io/blog/posts/bte1/#theorem-1.4.1-boltzmanns-h-theorem) is an analytical assertion of the fact that the entoropy $h(f)$ of such a distribution $f$ is a quantity increasing with time. \n",
    "\n",
    "Thus [Theorem 1.1](#toc1_1_) combined with the $H$-theorem provide a rigorous mathematical underpinning for our physical intuition that the Maxwell-Boltzmann distribution $M^f$ should be the candidate limit of a particle distribution function $f$ as $t\\rightarrow\\infty$, since this intuition aligns with the second law of thermodynamic's assertion that the entropy of an isolated system will increase until it reaches a maximum at equilibrium.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [1] Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
