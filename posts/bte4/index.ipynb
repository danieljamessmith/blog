{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"The Boltzmann Equation - 4. Maximum Entropy\"\n",
    "author: \"Daniel J Smith\"\n",
    "date: \"2024-04-04\"\n",
    "categories: [Mathematics, Probability Theory, Information Theory, Boltzmann Equation]\n",
    "title-block-banner: false\n",
    "image: 'preview.png'\n",
    "draft: false\n",
    "description:  \"We prove Cover's theorem from 'Elements of Information Theory' that the distribution function maximising the entropy over functions with given moment contraints takes the form of an exponential function. This result, combined with the H-theorem, provides a rigorous justification for our physical belief that the limiting form of a solution of the Boltzmann equation is a Maxwell-Boltzmann distribution.\"\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "1. [Maximum entropy](#toc1_)    \n",
    "1.1. [Theorem 1.1](#toc1_1_)    \n",
    "1.2. [Remark 1.2](#toc1_2_)    \n",
    "1.3. [Remark 1.3](#toc1_3_)    \n",
    "1.4. [Remark 1.4](#toc1_4_)    \n",
    "2. [References:](#toc2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=true\n",
    "\tminLevel=1\n",
    "\tmaxLevel=5\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Maximum entropy](#toc0_)\n",
    "\n",
    "Consider the following problem as posed by Cover[1]:\n",
    "\n",
    "> Maximise the differential entropy $h(f)$ over all densities $f$ satisfying\n",
    "> \n",
    "> 1.  $f(x)\\geq0$ with equality outside of a given set $S$. <br/><br/>\n",
    ">\n",
    ">\n",
    "> 2.  $\\int_Sf(x)\\,\\text{d}x = 1$ <br/><br/>\n",
    ">\n",
    "> \n",
    "> 3.  $\\int_S f(x)r_i(x)\\,\\text{d}x = \\alpha_i$ \n",
    ">\n",
    ">     for given functions $r_i$,\n",
    ">        $\\,i = 1,\\dots,m.$\n",
    "\n",
    "That is, we wish to maximise the entropy over all probability\n",
    "distributions supported on the set $S$ satisfying the $m$ given moment\n",
    "constraints $\\mathbb{E}[r_i(X)] = \\alpha_i, \\,\\,i = 1,\\dots,m.$\n",
    "\n",
    "It is natural to conjecture that the solution to this\n",
    "optimization problem takes the form of an exponential function. Indeed,\n",
    "we prove this in the following theorem by using the [Information Inequality](https://danieljamessmith.github.io/blog/posts/bte3/#theorem-2.2.3---information-inequality):\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. <a id='toc1_1_'></a>[Theorem 1.1](#toc0_)\n",
    "For $x\\in S$ define\n",
    "\n",
    "$$f^*(x) = \\exp{\\left[\\lambda_0 + \\sum_{i=0}^m \\lambda_i r_i(x)\\right]}$$\n",
    "\n",
    "where $\\lambda_0, \\lambda_1,\\dots,\\lambda_m$ are chosen such that \n",
    "\n",
    "$$\\int_Sf^*=1,\\quad\\int_Sf^*r_i=\\alpha_i.$$\n",
    "\n",
    "Then $f^*$ uniquely maximises the entropy $h(f)$ over all densities $f$ satisfying conditions 1, 2 and 3 as stated above.\n",
    "\n",
    "> *Proof.*\n",
    "> \n",
    ">\n",
    "> Let $g$ satisfy conditions 1, 2 and 3. Then:\n",
    ">\n",
    "> \\begin{align*}\n",
    "> h(g) &= -\\int g\\log g\\\\\n",
    "> &= -\\int g\\log\\frac{g}{f^*}f^*\\\\\n",
    "> &= -D(g\\,||\\,f^*) -\\int g\\log f^*\\\\\n",
    "> &\\leq -\\int g\\log f^*\\\\\n",
    "> &= -\\int g\\left(\\lambda_0 + \\sum_{i=1}^m\\lambda_ir_i\\right)\\\\\n",
    "> &= -\\int f^*\\left(\\lambda_0 + \\sum_{i=1}^m\\lambda_ir_i\\right)\\\\\n",
    "> &= -\\int f^* \\log f^*\\\\\n",
    "> &= h(f^*)\n",
    "> \\end{align*}\n",
    ">\n",
    "> in which we have equality iff we have equality in the information equality. i.e. iff $g = f^*$ a.e.\n",
    ">\n",
    ">â—»\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. <a id='toc1_2_'></a>[Remark 1.2](#toc0_)\n",
    "\n",
    "The maximum entropy can be infinite. For example, consider\n",
    "$S = \\mathbb{R}$ with constraint $\\mathbb{E}[X] = \\mu$ some fixed $\\mu\\in\\mathbb{R}.$ Then Gaussian\n",
    "distributions $X \\sim\\mathcal{N}(\\mu,\\sigma^2)$ \n",
    "satisfy the constraint for any variance $\\sigma^2>0$ . By\n",
    "[Example 2.1.2](https://danieljamessmith.github.io/blog/posts/bte3/#example-2.1.2---entropy-of-a-univariate-normal-distribution) from a previous post we have\n",
    "$$h(X) = \\frac{1}{2}\\log2\\pi e\\sigma^2 \\xrightarrow{\\: \\sigma^2 \\to \\infty \\: }\\infty.$$\n",
    "\n",
    "In words, we can construct probability densities on $\\mathbb{R}$ with arbitrarily\n",
    "large differential entropy satisfying a first moment constraint $\\mathbb{E}[X] = \\mu$ by\n",
    "considering Gaussian distributions $X \\sim\\mathcal{N}(\\mu,\\sigma^2)$ with fixed mean $\\mu$ and increasing\n",
    "variance $\\sigma^2$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. <a id='toc1_3_'></a>[Remark 1.3](#toc0_)\n",
    "\n",
    "Even if the maximum entropy is finite it need not be attained. That is,\n",
    "the constants $\\lambda_i$ introduced in\n",
    "[Theorem 1.1](#toc1_1_) need not exist. \n",
    "\n",
    "For example, consider\n",
    "probability densities $f$ on $S = \\mathbb{R}$ with moment constraints up to\n",
    "third order:\n",
    " $$\\begin{aligned}\n",
    "\\int_{-\\infty}^\\infty f(x) \\,\\text{d} x &= 1,\\\\\n",
    "\\int_{-\\infty}^\\infty x^if(x)\\,\\text{d} x &= \\alpha_i,\\quad i=1,2,3.\n",
    "\\end{aligned}$$ \n",
    "\n",
    "Then by\n",
    "Theorem 1 the maximum entropy distribution (if it exists)\n",
    "looks like\n",
    "\n",
    "$$f(x) = \\exp\\left[\\lambda_0 + \\lambda_1x + \\lambda_2x^2 + \\lambda_3x^3\\right].$$\n",
    "\n",
    "However, $f\\in L^1(\\mathbb{R})$ only if $\\lambda_3 = 0.$ Then we have\n",
    "four equations in three unknowns and thus it is in general not possible\n",
    "to determine the $\\lambda_i$. \n",
    "\n",
    "The failure of our technique in this case\n",
    "is simply explained, although $\\sup h(f) <\\infty$ there is not a\n",
    "probability density $f$ satisfying our constraints that achieves this\n",
    "supremum. \n",
    "\n",
    "To see that $\\sup h(f) <\\infty$ note that without the third\n",
    "moment constraint the maximum entropy distribution would be\n",
    "$\\mathcal{N}(0,\\alpha_2-\\alpha_1^2).$ Adding a further moment constraint\n",
    "could not increase the maximum entropy but could cause the supremum to\n",
    "no longer be achievable.\n",
    "\n",
    "We can however get arbitrarily close to the supremum by perturbing\n",
    "$\\mathcal{N}(0,\\alpha_2-\\alpha_1^2)$ at sufficiently large $x$ to force\n",
    "the third moment constraint to hold without violating the first and\n",
    "second moment constraints. $$\\begin{aligned}\n",
    "\\Longrightarrow \\sup h(f) &= h(\\mathcal{N}(0,\\alpha_2-\\alpha_1^2))\\\\\n",
    "&= \\frac{1}{2}\\log\\left[2\\pi e (\\alpha_2-\\alpha_1^2)\\right].\n",
    "\\end{aligned}$$ This illustrates that *the maximum entropy may only be\n",
    "$\\epsilon$-achievable.*\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. <a id='toc1_4_'></a>[Remark 1.4](#toc0_)\n",
    "\n",
    "The Maxwell-Boltzmann distribution $M^f$ associated to a particle distribution function $f$ takes the form of the maximum entropy distribution $f^*$ given in [Theorem 1.1](#toc1_1_) with respect to moment contraints corresponding to fixed total energy and fixed particle number.\n",
    "\n",
    "Boltzmann's $H$-theorem (see a previous post) is an analytical assertion of the fact that the entoropy $h(f)$ of such a distribution $f$ is a quantity increasing with time. \n",
    "\n",
    "Thus [Theorem 1.1](#toc1_1_) combined with the $H$-theorem provide a rigorous mathematical underpinning for our physical intuition that the Maxwell-Boltzmann distribution $M^f$ should be the candidate limit of a particle distribution function $f$ as $t\\rightarrow\\infty$, since this intuition aligns with the second law of thermodynamic's assertion that the entropy of an isolated system will increase until it reaches a maximum at equilibrium.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[References](#toc0_)\n",
    "\n",
    "- [1] Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
