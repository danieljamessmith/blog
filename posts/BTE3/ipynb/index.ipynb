{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d860b9",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"The Boltzmann Equation - 3. Information Theory\"\n",
    "author: \"Daniel Smith\"\n",
    "date: \"2024-03-23\"\n",
    "categories: [Mathematics, Probability Theory, Information Theory, Boltzmann Equation]\n",
    "title-block-banner: false\n",
    "image: 'preview.png'\n",
    "draft: true\n",
    "description:  \"We introduce key notions from information theory such as differential entropy and the Kullback-Liebler (KL) divergence and prove the information inequality.\"\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ac387-8e43-4006-9e47-35fb3c1f52b3",
   "metadata": {},
   "source": [
    "# 3. - Information Theory: Entropy, Mutual Information & KL Divergence\n",
    "\n",
    "---\n",
    "\n",
    "# 3.1 - Differential entropy\n",
    "\n",
    "The physicist's definition of the entropy $S$ of a system with a\n",
    "continuum of possible states with density $f$ is \n",
    "\n",
    "$S = -\\int f\\log f.$ \n",
    "\n",
    "Note that for a probability density $f$ representing particle distribution in phase space the entropy $S$ coincides with Boltzmann's H functional $H(f)$ up to a change of sign:\n",
    "\n",
    "$H(f) = \\int f\\log f.$\n",
    "\n",
    "\n",
    "\n",
    "We can generalise this definition to any probability density $f$ (for\n",
    "which the integral makes sense) and the resulting quantity is called the\n",
    "**differential entropy** of $f$.\n",
    "\n",
    "Intuitively, the entropy measures how\n",
    "localized a probability density is. A density that concentrates most of\n",
    "its mass in a small region will have low entropy, while a density that\n",
    "distributes its mass over a large region will have high entropy. This is\n",
    "simply a more precise statement of the commonly used heuristic that low\n",
    "entropy states are and high entropy states are .\n",
    "\n",
    "Throughout we fix a continuous random variable $X$ with density $f$ and\n",
    "denote the support set of $f$ by $S$.\n",
    "i.e. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "S &= \\text{supp}\\,(f)\\\\\n",
    "  &= \\left\\{ x\\in\\mathbb{R} \\,\\vert\\, f(x)\\neq0 \\right\\}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f599d1-c2ee-4a9f-8674-f6e56a869706",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Definition 3.1.1 (Differential Entropy)\n",
    "\n",
    "The **differential entropy** of the random variable $X$ is denoted by\n",
    "$h(X)$ or $h(f)$ (as it only depends on the density $f$) and is defined\n",
    "by \n",
    "\n",
    "$$h(f) = -\\int_S f(x) \\log f(x)\\,\\text{d} x.$$ \n",
    "\n",
    "If $\\log = \\ln$ then we say\n",
    "the differential entropy is measured in **nats**.\\\n",
    "If $\\log = \\log_2$ then we say the differential entropy is measured in\n",
    "**bits**.\n",
    "\n",
    "Unless otherwise specified, $\\log$ denotes the natural logarithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63cc2cd-70e8-4f15-bc6b-ef59998b1751",
   "metadata": {},
   "source": [
    "### Example 3.1.2 (Entropy of a univariate normal distribution)\n",
    "\n",
    "Let $X \\sim\\mathcal{N}(0,\\sigma^2)$. That is, suppose $X$ is a random variable with density\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{x^2}{2\\sigma^2}}.$$\n",
    "\n",
    "Then we compute \n",
    "$$\\begin{aligned}\n",
    "    h(f) &= -\\int f\\log f\\\\\n",
    "    &= -\\int_{\\mathbb{R}}f(x)\\left[-\\frac{x^2}{2\\sigma^2}-\\frac{1}{2}\\log 2\\pi\\sigma^2\\right]\\,\\text{d} x\\\\\n",
    "    &= \\frac{\\mathbb{E}[X^2]}{2\\sigma^2}+ \\frac{1}{2}\\log 2\\pi\\sigma^2\\\\\n",
    "    &= \\frac{1}{2}+\\frac{1}{2}\\log 2\\pi\\sigma^2\\\\\n",
    "    &= \\frac{1}{2}\\log 2\\pi e\\sigma^2\\;\\, \\text{nats}\\\\\n",
    "     &= \\frac{1}{2}\\log_2 2\\pi e\\sigma^2\\;\\, \\text{bits.}\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c780f-655d-4e14-b3f8-8532a945480c",
   "metadata": {},
   "source": [
    "### Theorem 3.1.3\n",
    "\n",
    "For any $c\\in\\mathbb{R}$ it holds that\n",
    "\n",
    "$$h(X+c) = h(X).$$\n",
    "\n",
    "\n",
    "*Proof.*\n",
    "\n",
    " $$\\begin{aligned}\n",
    "    h(X) &= -\\int f(x)\\log f(x)\\,\\text{d} x\\\\\n",
    "    &= - \\int f(x-c) \\log f(x-c)\\,\\text{d} x\\\\\n",
    "    &= h(X+c)\n",
    "\\end{aligned}$$ \n",
    "◻\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63e0b0-9e38-4ef0-b3c0-a5a0d108965c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Theorem 3.1.4\n",
    "\n",
    "For $a\\in\\mathbb{R}$ it holds that\n",
    "\n",
    "$$h(aX) = h(X) + \\log|a|.$$\n",
    "\n",
    "\n",
    "*Proof.*  \n",
    "\n",
    "Define the random variable $Y = aX$ with density\n",
    "\n",
    "$$f_Y(y) = \\frac{1}{|a|}f_X\\left(\\frac{y}{a}\\right).$$\n",
    "\n",
    "Then we can directly compute \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    h(aX) &= -\\int f_Y(y)\\log f_Y(y)\\,\\text{d} y\\\\\n",
    "    &= -\\int \\frac{1}{|a|}f_X\\left(\\frac{y}{a}\\right)\\log\\left[\\frac{1}{|a|}f_X\\left(\\frac{y}{a}\\right)\\right]\\,\\text{d} y\\\\\n",
    "    &= -\\int f_X(x)\\log f_X(x)\\,\\text{d} x +\\log|a|\\\\\n",
    "    &= h(X) +\\log|a|.\n",
    "\\end{aligned}$$ \n",
    "\n",
    "◻\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4cbd15-170f-4e20-b573-dba6101f23fe",
   "metadata": {},
   "source": [
    "\n",
    "### Corollary 3.1.5\n",
    "\n",
    "$$h(A\\mathbf{X}) = h(\\mathbf{X})+\\log|\\det A|$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d25e9c-735b-4768-8481-13e7efc50586",
   "metadata": {},
   "source": [
    "\n",
    "### Definition 3.1.6 (Joint & Conditional Entropy)\n",
    "\n",
    "The **joint differential entropy** of a set $X_1,\\dots,X_n$ of (jointly\n",
    "continuous) random variables with joint density $f = f(x_1,\\dots,x_n)$\n",
    "is defined as\n",
    "\n",
    "$$h(X_1,\\dots,X_n) = -\\int f(x_1,\\dots,x_n) \\log f(x_1,\\dots,x_n)\\,\\text{d}^{n}x.$$\n",
    "\n",
    "If $X$ and $Y$ have joint density $f_{X,Y}(x,y)$ then we define the\n",
    "**conditional differential entropy** as\n",
    "\n",
    "$$h(X|Y) = -\\int f_{X,Y}(x,y) \\log f_{X|Y}(x|y)\\,\\text{d} x\\,\\text{d} y.$$\n",
    "\n",
    "Since in general $f_{X|Y}(x|y) = f_{X,Y}(x,y)/f_Y(y)$ we immediately\n",
    "obtain the **chain rule**:\n",
    "\n",
    "$$h(X|Y) = h(X,Y) - h(Y),$$\n",
    "\n",
    "which may fail to hold if any of the concerned entropies are infinite or\n",
    "do not exist.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3335a6-61cf-4ede-b302-6b677a362a2b",
   "metadata": {},
   "source": [
    "### Example 3.1.7  (Entropy of a multivariate normal distribution)\n",
    "\n",
    "Let $X_1,\\dots,X_n$ have a multivariate Gaussian distribution with mean\n",
    "$\\mu$ and covariance matrix $K$. Then\n",
    "\n",
    "$$h(X_1,\\dots, X_n) = \\frac{1}{2}\\log\\left[(2\\pi e)^n\\det K\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03000f7c-33e4-474c-b8f8-511f27a76d1a",
   "metadata": {},
   "source": [
    "## 3.2 - Relative Entropy & Mutual Information\n",
    "\n",
    "### Definition 3.2.1 (KL divergence)\n",
    "\n",
    "The **relative entropy** (also known as the **Kullback-Liebler\n",
    "divergence**) $D(f\\,||\\,g)$ between two densities $f$ and $g$ is defined\n",
    "as\n",
    "\n",
    "$$D(f\\,||\\,g) = \\int f(x)\\log\\frac{f(x)}{g(x)}\\,\\text{d}x.$$\n",
    "\n",
    "---\n",
    "\n",
    "*Note:*\n",
    "$$D(f\\,||\\,g) < \\infty \\Longleftrightarrow \\text{supp}\\,f\\subseteq\\text{supp}\\,g.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb6e75",
   "metadata": {},
   "source": [
    "### Definition 3.2.2 (Mutual information)\n",
    "\n",
    "Given two random variables $X$ and $Y$ with joint density $f_{X,Y}$\n",
    "define the **mutual information** $I(X;Y)$ between $X$ and $Y$ by\n",
    "\n",
    "$$I(X;Y) = \\int f_{X,Y}(x,y)\\log\\left[\\frac{f_{X,Y}(x,y)}{f_X(x)f_Y(y)}\\right]\\,\\text{d}x\\,\\text{d}y.$$\n",
    "\n",
    "---\n",
    "\n",
    "From the definition it is clear that we have the formulas\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    I(X;Y) &= h(X) - h(X|Y)\\\\\n",
    "    &= h(Y) - h(Y|X)\\\\\n",
    "    &= h(X) + h(Y) - h(X,Y).\n",
    "\\end{aligned}$$\n",
    "\n",
    "Along with\n",
    "\n",
    "$$I(X;Y) = D(f_{X,Y}\\,||\\,f_X\\otimes f_Y).$$\n",
    "\n",
    "Note the special cases \n",
    "\n",
    "$$\\begin{aligned}\n",
    "I(X;Y) &= I(Y;X),\\\\\n",
    "I(X;X) &= h(X).\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f165c320",
   "metadata": {},
   "source": [
    "### Theorem 3.2.3 (Information inequality)\n",
    "\n",
    "For any pair of densities $f,\\,g$:\n",
    "\n",
    "$$D(f\\,||\\,g) \\geq 0,$$\n",
    "\n",
    "with equality if and only if $f = g$ a.e.\n",
    "\n",
    "\n",
    "*Proof.*  \n",
    "\n",
    "Without loss of generality assume $f/g\\geq1$.\n",
    "Use the fact that $\\int f = \\int g = 1$ to rewrite \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    D(f\\,||\\,g) &= \\int f\\log\\frac{f}{g}\\\\\n",
    "    &= \\int f\\left(\\frac{g}{f}-1-\\log\\frac{g}{f}\\right).\n",
    "\\end{aligned}$$\n",
    "\n",
    "Now note that for $t\\geq1$ we have the inequality:\n",
    "\n",
    "$$t - 1 -\\log t \\geq 0,$$\n",
    "\n",
    "in which equality holds iff $t=1.$ This can be easily established\n",
    "graphically or by means of elementary calculus. Applying this inequality\n",
    "to $f/g$ and integrating yields\n",
    "\n",
    "$$\\int \\frac{g}{f}-1-\\log\\frac{g}{f}\\;\\geq\\; 0$$\n",
    "\n",
    "with equality iff $f = g$ a.e. ◻\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef8f94",
   "metadata": {},
   "source": [
    "### Corollary 3.2.4\n",
    "\n",
    "For any pair of random variables $X,\\,Y$: $$I(X;Y)\\geq 0,$$ with\n",
    "equality if and only if $X$ and $Y$ are independent.\n",
    "\n",
    "\n",
    "*Proof.*  \n",
    "\n",
    "$$I(X;Y) = D(f_{X,Y}\\,||\\,f_X\\otimes f_Y) \\geq 0$$ \n",
    "\n",
    "with equality iff\n",
    "$f_{X,Y} = f_X\\otimes f_Y$ a.e. i.e. iff $X$ and $Y$ are independent. ◻\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf53cef",
   "metadata": {},
   "source": [
    "### Corollary 3.2.5\n",
    "\n",
    "For any pair of random variables $X,\\,Y$:\n",
    "\n",
    "$$h(X|Y) \\leq h(X),$$\n",
    "\n",
    "with equality if and only if $X$ and $Y$ are independent.\n",
    "\n",
    "\n",
    "*Proof.* \n",
    "\n",
    "$$h(X) - h(X|Y) = I(X;Y)\\,\\geq\\,0,$$ \n",
    "\n",
    "with equality iff $X$ and\n",
    "$Y$ are independent by Corollary 3.2.4. ◻\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a2c35",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "- [1] Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
